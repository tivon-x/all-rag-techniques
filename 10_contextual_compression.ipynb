{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/10_contextual_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "ZNB72M8fgTu8"
      },
      "source": [
        "# 上下文压缩以增强 RAG 系统\n",
        "在本笔记本中，我实现了一种上下文压缩技术，以提高我们的 RAG 系统的效率。我们将筛选并压缩检索到的文本块，仅保留最相关的部分，减少噪声并提高响应质量。\n",
        "\n",
        "在为 RAG 检索文档时，我们通常会得到包含相关和不相关信息的块。上下文压缩可以帮助我们：\n",
        "\n",
        "- 删除无关的句子和段落\n",
        "- 仅关注与查询相关的信息\n",
        "- 在上下文窗口中最大化有用信号\n",
        "- 提高效率，减少 LLM 需要处理的文本量\n",
        "\n",
        "让我们从头开始实现这种方法！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixnF1GJZgTu_"
      },
      "source": [
        "## 环境设置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "LoFs7TC0hrCl",
        "outputId": "25fc358b-bf41-4ae0-88e5-efed2c21a194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qwrb0StUgTvA"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RwMhU3EgTvC"
      },
      "source": [
        "## 从 pdf 文件中抽取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lTXzxl-hgTvC"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJdCVRbgTvD"
      },
      "source": [
        "## 分块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0_cTR0RWgTvE"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdwi4A_wgTvF"
      },
      "source": [
        "## OpenAI API Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "M_Z9DZiwhzwE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pUdNUcj9gTvG"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"doubao-lite-128k-240828\"\n",
        "embedding_model = \"doubao-embedding-text-240715\""
      ],
      "metadata": {
        "id": "eZB7jcFsiLZi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOtxswq0gTvH"
      },
      "source": [
        "## 构建向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hrnPo0pGgTvI"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the text corresponding to the index\n",
        "                \"metadata\": self.metadata[idx],  # Add the metadata corresponding to the index\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMmBXAYZgTvJ"
      },
      "source": [
        "## 嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3XDT_xzRgTvJ"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model, batch_size=10):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text.\n",
        "\n",
        "    Args:\n",
        "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "    batch_size (int): batch size for api calling\n",
        "\n",
        "    Returns:\n",
        "    List[float] or List[List[float]]: The embedding vector(s).\n",
        "    \"\"\"\n",
        "    # Handle both string and list inputs by ensuring input_text is always a list\n",
        "    input_text = text if isinstance(text, list) else [text]\n",
        "\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(input_text), batch_size):\n",
        "      batch = input_text[i : i + batch_size]\n",
        "      # Create embeddings for the batch using the specified model\n",
        "      response = client.embeddings.create(\n",
        "          model=model,\n",
        "          input=batch\n",
        "      )\n",
        "      all_embeddings.extend(item.embedding for item in response.data)\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt2SqOegTvK"
      },
      "source": [
        "## 构建文档处理 pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cG0RQ1CbgTvK"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for RAG.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the extracted text into smaller segments\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Create embeddings for each text chunk\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks, model=embedding_model)\n",
        "\n",
        "    # Initialize a simple vector store to store the chunks and their embeddings\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add each chunk and its corresponding embedding to the vector store\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d10FUMqwgTvK"
      },
      "source": [
        "## 实现上下文压缩\n",
        "这是我们的方法的核心——我们将使用大语言模型来筛选和压缩检索到的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sC_KV6b4gTvL"
      },
      "outputs": [],
      "source": [
        "def compress_chunk(chunk, query, model, compression_type=\"selective\"):\n",
        "    \"\"\"\n",
        "    Compress a retrieved chunk by keeping only the parts relevant to the query.\n",
        "\n",
        "    Args:\n",
        "        chunk (str): Text chunk to compress\n",
        "        query (str): User query\n",
        "        compression_type (str): Type of compression (\"selective\", \"summary\", or \"extraction\")\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Compressed chunk\n",
        "    \"\"\"\n",
        "    # Define system prompts for different compression approaches\n",
        "    if compression_type == \"selective\":\n",
        "        system_prompt = \"\"\"You are an expert at information filtering.\n",
        "        Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly\n",
        "        relevant to the user's query. Remove all irrelevant content.\n",
        "\n",
        "        Your output should:\n",
        "        1. ONLY include text that helps answer the query\n",
        "        2. Preserve the exact wording of relevant sentences (do not paraphrase)\n",
        "        3. Maintain the original order of the text\n",
        "        4. Include ALL relevant content, even if it seems redundant\n",
        "        5. EXCLUDE any text that isn't relevant to the query\n",
        "\n",
        "        Format your response as plain text with no additional comments.\"\"\"\n",
        "    elif compression_type == \"summary\":\n",
        "        system_prompt = \"\"\"You are an expert at summarization.\n",
        "        Your task is to create a concise summary of the provided chunk that focuses ONLY on\n",
        "        information relevant to the user's query.\n",
        "\n",
        "        Your output should:\n",
        "        1. Be brief but comprehensive regarding query-relevant information\n",
        "        2. Focus exclusively on information related to the query\n",
        "        3. Omit irrelevant details\n",
        "        4. Be written in a neutral, factual tone\n",
        "\n",
        "        Format your response as plain text with no additional comments.\"\"\"\n",
        "    else:  # extraction\n",
        "        system_prompt = \"\"\"You are an expert at information extraction.\n",
        "        Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant\n",
        "        to answering the user's query.\n",
        "\n",
        "        Your output should:\n",
        "        1. Include ONLY direct quotes of relevant sentences from the original text\n",
        "        2. Preserve the original wording (do not modify the text)\n",
        "        3. Include ONLY sentences that directly relate to the query\n",
        "        4. Separate extracted sentences with newlines\n",
        "        5. Do not add any commentary or additional text\n",
        "\n",
        "        Format your response as plain text with no additional comments.\"\"\"\n",
        "\n",
        "    # Define the user prompt with the query and document chunk\n",
        "    user_prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "\n",
        "        Document Chunk:\n",
        "        {chunk}\n",
        "\n",
        "        Extract only the content relevant to answering this query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response using the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the compressed chunk from the response\n",
        "    compressed_chunk = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Calculate compression ratio\n",
        "    original_length = len(chunk)\n",
        "    compressed_length = len(compressed_chunk)\n",
        "    compression_ratio = (original_length - compressed_length) / original_length * 100\n",
        "\n",
        "    return compressed_chunk, compression_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auoeQMUlgTvL"
      },
      "source": [
        "## 实现批量压缩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mhzFqEcLgTvL"
      },
      "outputs": [],
      "source": [
        "def batch_compress_chunks(chunks, query, model, compression_type=\"selective\"):\n",
        "    \"\"\"\n",
        "    Compress multiple chunks individually.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of text chunks to compress\n",
        "        query (str): User query\n",
        "        compression_type (str): Type of compression (\"selective\", \"summary\", or \"extraction\")\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: List of compressed chunks with compression ratios\n",
        "    \"\"\"\n",
        "    print(f\"Compressing {len(chunks)} chunks...\")  # Print the number of chunks to be compressed\n",
        "    results = []  # Initialize an empty list to store the results\n",
        "    total_original_length = 0  # Initialize a variable to store the total original length of chunks\n",
        "    total_compressed_length = 0  # Initialize a variable to store the total compressed length of chunks\n",
        "\n",
        "    # Iterate over each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Compressing chunk {i+1}/{len(chunks)}...\")  # Print the progress of compression\n",
        "        # Compress the chunk and get the compressed chunk and compression ratio\n",
        "        compressed_chunk, compression_ratio = compress_chunk(chunk, query, model, compression_type)\n",
        "        results.append((compressed_chunk, compression_ratio))  # Append the result to the results list\n",
        "\n",
        "        total_original_length += len(chunk)  # Add the length of the original chunk to the total original length\n",
        "        total_compressed_length += len(compressed_chunk)  # Add the length of the compressed chunk to the total compressed length\n",
        "\n",
        "    # Calculate the overall compression ratio\n",
        "    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100\n",
        "    print(f\"Overall compression ratio: {overall_ratio:.2f}%\")  # Print the overall compression ratio\n",
        "\n",
        "    return results  # Return the list of compressed chunks with compression ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxgKt9I5gTvM"
      },
      "source": [
        "## AI 响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "afEGGRIXgTvM"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text from compressed chunks\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context.\n",
        "    If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response using the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f-xkP4MgTvN"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NksVfbaWgTvN"
      },
      "outputs": [],
      "source": [
        "def rag_with_compression(pdf_path, query, model, k=10, compression_type=\"selective\"):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with contextual compression.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        k (int): Number of chunks to retrieve initially\n",
        "        compression_type (str): Type of compression\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        dict: Results including query, compressed chunks, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== RAG WITH CONTEXTUAL COMPRESSION ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Compression type: {compression_type}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Create an embedding for the query\n",
        "    query_embedding = create_embeddings(query, embedding_model)\n",
        "\n",
        "    # Retrieve the top k most similar chunks based on the query embedding\n",
        "    print(f\"Retrieving top {k} chunks...\")\n",
        "    results = vector_store.similarity_search(query_embedding, k=k)\n",
        "    retrieved_chunks = [result[\"text\"] for result in results]\n",
        "\n",
        "    # Apply compression to the retrieved chunks\n",
        "    compressed_results = batch_compress_chunks(retrieved_chunks, query, model, compression_type)\n",
        "    compressed_chunks = [result[0] for result in compressed_results]\n",
        "    compression_ratios = [result[1] for result in compressed_results]\n",
        "\n",
        "    # Filter out any empty compressed chunks\n",
        "    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n",
        "\n",
        "    if not filtered_chunks:\n",
        "        # If all chunks are compressed to empty strings, use the original chunks\n",
        "        print(\"Warning: All chunks were compressed to empty strings. Using original chunks.\")\n",
        "        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n",
        "    else:\n",
        "        compressed_chunks, compression_ratios = zip(*filtered_chunks)\n",
        "\n",
        "    # Generate context from the compressed chunks\n",
        "    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n",
        "\n",
        "    # Generate a response based on the compressed chunks\n",
        "    print(\"Generating response based on compressed chunks...\")\n",
        "    response = generate_response(query, context, model)\n",
        "\n",
        "    # Prepare the result dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"original_chunks\": retrieved_chunks,\n",
        "        \"compressed_chunks\": compressed_chunks,\n",
        "        \"compression_ratios\": compression_ratios,\n",
        "        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIYXwJjEgTvO"
      },
      "source": [
        "## 与标准 RAG 比较\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wYzX3kgYgTvO"
      },
      "outputs": [],
      "source": [
        "def standard_rag(pdf_path, query, model, k=10):\n",
        "    \"\"\"\n",
        "    Standard RAG without compression.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        k (int): Number of chunks to retrieve\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        dict: Results including query, chunks, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STANDARD RAG ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Create an embedding for the query\n",
        "    query_embedding = create_embeddings(query, embedding_model)\n",
        "\n",
        "    # Retrieve the top k most similar chunks based on the query embedding\n",
        "    print(f\"Retrieving top {k} chunks...\")\n",
        "    results = vector_store.similarity_search(query_embedding, k=k)\n",
        "    retrieved_chunks = [result[\"text\"] for result in results]\n",
        "\n",
        "    # Generate context from the retrieved chunks\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    # Generate a response based on the retrieved chunks\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(query, context, model)\n",
        "\n",
        "    # Prepare the result dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"chunks\": retrieved_chunks,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy9iexwkgTvP"
      },
      "source": [
        "## 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZccftaEPgTvP"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(query, responses, reference_answer):\n",
        "    \"\"\"\n",
        "    Evaluate multiple responses against a reference answer.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        responses (Dict[str, str]): Dictionary of responses by method\n",
        "        reference_answer (str): Reference answer\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation text\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior for evaluation\n",
        "    system_prompt = \"\"\"You are an objective evaluator of RAG responses. Compare different responses to the same query\n",
        "    and determine which is most accurate, comprehensive, and relevant to the query.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the query and reference answer\n",
        "    user_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "\n",
        "    Reference Answer: {reference_answer}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Add each response to the prompt\n",
        "    for method, response in responses.items():\n",
        "        user_prompt += f\"\\n{method.capitalize()} Response:\\n{response}\\n\"\n",
        "\n",
        "    # Add the evaluation criteria to the user prompt\n",
        "    user_prompt += \"\"\"\n",
        "    Please evaluate these responses based on:\n",
        "    1. Factual accuracy compared to the reference\n",
        "    2. Comprehensiveness - how completely they answer the query\n",
        "    3. Conciseness - whether they avoid irrelevant information\n",
        "    4. Overall quality\n",
        "\n",
        "    Rank the responses from best to worst with detailed explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate an evaluation response using the OpenAI API\n",
        "    evaluation_response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the evaluation text from the response\n",
        "    return evaluation_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SWj6qq2MgTvR"
      },
      "outputs": [],
      "source": [
        "def evaluate_compression(pdf_path, query, model, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
        "    \"\"\"\n",
        "    Compare different compression techniques with standard RAG.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        reference_answer (str): Optional reference answer\n",
        "        compression_types (List[str]): Compression types to evaluate\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n=== EVALUATING CONTEXTUAL COMPRESSION ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Run standard RAG without compression\n",
        "    standard_result = standard_rag(pdf_path, query, model)\n",
        "\n",
        "    # Dictionary to store results of different compression techniques\n",
        "    compression_results = {}\n",
        "\n",
        "    # Run RAG with each compression technique\n",
        "    for comp_type in compression_types:\n",
        "        print(f\"\\nTesting {comp_type} compression...\")\n",
        "        compression_results[comp_type] = rag_with_compression(pdf_path, query, model, compression_type=comp_type)\n",
        "\n",
        "    # Gather responses for evaluation\n",
        "    responses = {\n",
        "        \"standard\": standard_result[\"response\"]\n",
        "    }\n",
        "    for comp_type in compression_types:\n",
        "        responses[comp_type] = compression_results[comp_type][\"response\"]\n",
        "\n",
        "    # Evaluate responses if a reference answer is provided\n",
        "    if reference_answer:\n",
        "        evaluation = evaluate_responses(query, responses, reference_answer)\n",
        "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "        print(evaluation)\n",
        "    else:\n",
        "        evaluation = \"No reference answer provided for evaluation.\"\n",
        "\n",
        "    # Calculate metrics for each compression type\n",
        "    metrics = {}\n",
        "    for comp_type in compression_types:\n",
        "        metrics[comp_type] = {\n",
        "            \"avg_compression_ratio\": f\"{sum(compression_results[comp_type]['compression_ratios'])/len(compression_results[comp_type]['compression_ratios']):.2f}%\",\n",
        "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n",
        "            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n",
        "        }\n",
        "\n",
        "    # Return the evaluation results, responses, and metrics\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"responses\": responses,\n",
        "        \"evaluation\": evaluation,\n",
        "        \"metrics\": metrics,\n",
        "        \"standard_result\": standard_result,\n",
        "        \"compression_results\": compression_results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOql56LggTvS"
      },
      "source": [
        "## Running Our Complete System (Custom Query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SueAoyxMn6EA",
        "outputId": "8b3098e5-4529-43eb-85f0-72fba4bcf05d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "45uFrseygTvT",
        "outputId": "b9894f31-fa4a-4d80-aa02-f8e775eda526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVALUATING CONTEXTUAL COMPRESSION ===\n",
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "\n",
            "=== STANDARD RAG ===\n",
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Retrieving top 10 chunks...\n",
            "Generating response...\n",
            "\n",
            "=== RESPONSE ===\n",
            "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. This lack of transparency raises concerns about accountability and the potential for biases to go unnoticed. AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes. Ensuring fairness and mitigating bias in AI systems is a critical challenge. Additionally, privacy and security issues arise as AI systems often rely on large amounts of data, raising concerns about the protection of sensitive information. Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial to address these ethical concerns and build trust in AI.\n",
            "\n",
            "Testing selective compression...\n",
            "\n",
            "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "Compression type: selective\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Retrieving top 10 chunks...\n",
            "Compressing 10 chunks...\n",
            "Compressing chunk 1/10...\n",
            "Compressing chunk 2/10...\n",
            "Compressing chunk 3/10...\n",
            "Compressing chunk 4/10...\n",
            "Compressing chunk 5/10...\n",
            "Compressing chunk 6/10...\n",
            "Compressing chunk 7/10...\n",
            "Compressing chunk 8/10...\n",
            "Compressing chunk 9/10...\n",
            "Compressing chunk 10/10...\n",
            "Overall compression ratio: 54.85%\n",
            "Generating response based on compressed chunks...\n",
            "\n",
            "=== RESPONSE ===\n",
            "The ethical concerns surrounding the use of AI in decision-making include ensuring fairness and mitigating bias as AI systems can inherit and amplify biases in the training data, leading to unfair or discriminatory outcomes. Transparency and explainability are crucial as many AI systems are \"black boxes\" making it difficult to understand how they arrive at decisions. Protecting privacy and data security is also a concern as AI systems often rely on large amounts of data. Additionally, promoting fairness, transparency, and accountability in AI systems while protecting human rights and values is essential for ensuring the positive social impact of AI. Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial to address these concerns.\n",
            "\n",
            "Testing summary compression...\n",
            "\n",
            "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "Compression type: summary\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Retrieving top 10 chunks...\n",
            "Compressing 10 chunks...\n",
            "Compressing chunk 1/10...\n",
            "Compressing chunk 2/10...\n",
            "Compressing chunk 3/10...\n",
            "Compressing chunk 4/10...\n",
            "Compressing chunk 5/10...\n",
            "Compressing chunk 6/10...\n",
            "Compressing chunk 7/10...\n",
            "Compressing chunk 8/10...\n",
            "Compressing chunk 9/10...\n",
            "Compressing chunk 10/10...\n",
            "Overall compression ratio: 61.44%\n",
            "Generating response based on compressed chunks...\n",
            "\n",
            "=== RESPONSE ===\n",
            "The ethical concerns surrounding the use of AI in decision-making include bias and fairness (AI systems can inherit and amplify biases in data, leading to unfair outcomes), transparency and explainability (many AI systems are \"black boxes\" making it difficult to understand how they make decisions), privacy and security (AI systems rely on large amounts of data raising privacy and security concerns), job displacement due to AI's automation capabilities, and questions about autonomy, control, and unintended consequences. Addressing these concerns is crucial for ensuring the positive social impact of AI and building trust through transparency and public engagement.\n",
            "\n",
            "Testing extraction compression...\n",
            "\n",
            "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "Compression type: extraction\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Retrieving top 10 chunks...\n",
            "Compressing 10 chunks...\n",
            "Compressing chunk 1/10...\n",
            "Compressing chunk 2/10...\n",
            "Compressing chunk 3/10...\n",
            "Compressing chunk 4/10...\n",
            "Compressing chunk 5/10...\n",
            "Compressing chunk 6/10...\n",
            "Compressing chunk 7/10...\n",
            "Compressing chunk 8/10...\n",
            "Compressing chunk 9/10...\n",
            "Compressing chunk 10/10...\n",
            "Overall compression ratio: 59.19%\n",
            "Generating response based on compressed chunks...\n",
            "\n",
            "=== RESPONSE ===\n",
            "The ethical concerns surrounding the use of AI in decision-making include bias and fairness (AI systems can inherit and amplify biases in training data, leading to unfair or discriminatory outcomes), transparency and explainability (many AI systems are \"black boxes\" making it difficult to understand how they make decisions), privacy and data security (AI systems often rely on large amounts of data raising concerns about protecting sensitive information), and control, accountability, and the potential for unintended consequences (as AI becomes more autonomous, questions arise about who has control and who is accountable, and potential for unexpected results). Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial to address these concerns.\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "1. Reference Answer: This response is the best as it is the most comprehensive and detailed. It covers all the main ethical concerns including bias in models, lack of transparency, privacy risks, job displacement, and power concentration. It provides specific examples and explanations for each concern, making it very factual and accurate. It also emphasizes the importance of ensuring fairness, accountability, and transparency in AI systems, which is a key aspect of the query.\n",
            "2. Standard Response: This response is also very good. It covers the main concerns of transparency, bias, privacy, and the need for ethical frameworks. It is concise and to the point while still providing important information. It is slightly less detailed than the reference answer but still covers the main points.\n",
            "3. Summary Response: This response is a good summary of the main ethical concerns. It covers the key points such as bias, transparency, privacy, and job displacement. However, it is less detailed than the previous two responses and may not provide as much depth in explaining each concern.\n",
            "4. Extraction Response: This response is the least comprehensive. While it covers some of the main concerns such as bias, transparency, and privacy, it does not mention job displacement or other important aspects. It also lacks the detailed explanations and examples provided in the other responses.\n",
            "\n",
            "Overall, the Reference Answer is the best as it is the most comprehensive and detailed. The Standard Response is also excellent and provides a concise summary. The Summary Response is a good summary but may lack some depth. The Extraction Response is the least comprehensive and may not fully address the query. \n"
          ]
        }
      ],
      "source": [
        "# Path to the PDF document containing information on AI ethics\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\"\n",
        "\n",
        "# Query to extract relevant information from the document\n",
        "query = \"What are the ethical concerns surrounding the use of AI in decision-making?\"\n",
        "\n",
        "# Optional reference answer for evaluation\n",
        "reference_answer = \"\"\"\n",
        "The use of AI in decision-making raises several ethical concerns.\n",
        "- Bias in AI models can lead to unfair or discriminatory outcomes, especially in critical areas like hiring, lending, and law enforcement.\n",
        "- Lack of transparency and explainability in AI-driven decisions makes it difficult for individuals to challenge unfair outcomes.\n",
        "- Privacy risks arise as AI systems process vast amounts of personal data, often without explicit consent.\n",
        "- The potential for job displacement due to automation raises social and economic concerns.\n",
        "- AI decision-making may also concentrate power in the hands of a few large tech companies, leading to accountability challenges.\n",
        "- Ensuring fairness, accountability, and transparency in AI systems is essential for ethical deployment.\n",
        "\"\"\"\n",
        "\n",
        "# Run evaluation with different compression techniques\n",
        "# Compression types:\n",
        "# - \"selective\": Retains key details while omitting less relevant parts\n",
        "# - \"summary\": Provides a concise version of the information\n",
        "# - \"extraction\": Extracts relevant sentences verbatim from the document\n",
        "results = evaluate_compression(\n",
        "    pdf_path=pdf_path,\n",
        "    query=query,\n",
        "    model=model_name,\n",
        "    reference_answer=reference_answer,\n",
        "    compression_types=[\"selective\", \"summary\", \"extraction\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDa6GaCgTvU"
      },
      "source": [
        "## Visualizing Compression Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pCHVc3NpgTvU"
      },
      "outputs": [],
      "source": [
        "def visualize_compression_results(evaluation_results):\n",
        "    \"\"\"\n",
        "    Visualize the results of different compression techniques.\n",
        "\n",
        "    Args:\n",
        "        evaluation_results (Dict): Results from evaluate_compression function\n",
        "    \"\"\"\n",
        "    # Extract the query and standard chunks from the evaluation results\n",
        "    query = evaluation_results[\"query\"]\n",
        "    standard_chunks = evaluation_results[\"standard_result\"][\"chunks\"]\n",
        "\n",
        "    # Print the query\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Get a sample chunk to visualize (using the first chunk)\n",
        "    original_chunk = standard_chunks[0]\n",
        "\n",
        "    # Iterate over each compression type and show a comparison\n",
        "    for comp_type in evaluation_results[\"compression_results\"].keys():\n",
        "        compressed_chunks = evaluation_results[\"compression_results\"][comp_type][\"compressed_chunks\"]\n",
        "        compression_ratios = evaluation_results[\"compression_results\"][comp_type][\"compression_ratios\"]\n",
        "\n",
        "        # Get the corresponding compressed chunk and its compression ratio\n",
        "        compressed_chunk = compressed_chunks[0]\n",
        "        compression_ratio = compression_ratios[0]\n",
        "\n",
        "        print(f\"\\n=== {comp_type.upper()} COMPRESSION EXAMPLE ===\\n\")\n",
        "\n",
        "        # Show the original chunk (truncated if too long)\n",
        "        print(\"ORIGINAL CHUNK:\")\n",
        "        print(\"-\" * 40)\n",
        "        if len(original_chunk) > 800:\n",
        "            print(original_chunk[:800] + \"... [truncated]\")\n",
        "        else:\n",
        "            print(original_chunk)\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Length: {len(original_chunk)} characters\\n\")\n",
        "\n",
        "        # Show the compressed chunk\n",
        "        print(\"COMPRESSED CHUNK:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(compressed_chunk)\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Length: {len(compressed_chunk)} characters\")\n",
        "        print(f\"Compression ratio: {compression_ratio:.2f}%\\n\")\n",
        "\n",
        "        # Show overall statistics for this compression type\n",
        "        avg_ratio = sum(compression_ratios) / len(compression_ratios)\n",
        "        print(f\"Average compression across all chunks: {avg_ratio:.2f}%\")\n",
        "        print(f\"Total context length reduction: {evaluation_results['metrics'][comp_type]['avg_compression_ratio']}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    # Show a summary table of compression techniques\n",
        "    print(\"\\n=== COMPRESSION SUMMARY ===\\n\")\n",
        "    print(f\"{'Technique':<15} {'Avg Ratio':<15} {'Context Length':<15} {'Original Length':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Print the metrics for each compression type\n",
        "    for comp_type, metrics in evaluation_results[\"metrics\"].items():\n",
        "        print(f\"{comp_type:<15} {metrics['avg_compression_ratio']:<15} {metrics['total_context_length']:<15} {metrics['original_context_length']:<15}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2D2VqkISgTvV",
        "outputId": "bb214965-506c-4c6c-d08e-5211992e8bd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "=== SELECTIVE COMPRESSION EXAMPLE ===\n",
            "\n",
            "ORIGINAL CHUNK:\n",
            "----------------------------------------\n",
            "control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
            "concerns. International discussions and regulations are needed to address the risks associated \n",
            "with AI-powered weapons. \n",
            "Chapter 5: The Future of Artificial Intelligence \n",
            "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
            "across various domains. Key trends and areas of development include: \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
            "techniques are being developed to provide insights into how AI m... [truncated]\n",
            "----------------------------------------\n",
            "Length: 1000 characters\n",
            "\n",
            "COMPRESSED CHUNK:\n",
            "----------------------------------------\n",
            "control, accountability, and the potential for unintended consequences. Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial.\n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security concerns. International discussions and regulations are needed to address the risks associated with AI-powered weapons.\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI techniques are being developed to provide insights into how AI models make decisions, enhancing trust and accountability.\n",
            "----------------------------------------\n",
            "Length: 592 characters\n",
            "Compression ratio: 40.80%\n",
            "\n",
            "Average compression across all chunks: 54.85%\n",
            "Total context length reduction: 54.85%\n",
            "================================================================================\n",
            "\n",
            "=== SUMMARY COMPRESSION EXAMPLE ===\n",
            "\n",
            "ORIGINAL CHUNK:\n",
            "----------------------------------------\n",
            "control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
            "concerns. International discussions and regulations are needed to address the risks associated \n",
            "with AI-powered weapons. \n",
            "Chapter 5: The Future of Artificial Intelligence \n",
            "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
            "across various domains. Key trends and areas of development include: \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
            "techniques are being developed to provide insights into how AI m... [truncated]\n",
            "----------------------------------------\n",
            "Length: 1000 characters\n",
            "\n",
            "COMPRESSED CHUNK:\n",
            "----------------------------------------\n",
            "Control, accountability, and potential for unintended consequences. Establishing guidelines and ethical frameworks for AI. Weaponization of AI in autonomous weapons systems with risks and need for international discussions and regulations. Explainable AI (XAI) to make AI systems more transparent and understandable. AI at the edge for processing data locally.\n",
            "----------------------------------------\n",
            "Length: 360 characters\n",
            "Compression ratio: 64.00%\n",
            "\n",
            "Average compression across all chunks: 61.44%\n",
            "Total context length reduction: 61.44%\n",
            "================================================================================\n",
            "\n",
            "=== EXTRACTION COMPRESSION EXAMPLE ===\n",
            "\n",
            "ORIGINAL CHUNK:\n",
            "----------------------------------------\n",
            "control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
            "concerns. International discussions and regulations are needed to address the risks associated \n",
            "with AI-powered weapons. \n",
            "Chapter 5: The Future of Artificial Intelligence \n",
            "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
            "across various domains. Key trends and areas of development include: \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
            "techniques are being developed to provide insights into how AI m... [truncated]\n",
            "----------------------------------------\n",
            "Length: 1000 characters\n",
            "\n",
            "COMPRESSED CHUNK:\n",
            "----------------------------------------\n",
            "control, accountability, and the potential for unintended consequences. Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial.\n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security concerns. International discussions and regulations are needed to address the risks associated with AI-powered weapons.\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI techniques are being developed to provide insights into how AI models make decisions, enhancing trust and accountability.\n",
            "----------------------------------------\n",
            "Length: 592 characters\n",
            "Compression ratio: 40.80%\n",
            "\n",
            "Average compression across all chunks: 59.19%\n",
            "Total context length reduction: 59.19%\n",
            "================================================================================\n",
            "\n",
            "=== COMPRESSION SUMMARY ===\n",
            "\n",
            "Technique       Avg Ratio       Context Length  Original Length\n",
            "------------------------------------------------------------\n",
            "selective       54.85%          4533            10018          \n",
            "summary         61.44%          3874            10018          \n",
            "extraction      59.19%          4099            10018          \n"
          ]
        }
      ],
      "source": [
        "# Visualize the compression results\n",
        "visualize_compression_results(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}