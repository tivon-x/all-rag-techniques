{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "csv8NY84x8li"
      },
      "source": [
        "## 在简单 RAG 中评估块大小\n",
        "\n",
        "选择合适的块大小对于提高检索增强生成（RAG）流程中的检索准确性至关重要。目标是在检索性能和响应质量之间取得平衡。\n",
        "\n",
        "本节通过以下步骤评估不同的块大小：\n",
        "\n",
        "1. 从 PDF 中提取文本。\n",
        "2. 将文本分割成不同大小的块。\n",
        "3. 为每个块创建嵌入。\n",
        "4. 检索与查询相关的块。\n",
        "5. 使用检索到的块生成响应。\n",
        "6. 评估响应的忠实度和相关性。\n",
        "7. 比较不同块大小的结果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WDAQfW4x8lj"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "b7cyc8-1AGNr",
        "outputId": "d234a8f3-6860-494a-d26e-76312ba03335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d8WWU_Xmx8lj"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBH4LUiHx8lk"
      },
      "source": [
        "## 设置 OpenAI API 客户端\n",
        "我们初始化 OpenAI 客户端以生成嵌入和响应。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "ADS2vT_dAfIN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"doubao-lite-128k-240828\"\n",
        "embedding_model = \"doubao-embedding-text-240715\""
      ],
      "metadata": {
        "id": "3KheaIqkBX69"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8AB8lPbvx8lk"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL3UjTiPx8lk"
      },
      "source": [
        "## 从 PDF 文件中提取文本\n",
        "现在，我们加载 PDF 文件，提取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2QVE-bQvx8lk",
        "outputId": "9a192b22-f156-4185-f0df-45bfca84f384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Understanding Artificial Intelligence \n",
            "Chapter 1: Introduction to Artificial Intelligence \n",
            "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
            "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
            "the project of developing systems endowed with the intellectual processes characteristic of \n",
            "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
            "experience. Over the past f\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    从 PDF 文件中抽取文本.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): PDF 文件的路径\n",
        "\n",
        "    Returns:\n",
        "    str: 抽取的文本.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page in mypdf:\n",
        "        # Extract text from the current page and add spacing\n",
        "        all_text += page.get_text(\"text\") + \" \"\n",
        "\n",
        "    # Return the extracted text, stripped of leading/trailing whitespace\n",
        "    return all_text.strip()\n",
        "\n",
        "# PDF 文件路径\n",
        "pdf_path = \"./data/AI_Information.pdf\"\n",
        "\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "print(extracted_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keDV7ibkx8ll"
      },
      "source": [
        "## 对提取的文本进行分块\n",
        "为了提高检索效果，我们将提取的文本分割成不同大小的重叠块。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rAKcoGdkx8ll",
        "outputId": "9cbd9d4a-c69d-445a-b7e1-de60c178f509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Size: 128, Number of Chunks: 326\n",
            "Chunk Size: 256, Number of Chunks: 164\n",
            "Chunk Size: 512, Number of Chunks: 82\n"
          ]
        }
      ],
      "source": [
        "def chunk_text(text: str, n: int, overlap: int):\n",
        "    \"\"\"\n",
        "    将给定的文本分割成具有重叠的 n 个字符的段。\n",
        "\n",
        "    Args:\n",
        "    text (str): 要被分块的文本\n",
        "    n (int): 每个分块的字符数量.\n",
        "    overlap (int): 分块之间重叠的字符数量.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: 文本分块列表.\n",
        "    \"\"\"\n",
        "    chunks = []  # 初始化一个空列表来存储分块的内容。\n",
        "\n",
        "    # 以 (n - overlap) 为步长遍历文本。\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # 将从索引 i 到 i + n 的文本块追加到 chunks 列表中\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # 返回文本分块列表\n",
        "\n",
        "# 定义要评估的不同块大小\n",
        "chunk_sizes = [128, 256, 512]\n",
        "\n",
        "# 创建一个字典，用于存储每个块大小的文本块\n",
        "text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n",
        "\n",
        "# Print the number of chunks created for each chunk size\n",
        "for size, chunks in text_chunks_dict.items():\n",
        "    print(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIAM-mIx8ll"
      },
      "source": [
        "## 为文本块创建嵌入\n",
        "嵌入将文本转换为数值向量，从而可以高效地进行相似性搜索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW4lPwM5x8ll"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def create_embeddings(texts, model=\"doubao-embedding-text-240715\"):\n",
        "    \"\"\"\n",
        "    生成文本列表对应的嵌入向量列表.\n",
        "\n",
        "    Args:\n",
        "    texts (List[str]): 输入文本列表.\n",
        "    model (str): 嵌入模型，默认为 doubao-embedding-text-240715\n",
        "\n",
        "    Returns:\n",
        "    List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(model=model, input=texts)\n",
        "    return [np.array(embedding.embedding) for embedding in response.data]\n",
        "\n",
        "def batch_read(lst, batch_size = 10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "      yield lst[i : i + batch_size]\n",
        "\n",
        "\n",
        "# 为每个块大小生成嵌入\n",
        "# 遍历 text_chunks_dict 中的每个块大小及其对应的块\n",
        "chunk_embeddings_dict = {}\n",
        "\n",
        "for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\"):\n",
        "  chunk_embeddings_dict[size] = []\n",
        "  for batch in tqdm(batch_read(chunks, 10), desc=\"batch\"):\n",
        "    chunk_embeddings_dict[size].extend(create_embeddings(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K5R1Ynsx8ll"
      },
      "source": [
        "## 执行语义搜索\n",
        "我们使用余弦相似度来找到与用户查询最相关的文本块。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qlBJL5fSx8ll"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    计算两个向量之间的余弦相似度\n",
        "\n",
        "    Args:\n",
        "    vec1 (np.ndarray): 第一个向量.\n",
        "    vec2 (np.ndarray): 第二个向量.\n",
        "\n",
        "    Returns:\n",
        "    float: 两个向量之间的余弦相似度.\n",
        "    \"\"\"\n",
        "    # 计算两个向量的点积，并除以它们范数的乘积\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lYrzWE2Ax8ll"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n",
        "    \"\"\"\n",
        "    检索最相关的前 k 个文本块\n",
        "\n",
        "    Args:\n",
        "    query (str): 语义搜索的查询\n",
        "    text_chunks (List[str]): 要进行检索的文本块列表\n",
        "    embeddings (List[dict]): 文本块对应的嵌入向量列表.\n",
        "    k (int): 要返回的最相关的文本块的数量，默认为 5.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: 基于查询的最相关的 k 个文本块的列表.\n",
        "    \"\"\"\n",
        "    query_embedding = create_embeddings([query])[0]\n",
        "\n",
        "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
        "\n",
        "    # 最相关的前 k 个文本块\n",
        "    # 从小到大排序，所以是-k；\n",
        "    # 反转序列成从大到小排序\n",
        "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
        "\n",
        "    return [text_chunks[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lJ5AijrNx8lm",
        "outputId": "084e9989-b27d-4699-87f7-62599fc3c998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Transparency and Explainability \\nTransparency and explainability are key to building trust in AI. Making AI systems understandable \\nand providing insights into their decision-making processes helps users assess their reliability \\nand fairness. \\nRobustness ', 'd explainability is \\ncrucial for building trust and accountability. \\n \\n  \\nPrivacy and Security \\nAI systems often rely on large amounts of data, raising concerns about privacy and data security. \\nProtecting sensitive information and ensuring responsible dat', 'he world are developing AI strategies and policy frameworks to guide the \\ndevelopment and deployment of AI. These frameworks address ethical considerations, promote \\ninnovation, and ensure responsible AI practices. \\nRegulation of AI \\nThe regulation of AI i', ', applications, ethical implications, and future directions of \\nAI, we can better navigate the opportunities and challenges presented by this transformative \\ntechnology. Continued research, responsible development, and thoughtful governance are \\nessential ', 'y and Explainability \\nTransparency and explainability are essential for building trust in AI systems. Explainable AI (XAI) \\ntechniques aim to make AI decisions more understandable, enabling users to assess their \\nfairness and accuracy. \\nPrivacy and Data Pr']\n"
          ]
        }
      ],
      "source": [
        "# 加载验证数据集\n",
        "with open('data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "query = data[0]['question']\n",
        "\n",
        "retrieved_chunks_dict = {size: retrieve_relevant_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size]) for size in chunk_sizes}\n",
        "\n",
        "# Print retrieved chunks for chunk size 256\n",
        "print(retrieved_chunks_dict[256])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18cDAJ3x8lm"
      },
      "source": [
        "## 基于检索到的块生成响应\n",
        "让我们根据块大小为 `256` 的检索到的文本生成一个响应。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cI8JBDO-x8lm",
        "outputId": "e32b1e2a-9440-4ce9-86c8-433723ab7819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explainable AI (XAI) techniques aim to make AI decisions more understandable. It is considered important because transparency and explainability are key to building trust in AI. Making AI systems understandable and providing insights into their decision-making processes helps users assess their reliability and fairness. It is crucial for building trust and accountability.\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
        "\n",
        "def generate_response(query, system_prompt, retrieved_chunks, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    根据 system prompt 和 user message 从 AI 模型生成响应。\n",
        "\n",
        "    Args:\n",
        "    system_prompt (str): system prompt\n",
        "    user_message (str): 用户消息或查询\n",
        "    model (str): LLM, 默认为qwen2.5-7b-instruct-1m.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    str: AI 响应消息.\n",
        "    \"\"\"\n",
        "    # Combine retrieved chunks into a single context string\n",
        "    context = \"\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Generate the AI response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the content of the AI response\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generate AI responses for each chunk size\n",
        "ai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n",
        "\n",
        "# Print the response for chunk size 256\n",
        "print(ai_responses_dict[256])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXFzX1Mxx8lm"
      },
      "source": [
        "## 评估 AI 响应\n",
        "我们使用强大的大语言模型根据忠实度和相关性对响应进行评分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aYem8h6ax8lm"
      },
      "outputs": [],
      "source": [
        "# 定义评估评分系统常量\n",
        "SCORE_FULL = 1.0     # 完全匹配或完全令人满意\n",
        "SCORE_PARTIAL = 0.5  # 部分匹配或有些令人满意\n",
        "SCORE_NONE = 0.0     # 不匹配或令人不满意"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "shbBbhR5x8lm"
      },
      "outputs": [],
      "source": [
        "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
        "Evaluate the faithfulness of the AI response compared to the true answer.\n",
        "User Query: {question}\n",
        "AI Response: {response}\n",
        "True Answer: {true_answer}\n",
        "\n",
        "Faithfulness measures how well the AI response aligns with facts in the true answer, without hallucinations.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Score STRICTLY using only these values:\n",
        "    * {full} = Completely faithful, no contradictions with true answer\n",
        "    * {partial} = Partially faithful, minor contradictions\n",
        "    * {none} = Not faithful, major contradictions or hallucinations\n",
        "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LMI-cub5x8lm"
      },
      "outputs": [],
      "source": [
        "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
        "Evaluate the relevancy of the AI response to the user query.\n",
        "User Query: {question}\n",
        "AI Response: {response}\n",
        "\n",
        "Relevancy measures how well the response addresses the user's question.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Score STRICTLY using only these values:\n",
        "    * {full} = Completely relevant, directly addresses the query\n",
        "    * {partial} = Partially relevant, addresses some aspects\n",
        "    * {none} = Not relevant, fails to address the query\n",
        "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nai4M8jMx8lm",
        "outputId": "f90daa90-0edc-4987-acb4-57e077351d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faithfulness Score (Chunk Size 256): 1.0\n",
            "Relevancy Score (Chunk Size 256): 1.0\n",
            "\n",
            "\n",
            "Faithfulness Score (Chunk Size 128): 1.0\n",
            "Relevancy Score (Chunk Size 128): 1.0\n",
            "\n",
            "\n",
            "Faithfulness Score (Chunk Size 512): 1.0\n",
            "Relevancy Score (Chunk Size 512): 1.0\n"
          ]
        }
      ],
      "source": [
        "def evaluate_response(question, response, true_answer):\n",
        "        \"\"\"\n",
        "        根据忠实度和相关性评估 AI 生成响应的质量。\n",
        "\n",
        "        Args:\n",
        "        question (str): 用户初始问题.\n",
        "        response (str): 要被评估的 AI 生成的响应.\n",
        "        true_answer (str): 用作真实答案的正确答案h.\n",
        "\n",
        "        Returns:\n",
        "        Tuple[float, float]: A tuple containing (faithfulness_score, relevancy_score).\n",
        "                                                Each score is one of: 1.0 (full), 0.5 (partial), or 0.0 (none).\n",
        "        \"\"\"\n",
        "        # 格式化，填充信息\n",
        "        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
        "                question=question,\n",
        "                response=response,\n",
        "                true_answer=true_answer,\n",
        "                full=SCORE_FULL,\n",
        "                partial=SCORE_PARTIAL,\n",
        "                none=SCORE_NONE\n",
        "        )\n",
        "\n",
        "        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
        "                question=question,\n",
        "                response=response,\n",
        "                full=SCORE_FULL,\n",
        "                partial=SCORE_PARTIAL,\n",
        "                none=SCORE_NONE\n",
        "        )\n",
        "\n",
        "        # Request faithfulness evaluation from the model\n",
        "        faithfulness_response = client.chat.completions.create(\n",
        "               model=\"deepseek-v3-250324\",\n",
        "                temperature=0,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n",
        "                        {\"role\": \"user\", \"content\": faithfulness_prompt}\n",
        "                ]\n",
        "        )\n",
        "\n",
        "        # Request relevancy evaluation from the model\n",
        "        relevancy_response = client.chat.completions.create(\n",
        "                model=\"deepseek-v3-250324\",\n",
        "                temperature=0,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n",
        "                        {\"role\": \"user\", \"content\": relevancy_prompt}\n",
        "                ]\n",
        "        )\n",
        "\n",
        "        # Extract scores and handle potential parsing errors\n",
        "        try:\n",
        "                faithfulness_score = float(faithfulness_response.choices[0].message.content.strip())\n",
        "        except ValueError:\n",
        "                print(\"Warning: Could not parse faithfulness score, defaulting to 0\")\n",
        "                faithfulness_score = 0.0\n",
        "\n",
        "        try:\n",
        "                relevancy_score = float(relevancy_response.choices[0].message.content.strip())\n",
        "        except ValueError:\n",
        "                print(\"Warning: Could not parse relevancy score, defaulting to 0\")\n",
        "                relevancy_score = 0.0\n",
        "\n",
        "        return faithfulness_score, relevancy_score\n",
        "\n",
        "true_answer = data[0]['ideal_answer']\n",
        "\n",
        "faithfulness, relevancy = evaluate_response(query, ai_responses_dict[256], true_answer)\n",
        "faithfulness2, relevancy2 = evaluate_response(query, ai_responses_dict[128], true_answer)\n",
        "faithfulness3, relevancy3 = evaluate_response(query, ai_responses_dict[512], true_answer)\n",
        "\n",
        "\n",
        "# print the evaluation scores\n",
        "print(f\"Faithfulness Score (Chunk Size 256): {faithfulness}\")\n",
        "print(f\"Relevancy Score (Chunk Size 256): {relevancy}\")\n",
        "\n",
        "print(f\"\\n\")\n",
        "\n",
        "print(f\"Faithfulness Score (Chunk Size 128): {faithfulness2}\")\n",
        "print(f\"Relevancy Score (Chunk Size 128): {relevancy2}\")\n",
        "\n",
        "print(f\"\\n\")\n",
        "\n",
        "print(f\"Faithfulness Score (Chunk Size 512): {faithfulness3}\")\n",
        "print(f\"Relevancy Score (Chunk Size 512): {relevancy3}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}