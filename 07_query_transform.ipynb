{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/07_query_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "VvC3eUlcmrI6"
      },
      "source": [
        "# 面向增强 RAG 系统的查询转换\n",
        "\n",
        "本笔记本实现了三种查询转换技术，以增强 RAG 系统中的检索性能，无需依赖 LangChain 等专业库。通过修改用户查询，我们可以显著提高检索信息的相关性和全面性。\n",
        "\n",
        "## 主要转换技术\n",
        "\n",
        "1. **查询重写（Query Rewriting）**：使查询更具针对性和详细性，以提高搜索精度。\n",
        "2. **后退提示 （Step-back Prompting）**：生成更宽泛的查询，以检索有用的上下文信息。\n",
        "3. **子查询分解 （Sub-query composition）**：将复杂查询分解为更简单的组件，以实现全面检索。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfNDD58kmrI7"
      },
      "source": [
        "## 环境设置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "EAhmLkg1m83Q",
        "outputId": "f08cfb55-b517-4128-da65-fe81cb34d438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sXo-BxOumrI7"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_SJxa9ymrI7"
      },
      "source": [
        "## 设置 OpenAI API 客户端\n",
        "我们初始化 OpenAI 客户端以生成嵌入和响应。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "DfuPFDs6nbUJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "djwiDbVKmrI8"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZOaTQL0mrI8"
      },
      "source": [
        "## 实现查询转换\n",
        "### 1. Query Rewriting 查询改写\n",
        "这种技术使查询更具针对性和详细性，以提高检索的精度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VCchlCvFmrI8"
      },
      "outputs": [],
      "source": [
        "def rewrite_query(original_query, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original user query\n",
        "        model (str): The model to use for query rewriting\n",
        "\n",
        "    Returns:\n",
        "        str: The rewritten query\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"\"\"\n",
        "    You are an AI assistant specialized in improving search queries.\n",
        "    Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the user prompt with the original query to be rewritten\n",
        "    user_prompt = f\"\"\"\n",
        "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Rewritten query:\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the rewritten query using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0.0,  # 更低的温度用于确定性输出\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE6YJxEKmrI8"
      },
      "source": [
        "### 2. Step-back Prompting\n",
        "这种技术生成更宽泛的查询，以获取上下文背景信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VLOKFymAmrI8"
      },
      "outputs": [],
      "source": [
        "def generate_step_back_query(original_query, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generates a more general 'step-back' query to retrieve broader context.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original user query\n",
        "        model (str): The model to use for step-back query generation\n",
        "\n",
        "    Returns:\n",
        "        str: The step-back query\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
        "\n",
        "    # Define the user prompt with the original query to be generalized\n",
        "    user_prompt = f\"\"\"\n",
        "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Step-back query:\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the step-back query using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0.1,  # 略高的温度以产生一些变化\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the step-back query, stripping any leading/trailing whitespace\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RIuth4PmrI8"
      },
      "source": [
        "### 3. 子查询分解\n",
        "这种技术将复杂查询分解为更简单的组成部分，以便进行全面检索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BHCQQQUkmrI8"
      },
      "outputs": [],
      "source": [
        "def decompose_query(original_query, num_subqueries=4, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Decomposes a complex query into simpler sub-queries.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original complex query\n",
        "        num_subqueries (int): Number of sub-queries to generate\n",
        "        model (str): The model to use for query decomposition\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of simpler sub-queries\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
        "\n",
        "    # Define the user prompt with the original query to be decomposed\n",
        "    user_prompt = f\"\"\"\n",
        "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
        "    1. [First sub-query]\n",
        "    2. [Second sub-query]\n",
        "    And so on...\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the sub-queries using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0.2,  # Slightly higher temperature for some variation\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Process the response to extract sub-queries\n",
        "    content = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract numbered queries using simple parsing\n",
        "    lines = content.split(\"\\n\")\n",
        "    sub_queries = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
        "            # Remove the number and leading space\n",
        "            query = line.strip()\n",
        "            query = query[query.find(\".\")+1:].strip()\n",
        "            sub_queries.append(query)\n",
        "\n",
        "    return sub_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGywyTlJmrI9"
      },
      "source": [
        "## 展示查询转换技术"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9JVZhfgJmrI9",
        "outputId": "af308fca-0ebf-4c5c-9ca7-3d3ed667274c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Query: What are the impacts of AI on job automation and employment?\n",
            "\n",
            "1. Rewritten Query:\n",
            "What specific industries are most affected by the impacts of AI on job automation and employment? How does AI lead to the displacement of certain job roles and what new job opportunities emerge in the context of AI-driven job automation? In which geographical regions are the impacts of AI on job automation and employment most pronounced? What are the long-term and short-term effects of AI on job automation and employment for different skill levels?\n",
            "\n",
            "2. Step-back Query:\n",
            "What are the general impacts of emerging technologies on job automation and employment?\n",
            "\n",
            "3. Sub-queries:\n",
            "   1. What are the specific jobs that are likely to be automated by AI?\n",
            "   2. How does AI lead to the automation of jobs?\n",
            "   3. What are the short-term impacts of AI on employment?\n",
            "   4. What are the long-term impacts of AI on employment?\n"
          ]
        }
      ],
      "source": [
        "# Example query\n",
        "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
        "\n",
        "# Apply query transformations\n",
        "print(\"Original Query:\", original_query)\n",
        "\n",
        "# Query Rewriting\n",
        "rewritten_query = rewrite_query(original_query)\n",
        "print(\"\\n1. Rewritten Query:\")\n",
        "print(rewritten_query)\n",
        "\n",
        "# Step-back Prompting\n",
        "step_back_query = generate_step_back_query(original_query)\n",
        "print(\"\\n2. Step-back Query:\")\n",
        "print(step_back_query)\n",
        "\n",
        "# Sub-query Decomposition\n",
        "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
        "print(\"\\n3. Sub-queries:\")\n",
        "for i, query in enumerate(sub_queries, 1):\n",
        "    print(f\"   {i}. {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58PUvvlxmrI9"
      },
      "source": [
        "## 构建一个简单的向量存储\n",
        "为了展示查询转换如何与检索集成，让我们实现一个简单的向量存储。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XD0Bs_BEmrI9"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Compute cosine similarity between query vector and stored vector\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the corresponding text\n",
        "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k similar items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhyvwOoSmrI-"
      },
      "source": [
        "## 创建嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f4Wem1GKmrI-"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(input, model=\"doubao-embedding-text-240715\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text using the specified OpenAI model.\n",
        "\n",
        "    Args:\n",
        "    input (str | list[str]): The input text for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "\n",
        "    Returns:\n",
        "    list[float | list[float]]: The embedding vector.\n",
        "    \"\"\"\n",
        "    def batch_read(lst, batch_size=10):\n",
        "      for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i : i + batch_size]\n",
        "\n",
        "    # Handle both string and list inputs by converting string input to a list\n",
        "    input_texts = input if isinstance(input, list) else [input]\n",
        "\n",
        "    # returned embeddings\n",
        "    embeddings = []\n",
        "\n",
        "    for batch in batch_read(input_texts):\n",
        "      # Create embeddings for the input text using the specified model\n",
        "      response = client.embeddings.create(\n",
        "          model=model,\n",
        "          input=input_texts\n",
        "      )\n",
        "      embeddings.extend(item.embedding for item in response.data)\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHHbdqZOmrI-"
      },
      "source": [
        "## 实现具有查询转换的 RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 抽取文本"
      ],
      "metadata": {
        "id": "3mCy77NuyHJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0o4_4MwqmrI-"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 分块"
      ],
      "metadata": {
        "id": "N97KFwT-yKzB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aaoThQSdmrI_"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 处理文档并生成向量数据库"
      ],
      "metadata": {
        "id": "lk3EHCJkx0A1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f2YDyjKqmrI_"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for RAG, return a vector store\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    # Create embeddings for all chunks at once for efficiency\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Create vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add chunks to vector store\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return store"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 查询"
      ],
      "metadata": {
        "id": "oWsgRQSixxr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UFcXYCx0mrI_"
      },
      "outputs": [],
      "source": [
        "def transformed_search(query, vector_store, transformation_type = None, top_k=3):\n",
        "    \"\"\"\n",
        "    Search using a transformed query.\n",
        "\n",
        "    Args:\n",
        "        query (str): Original query\n",
        "        vector_store (SimpleVectorStore): Vector store to search\n",
        "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
        "        top_k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Search results\n",
        "    \"\"\"\n",
        "    print(f\"Transformation type: {transformation_type}\")\n",
        "    print(f\"Original query: {query}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if transformation_type == \"rewrite\":\n",
        "        # Query rewriting\n",
        "        transformed_query = rewrite_query(query)\n",
        "        print(f\"Rewritten query: {transformed_query}\")\n",
        "\n",
        "        # Create embedding for transformed query\n",
        "        query_embedding = create_embeddings(transformed_query)\n",
        "\n",
        "        # Search with rewritten query\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    elif transformation_type == \"step_back\":\n",
        "        # Step-back prompting\n",
        "        transformed_query = generate_step_back_query(query)\n",
        "        print(f\"Step-back query: {transformed_query}\")\n",
        "\n",
        "        # Create embedding for transformed query\n",
        "        query_embedding = create_embeddings(transformed_query)\n",
        "\n",
        "        # Search with step-back query\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    elif transformation_type == \"decompose\":\n",
        "        # Sub-query decomposition\n",
        "        sub_queries = decompose_query(query)\n",
        "        print(\"Decomposed into sub-queries:\")\n",
        "        for i, sub_q in enumerate(sub_queries, 1):\n",
        "            print(f\"{i}. {sub_q}\")\n",
        "\n",
        "        # Create embeddings for all sub-queries\n",
        "        sub_query_embeddings = create_embeddings(sub_queries)\n",
        "\n",
        "        # Search with each sub-query and combine results\n",
        "        all_results = []\n",
        "        for i, embedding in enumerate(sub_query_embeddings):\n",
        "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
        "            all_results.extend(sub_results)\n",
        "\n",
        "        # Remove duplicates (keep highest similarity score)\n",
        "        seen_texts = {}\n",
        "        for result in all_results:\n",
        "            text = result[\"text\"]\n",
        "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
        "                seen_texts[text] = result\n",
        "\n",
        "        # Sort by similarity and take top_k\n",
        "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
        "\n",
        "    else:\n",
        "        # Regular search without transformation\n",
        "        query_embedding = create_embeddings(query)\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSwMWg4EmrI_"
      },
      "source": [
        "### 5. 使用转换后的查询生成响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "anYQhYEGmrJA"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Retrieved context\n",
        "        model (str): The model to use for response generation\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
        "\n",
        "    # Define the user prompt with the context and query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,  # Low temperature for deterministic output\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the generated response, stripping any leading/trailing whitespace\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjcvJ_QhmrJA"
      },
      "source": [
        "### 6. 运行 RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2WEaBT7nmrJA"
      },
      "outputs": [],
      "source": [
        "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
        "    \"\"\"\n",
        "    Run complete RAG pipeline with optional query transformation.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, transformed query, context, and response\n",
        "    \"\"\"\n",
        "    # Process the document to create a vector store\n",
        "    vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Apply query transformation and search\n",
        "    if transformation_type:\n",
        "        # Perform search with transformed query\n",
        "        results = transformed_search(query, vector_store, transformation_type)\n",
        "    else:\n",
        "        # Perform regular search without transformation\n",
        "        query_embedding = create_embeddings(query)\n",
        "        results = vector_store.similarity_search(query_embedding, k=3)\n",
        "\n",
        "    # Combine context from search results\n",
        "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
        "\n",
        "    # Generate response based on the query and combined context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the results including original query, transformation type, context, and response\n",
        "    return {\n",
        "        \"original_query\": query,\n",
        "        \"transformation_type\": transformation_type,\n",
        "        \"context\": context,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFtTOJf_mrJA"
      },
      "source": [
        "### 7. 评估查询转换"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jg0rlgccmrJA"
      },
      "outputs": [],
      "source": [
        "def compare_responses(results, reference_answer, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Compare responses from different query transformation techniques.\n",
        "\n",
        "    Args:\n",
        "        results (Dict): Results from different transformation techniques\n",
        "        reference_answer (str): Reference answer for comparison\n",
        "        model (str): Model for evaluation\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
        "    Your task is to compare different responses generated using various query transformation techniques\n",
        "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
        "\n",
        "    # Prepare the comparison text with the reference answer and responses from each technique\n",
        "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
        "\n",
        "    for technique, result in results.items():\n",
        "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
        "\n",
        "    # Define the user prompt with the comparison text\n",
        "    user_prompt = f\"\"\"\n",
        "    {comparison_text}\n",
        "\n",
        "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
        "\n",
        "    For each technique (original, rewrite, step_back, decompose):\n",
        "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
        "    2. Identify strengths and weaknesses\n",
        "\n",
        "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the evaluation response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"=============================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aBYYzL9rmrJB"
      },
      "outputs": [],
      "source": [
        "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluate different transformation techniques for the same query.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): Query to evaluate\n",
        "        reference_answer (str): Optional reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    # Define the transformation techniques to evaluate\n",
        "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
        "    results = {}\n",
        "\n",
        "    # Run RAG with each transformation technique\n",
        "    for transformation_type in transformation_types:\n",
        "        type_name = transformation_type if transformation_type else \"original\"\n",
        "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
        "\n",
        "        # Get the result for the current transformation type\n",
        "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
        "        results[type_name] = result\n",
        "\n",
        "        # Print the response for the current transformation type\n",
        "        print(f\"Response with {type_name} query:\")\n",
        "        print(result[\"response\"])\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    # Compare results if a reference answer is provided\n",
        "    if reference_answer:\n",
        "        compare_responses(results, reference_answer)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvIhQyfVmrJB"
      },
      "source": [
        "开始评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sF1S_S_imrJB",
        "outputId": "e48c473d-2ad0-46fb-e2cb-9117e0ca3f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running RAG with original query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Response with original query:\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It involves developing techniques to provide insights into how AI models make decisions. This is important because it enhances trust and accountability. By making AI systems understandable, users can assess their fairness and accuracy. It helps address concerns about the potential for unintended consequences and builds confidence in the use of AI. XAI is crucial in various aspects such as ensuring ethical behavior, establishing accountability and responsibility for AI systems, and enabling users to have more control and agency in their interactions with AI.\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with rewrite query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: rewrite\n",
            "Original query: What is 'Explainable AI' and why is it considered important?\n",
            "Rewritten query: What are the key principles and techniques of Explainable AI? And in what specific fields or industries is it considered important, such as healthcare, finance, or transportation? Additionally, how does Explainable AI differ from traditional AI in terms of its ability to provide clear explanations and interpretations?\n",
            "Response with rewrite query:\n",
            "Explainable AI (XAI) techniques aim to make AI decisions more understandable. It is important because it enables users to assess the fairness and accuracy of AI. Making AI systems understandable and providing insights into their decision-making processes helps users assess their reliability and fairness. It is key to building trust in AI as it enhances trust and accountability. XAI is being developed to provide insights into how AI models make decisions, which is crucial for various aspects such as ensuring the robustness and reliability of AI systems and empowering users with control and agency over AI.\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with step_back query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: step_back\n",
            "Original query: What is 'Explainable AI' and why is it considered important?\n",
            "Step-back query: What is 'AI' in general and why is the concept of explainability considered important in the context of AI?\n",
            "Response with step_back query:\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It involves developing techniques to provide insights into how AI models make decisions. This is considered important because it enhances trust and accountability. By making AI systems understandable, users can assess their reliability and fairness. It helps in addressing concerns related to the potential for unintended consequences and builds confidence in the use of AI. XAI is crucial for ensuring that users have a clear understanding of how AI is making decisions and can make informed decisions themselves. It also plays a significant role in ethical design and development of AI systems by allowing for ethical impact assessments and adherence to ethical guidelines.\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with decompose query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: decompose\n",
            "Original query: What is 'Explainable AI' and why is it considered important?\n",
            "Decomposed into sub-queries:\n",
            "1. What is 'Explainable AI'?\n",
            "2. Why is 'Explainable AI' considered important in the field of artificial intelligence?\n",
            "3. What are the key features or characteristics of 'Explainable AI'?\n",
            "4. How does 'Explainable AI' differ from other types of AI?\n",
            "Response with decompose query:\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It aims to provide insights into how AI models make decisions, enhancing trust and accountability. XAI techniques are being developed to enable users to assess the fairness and accuracy of AI decisions. It is important because it helps build trust in AI by making the systems understandable and providing insights into their decision-making processes. This allows users to assess the reliability and fairness of AI, which is crucial for addressing potential harms and ensuring ethical behavior. It also helps in establishing accountability and responsibility for AI systems by defining roles and responsibilities for developers, deployers, and users. Continued advancements in XAI are expected to drive further breakthroughs in AI and improve the interpretability of deep learning models.\n",
            "==================================================\n",
            "\n",
            "===== EVALUATION RESULTS =====\n",
            "| Technique | Score | Strengths | Weaknesses |\n",
            "|--|--|--|--|\n",
            "| Original | 8 | - Accurate and comprehensive in explaining the purpose and importance of XAI. <br> - Covers various aspects such as trust, accountability, and fairness. | - The language might be a bit more complex in some parts. |\n",
            "| Rewrite | 7 | - Good at emphasizing the aim of making AI decisions understandable. <br> - Mentions key points like reliability and fairness. | - May not be as detailed as the original in some areas. |\n",
            "| Step_back | 8 | - Focuses on the user's perspective and the need for understanding AI decisions. <br> - Highlights ethical aspects well. | - Slightly less detailed in explaining the technical aspects compared to the original. |\n",
            "| Decompose | 7 | - Breaks down the concepts well and explains different aspects separately. <br> - Mentions accountability and responsibility clearly. | - Might seem a bit fragmented compared to the original. |\n",
            "\n",
            "Ranking from best to worst: Original, Step_back, Rewrite, Decompose.\n",
            "\n",
            "The original technique performed best overall because it provides a more detailed and comprehensive explanation of XAI, covering multiple aspects with clarity. It gives a well-rounded view of the topic without sacrificing accuracy. While the other techniques have their own strengths, they lack the level of detail and comprehensiveness of the original response. \n",
            "=============================\n"
          ]
        }
      ],
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('./data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"./data/AI_Information.pdf\"\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CSu_sX0y5Ii"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}