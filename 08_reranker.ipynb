{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/08_reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "GSALf6EZ19fS"
      },
      "source": [
        "# 用于增强 RAG 系统的重排序\n",
        "\n",
        "本笔记本实现了重排序技术，以提高 RAG 系统中的检索质量。重排序是初始检索之后的第二个过滤步骤，确保用于响应生成的内容是最相关的。\n",
        "\n",
        "## 重排序的关键概念\n",
        "\n",
        "1. **初始检索**：使用基本相似性搜索进行第一轮检索（速度较快但准确性较低）\n",
        "2. **文档评分**：评估每个检索到的文档与查询的相关性\n",
        "3. **重新排序**：根据相关性分数对文档进行排序\n",
        "4. **选择**：仅使用最相关的文档进行响应生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-QJkYla19fT"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "DJbX5MqM9j7F",
        "outputId": "d1feba6c-8bb1-41d3-d73a-ec8a267b5817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zhOgGLBd19fU"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho0iQQnR19fU"
      },
      "source": [
        "## 抽取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aiFRFGrt19fU"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvKVHR8919fV"
      },
      "source": [
        "## 分块\n",
        "一旦我们提取了文本，我们会将其分成更小的、有重叠部分的块，以提高检索的准确性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_u6s7iAr19fV"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcO7IhjM19fV"
      },
      "source": [
        "## OpenAI API Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "8Ntx4U6n9wA2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nf8ISO4N19fV"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkFm208119fV"
      },
      "source": [
        "## 构建向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lXUTsJjm19fV"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Compute cosine similarity between query vector and stored vector\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the corresponding text\n",
        "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k similar items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8uAHbr_19fW"
      },
      "source": [
        "## 创建嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pEt_hwmH19fW"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(input, model=\"doubao-embedding-text-240715\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given input using the specified OpenAI model.\n",
        "\n",
        "    Args:\n",
        "    input (str | list[str]): The input for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "\n",
        "    Returns:\n",
        "    List[float | list[float]]: The embedding vector.\n",
        "    \"\"\"\n",
        "    def batch_read(lst, batch_size=10):\n",
        "      for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i : i + batch_size]\n",
        "\n",
        "    # Handle both string and list inputs by converting string input to a list\n",
        "    input_texts = input if isinstance(input, list) else [input]\n",
        "\n",
        "    # returned embeddings\n",
        "    embeddings = []\n",
        "\n",
        "    for batch in batch_read(input_texts):\n",
        "      # Create embeddings for the input text using the specified model\n",
        "      response = client.embeddings.create(\n",
        "          model=model,\n",
        "          input=batch\n",
        "      )\n",
        "      embeddings.extend(item.embedding for item in response.data)\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI2wNXwU19fW"
      },
      "source": [
        "## 文档处理流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZjUgBP5t19fW"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for RAG, and then return a vectorstore which has stored the document.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the extracted text\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Create embeddings for the text chunks\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Initialize a simple vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add each chunk and its embedding to the vector store\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1JWN77i19fW"
      },
      "source": [
        "## 实现基于大语言模型的重排序"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AifX27DM19fW"
      },
      "outputs": [],
      "source": [
        "def rerank_with_llm(query, results, top_n=3, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Reranks search results using LLM relevance scoring.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Initial search results\n",
        "        top_n (int): Number of results to return after reranking\n",
        "        model (str): Model to use for scoring\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Reranked results\n",
        "    \"\"\"\n",
        "    print(f\"Reranking {len(results)} documents...\")  # Print the number of documents to be reranked\n",
        "\n",
        "    scored_results = []  # Initialize an empty list to store scored results\n",
        "\n",
        "    # Define the system prompt for the LLM\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\n",
        "Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
        "\n",
        "Guidelines:\n",
        "- Score 0-2: Document is completely irrelevant\n",
        "- Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
        "- Score 6-8: Document is relevant and partially answers the query\n",
        "- Score 9-10: Document is highly relevant and directly answers the query\n",
        "\n",
        "You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n",
        "\n",
        "    # Iterate through each result\n",
        "    for i, result in enumerate(results):\n",
        "        # Show progress every 5 documents\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
        "\n",
        "        # Define the user prompt for the LLM\n",
        "        user_prompt = f\"\"\"\n",
        "Query: {query}\n",
        "\n",
        "Document:\n",
        "{result['text']}\n",
        "\n",
        "Rate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n",
        "\n",
        "        # Get the LLM response\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract the score from the LLM response\n",
        "        score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Use regex to extract the numerical score\n",
        "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
        "        if score_match:\n",
        "            score = float(score_match.group(1))\n",
        "        else:\n",
        "            # If score extraction fails, use similarity score as fallback\n",
        "            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
        "            score = result[\"similarity\"] * 10\n",
        "\n",
        "        # Append the scored result to the list\n",
        "        scored_results.append({\n",
        "            \"text\": result[\"text\"],\n",
        "            \"metadata\": result[\"metadata\"],\n",
        "            \"similarity\": result[\"similarity\"],\n",
        "            \"relevance_score\": score\n",
        "        })\n",
        "\n",
        "    # Sort results by relevance score in descending order\n",
        "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top_n results\n",
        "    return reranked_results[:top_n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flFn-VDs19fX"
      },
      "source": [
        "## 简单的基于关键词的重排序"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "otjNvNQU19fX"
      },
      "outputs": [],
      "source": [
        "def rerank_with_keywords(query, results, top_n=3):\n",
        "    \"\"\"\n",
        "    A simple alternative reranking method based on keyword matching and position.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Initial search results\n",
        "        top_n (int): Number of results to return after reranking\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Reranked results\n",
        "    \"\"\"\n",
        "    # Extract important keywords from the query\n",
        "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
        "\n",
        "    scored_results = []  # Initialize a list to store scored results\n",
        "\n",
        "    for result in results:\n",
        "        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n",
        "\n",
        "        # Base score starts with vector similarity\n",
        "        base_score = result[\"similarity\"] * 0.5\n",
        "\n",
        "        # Initialize keyword score\n",
        "        keyword_score = 0\n",
        "        for keyword in keywords:\n",
        "            if keyword in document_text:\n",
        "                # Add points for each keyword found\n",
        "                keyword_score += 0.1\n",
        "\n",
        "                # Add more points if keyword appears near the beginning\n",
        "                first_position = document_text.find(keyword)\n",
        "                if first_position < len(document_text) / 4:  # In the first quarter of the text\n",
        "                    keyword_score += 0.1\n",
        "\n",
        "                # Add points for keyword frequency\n",
        "                frequency = document_text.count(keyword)\n",
        "                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n",
        "\n",
        "        # Calculate the final score by combining base score and keyword score\n",
        "        final_score = base_score + keyword_score\n",
        "\n",
        "        # Append the scored result to the list\n",
        "        scored_results.append({\n",
        "            \"text\": result[\"text\"],\n",
        "            \"metadata\": result[\"metadata\"],\n",
        "            \"similarity\": result[\"similarity\"],\n",
        "            \"relevance_score\": final_score\n",
        "        })\n",
        "\n",
        "    # Sort results by final relevance score in descending order\n",
        "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top_n results\n",
        "    return reranked_results[:top_n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64q5J2Y019fX"
      },
      "source": [
        "## 响应生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dd9nMilr19fX"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Retrieved context\n",
        "        model (str): Model to use for response generation\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
        "\n",
        "    # Create the user prompt by combining the context and query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIUx7zF019fX"
      },
      "source": [
        "## 带有重排序的完整 RAG 流程\n",
        "到目前为止，我们已经实现了 RAG 流程的核心组件，包括文档处理、问答和重排序。现在，我们将把这些组件结合起来，创建一个完整的 RAG 流程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GqtUQXVl19fX"
      },
      "outputs": [],
      "source": [
        "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline incorporating reranking.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        reranking_method (str): Method for reranking ('llm' or 'keywords')\n",
        "        top_n (int): Number of results to return after reranking\n",
        "        model (str): Model for response generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, context, and response\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Initial retrieval (get more than we need for reranking)\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
        "\n",
        "    # Apply reranking\n",
        "    if reranking_method == \"llm\":\n",
        "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
        "    elif reranking_method == \"keywords\":\n",
        "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
        "    else:\n",
        "        # No reranking, just use top results from initial retrieval\n",
        "        reranked_results = initial_results[:top_n]\n",
        "\n",
        "    # Combine context from reranked results\n",
        "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "\n",
        "    # Generate response based on context\n",
        "    response = generate_response(query, context, model)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"reranking_method\": reranking_method,\n",
        "        \"initial_results\": initial_results[:top_n],\n",
        "        \"reranked_results\": reranked_results,\n",
        "        \"context\": context,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vzkVmuu19fX"
      },
      "source": [
        "## 评估重排质量"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zq5gUbH0DCBS",
        "outputId": "9020cfed-1a5e-494b-8c1e-5a352baeba6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tqI9Xn2b19fX"
      },
      "outputs": [],
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('./drive/MyDrive/colab_data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mGzBPJpH19fX",
        "outputId": "1b488a97-d78f-45ee-e616-ca52ea3c89d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Comparing retrieval methods...\n",
            "\n",
            "=== STANDARD RETRIEVAL ===\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "The context shows that AI is having a significant impact in various fields such as management, algorithmic trading, customer service, and many others. In industries with repetitive or routine tasks, there are concerns about job displacement but also new opportunities and the need for reskilling. AI is used in areas like supply chain operations, human resources, marketing and sales, financial services, healthcare, transportation, and retail. It can analyze large datasets, predict market movements, automate processes, and enhance decision-making. All these examples suggest that AI has the potential to transform the way we live and work by improving efficiency, providing new insights, and creating new job roles that require skills in working with AI systems. It is likely to play an increasingly important role in shaping our future lives and work. \n",
            "\n",
            "=== LLM-BASED RERANKING ===\n",
            "Reranking 10 documents...\n",
            "Scoring document 1/10...\n",
            "Scoring document 6/10...\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "AI has the potential to transform the way we live and work in multiple ways. In various fields such as supply chain operations, it predicts demand, manages inventory, and streamlines logistics, improving efficiency and reducing waste. In HR, it automates recruitment processes, personalizes training, and provides insights for employee engagement and retention. In marketing and sales, it analyzes customer data, personalizes campaigns, and predicts sales trends. In financial services, it detects fraud, manages risk, and automates processes. In medicine, it assists in diagnosis, drug discovery, and personalized medicine. In transportation, it leads to the development of self-driving cars and traffic optimization systems. In retail, it offers personalized recommendations and optimizes supply chain. In manufacturing, it enhances production processes.\n",
            "\n",
            "However, the increasing capabilities of AI also raise concerns about job displacement, especially in industries with repetitive or routine tasks. But it also creates new opportunities and transforms existing roles. Reskilling and upskilling initiatives are needed to address these potential impacts. The future of work is likely to involve increased collaboration between humans and AI systems, where AI tools augment human capabilities and provide support for decision-making. Overall, AI has the potential to bring significant changes and improvements to our lives and work. \n",
            "\n",
            "=== KEYWORD-BASED RERANKING ===\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "The context mentions several aspects related to AI. It discusses how AI systems are \"black boxes\" making it hard to understand their decision-making processes but emphasizes the importance of enhancing transparency and explainability for building trust. It also highlights concerns about privacy and security with AI relying on large amounts of data. Regarding job displacement, it is noted that AI's automation capabilities raise such concerns, especially in industries with repetitive tasks. However, it also mentions that AI creates new opportunities and transforms existing roles. Additionally, it talks about human-AI collaboration in the future of work, where AI tools can augment human capabilities and provide insights. There is also mention of new job roles emerging from AI development. Overall, the context suggests that AI has the potential to transform the way we live and work by creating new opportunities, transforming existing jobs, and enabling human-AI collaboration. \n"
          ]
        }
      ],
      "source": [
        "# Process document\n",
        "vector_store = process_document(pdf_path)\n",
        "\n",
        "# Example query\n",
        "query = \"Does AI have the potential to transform the way we live and work?\"\n",
        "\n",
        "# Compare different methods\n",
        "print(\"Comparing retrieval methods...\")\n",
        "\n",
        "# 1. Standard retrieval (no reranking)\n",
        "print(\"\\n=== STANDARD RETRIEVAL ===\")\n",
        "standard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{standard_results['response']}\")\n",
        "\n",
        "# 2. LLM-based reranking\n",
        "print(\"\\n=== LLM-BASED RERANKING ===\")\n",
        "llm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{llm_results['response']}\")\n",
        "\n",
        "# 3. Keyword-based reranking\n",
        "print(\"\\n=== KEYWORD-BASED RERANKING ===\")\n",
        "keyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{keyword_results['response']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GB1PQuMr19fY"
      },
      "outputs": [],
      "source": [
        "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluates the quality of reranked results compared to standard results.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        standard_results (Dict): Results from standard retrieval\n",
        "        reranked_results (Dict): Results from reranked retrieval\n",
        "        reference_answer (str, optional): Reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation output\n",
        "    \"\"\"\n",
        "    # Define the system prompt for the AI evaluator\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
        "    Compare the retrieved contexts and responses from two different retrieval methods.\n",
        "    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n",
        "\n",
        "    # Prepare the comparison text with truncated contexts and responses\n",
        "    comparison_text = f\"\"\"Query: {query}\n",
        "\n",
        "Standard Retrieval Context:\n",
        "{standard_results['context'][:1000]}... [truncated]\n",
        "\n",
        "Standard Retrieval Answer:\n",
        "{standard_results['response']}\n",
        "\n",
        "Reranked Retrieval Context:\n",
        "{reranked_results['context'][:1000]}... [truncated]\n",
        "\n",
        "Reranked Retrieval Answer:\n",
        "{reranked_results['response']}\"\"\"\n",
        "\n",
        "    # If a reference answer is provided, include it in the comparison text\n",
        "    if reference_answer:\n",
        "        comparison_text += f\"\"\"\n",
        "\n",
        "Reference Answer:\n",
        "{reference_answer}\"\"\"\n",
        "\n",
        "    # Create the user prompt for the AI evaluator\n",
        "    user_prompt = f\"\"\"\n",
        "{comparison_text}\n",
        "\n",
        "Please evaluate which retrieval method provided:\n",
        "1. More relevant context\n",
        "2. More accurate answer\n",
        "3. More comprehensive answer\n",
        "4. Better overall performance\n",
        "\n",
        "Provide a detailed analysis with specific examples.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate the evaluation response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"doubao-lite-128k-240828\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Return the evaluation output\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "S_Ej48P519fY",
        "outputId": "e81e638f-f252-438b-c588-0a4bf2eae1b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "1. **More relevant context**: Both retrieval methods provide relevant context as they both discuss various aspects of how AI can transform different fields such as management, trading, customer service, supply chain, HR, marketing, etc. However, the reranked retrieval context seems to have a slightly more focused set of examples within each field. For example, in the supply chain section, it specifically mentions predicting demand, managing inventory, and streamlining logistics, which gives a more detailed picture of AI's impact in that area compared to the standard retrieval context which just mentions it in a more general way.\n",
            "2. **More accurate answer**: Both answers are accurate as they convey the main points about how AI has the potential to transform various aspects of life and work, including the concerns about job displacement and the need for reskilling. There isn't a significant difference in accuracy between the two.\n",
            "3. **More comprehensive answer**: The reranked retrieval answer is more comprehensive. It provides more detailed examples within each field (e.g. in HR - automating recruitment, personalizing training; in marketing - analyzing customer data, predicting sales trends) and also mentions additional fields like medicine, transportation, and manufacturing where AI can have an impact. This gives a more holistic view of the potential of AI to transform different areas of life and work.\n",
            "4. **Better overall performance**: The reranked retrieval method performs better overall. It offers more detailed and specific context within each field, which makes the answer more comprehensive and easier to understand. It provides a more in-depth exploration of how AI is transforming different aspects of life and work, enhancing the overall quality and usefulness of the information.\n",
            "\n",
            "In conclusion, while both retrieval methods are good, the reranked retrieval method provides more relevant context, a more comprehensive answer, and better overall performance. \n"
          ]
        }
      ],
      "source": [
        "# Evaluate the quality of reranked results compared to standard results\n",
        "evaluation = evaluate_reranking(\n",
        "    query=query,  # The user query\n",
        "    standard_results=standard_results,  # Results from standard retrieval\n",
        "    reranked_results=llm_results,  # Results from LLM-based reranking\n",
        "    reference_answer=reference_answer  # Reference answer for comparison\n",
        ")\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "print(evaluation)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}