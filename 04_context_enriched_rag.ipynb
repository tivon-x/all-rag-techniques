{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "-yE7IPGGT7sP"
      },
      "source": [
        "## RAG 中的上下文增强检索\n",
        "检索增强生成（RAG）通过从外部来源检索相关知识来增强 AI 响应。传统检索方法返回孤立的文本块，可能导致答案不完整。\n",
        "\n",
        "为了解决这一问题，我们引入了上下文增强检索，它确保检索到的信息包括相邻的块，以提高连贯性。\n",
        "\n",
        "本笔记本中的步骤：\n",
        "- 数据摄取：从 PDF 中提取文本。\n",
        "- 带上下文重叠的分块：将文本分割成带有重叠的块，以保留上下文。\n",
        "- 嵌入创建：将文本块转换为数值表示。\n",
        "- 上下文感知检索：检索相关块及其相邻块，以提高完整性。\n",
        "- 响应生成：使用语言模型根据检索到的上下文生成响应。\n",
        "- 评估：评估模型的响应准确性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9icII9kT7sQ"
      },
      "source": [
        "## Setting Up the Environment\n",
        "We begin by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "W6yLK8hWU5Qw",
        "outputId": "2d59a3f1-538f-4faf-f65a-7e3ae92baada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dqRjeORjT7sR"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psgux-lRT7sR"
      },
      "source": [
        "## 从 PDF 文件中提取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EM2jj_j2T7sR"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z15f2mE_T7sR"
      },
      "source": [
        "## 对提取的文本进行分块\n",
        "在提取文本后，我们将文本划分为更小的、有重叠的部分，以提高检索的准确性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KUC1b442T7sS"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bwt83S6T7sS"
      },
      "source": [
        "## OpenAI 客户端"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "AaxvrI7nc9tc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bm2MGyLBT7sS"
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI client with the base URL and API key\n",
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdaCAt3ST7sS"
      },
      "source": [
        "## 从 PDF 文件中提取和分块文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F0-RPJVgT7sS",
        "outputId": "a51af206-205e-432b-f653-7e863ccb0445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text chunks: 42\n",
            "\n",
            "First text chunk:\n",
            "Understanding Artificial Intelligence \n",
            "Chapter 1: Introduction to Artificial Intelligence \n",
            "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
            "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
            "the project of developing systems endowed with the intellectual processes characteristic of \n",
            "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
            "experience. Over the past few decades, advancements in computing power and data availability \n",
            "have significantly accelerated the development and deployment of AI. \n",
            "Historical Context \n",
            "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
            "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
            "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
            "and symbolic methods. The 1980s saw a rise in exp\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the PDF file\n",
        "pdf_path = \"./data/AI_Information.pdf\"\n",
        "\n",
        "# Extract text from the PDF file\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
        "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
        "\n",
        "# Print the number of text chunks created\n",
        "print(\"Number of text chunks:\", len(text_chunks))\n",
        "\n",
        "# Print the first text chunk\n",
        "print(\"\\nFirst text chunk:\")\n",
        "print(text_chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuTb-qWlT7sU"
      },
      "source": [
        "## 创建文本嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MF523YAZT7sU",
        "outputId": "88ca72be-3409-4b18-82eb-24e219722b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch: 5it [00:07,  1.47s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def create_embeddings(texts, model=\"doubao-embedding-text-240715\"):\n",
        "    \"\"\"\n",
        "    生成文本列表对应的嵌入向量列表.\n",
        "\n",
        "    Args:\n",
        "    texts (List[str]): 输入文本列表.\n",
        "    model (str): 嵌入模型，默认为 doubao-embedding-text-240715\n",
        "\n",
        "    Returns:\n",
        "    List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(model=model, input=texts)\n",
        "    return [np.array(embedding.embedding) for embedding in response.data]\n",
        "\n",
        "def batch_read(lst, batch_size = 10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "      yield lst[i : i + batch_size]\n",
        "\n",
        "embeddings = []\n",
        "for batch in tqdm(batch_read(text_chunks), desc=\"batch\"):\n",
        "  embeddings.extend(create_embeddings(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWiNFX_AT7sV"
      },
      "source": [
        "## 实现上下文感知语义搜索\n",
        "我们修改检索方式，包括相邻块以提供更好的上下文。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fvs5K3ooT7sV"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "    vec1 (np.ndarray): The first vector.\n",
        "    vec2 (np.ndarray): The second vector.\n",
        "\n",
        "    Returns:\n",
        "    float: The cosine similarity between the two vectors.\n",
        "    \"\"\"\n",
        "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UXoNSy95T7sV"
      },
      "outputs": [],
      "source": [
        "def context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1):\n",
        "    \"\"\"\n",
        "    检索最相关的块及其相邻块。\n",
        "\n",
        "    Args:\n",
        "    query (str): 查询.\n",
        "    text_chunks (List[str]): 文本块列表.\n",
        "    embeddings (List[np.ndarray]): 文本块嵌入列表.\n",
        "    k (int): 需要检索的相关文本块数量.\n",
        "    context_size (int): 要包含的相邻块数量，前后各取context_size的文本块\n",
        "\n",
        "    Returns:\n",
        "    List[str]: 带有上下文信息的相关文本块。\n",
        "    \"\"\"\n",
        "    query_embedding = create_embeddings([query])[0]\n",
        "    similarity_scores = []\n",
        "\n",
        "    # 计算查询与每个文本块嵌入之间的相似度分数\n",
        "    for i, chunk_embedding in enumerate(embeddings):\n",
        "        similarity_score = cosine_similarity(query_embedding, chunk_embedding)\n",
        "        similarity_scores.append((i, similarity_score))\n",
        "\n",
        "    # 按相似度分数降序排序（相似度最高的在前）\n",
        "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # 获取最相关的文本块的索引\n",
        "    top_index = similarity_scores[0][0]\n",
        "\n",
        "    # 定义上下文包含的范围\n",
        "    # 确保不会超出文本块的边界\n",
        "    start = max(0, top_index - context_size)\n",
        "    end = min(len(text_chunks), top_index + context_size + 1)\n",
        "\n",
        "    return [text_chunks[i] for i in range(start, end)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tP_k1RpT7sV"
      },
      "source": [
        "## 使用上下文检索运行查询\n",
        "我们现在测试上下文增强检索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TP69sUYQT7sV",
        "outputId": "1feb9b97-4468-424c-957d-4732d2ff7045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Context 1:\n",
            "inability \n",
            "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
            "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
            "crucial for building trust and accountability. \n",
            " \n",
            " \n",
            "Privacy and Security \n",
            "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
            "Protecting sensitive information and ensuring responsible data handling are essential. \n",
            "Job Displacement \n",
            "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
            "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
            "of AI-driven automation is a key challenge. \n",
            "Autonomy and Control \n",
            "As AI systems become more autonomous, questions arise about control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The p\n",
            "=====================================\n",
            "Context 2:\n",
            "control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
            "concerns. International discussions and regulations are needed to address the risks associated \n",
            "with AI-powered weapons. \n",
            "Chapter 5: The Future of Artificial Intelligence \n",
            "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
            "across various domains. Key trends and areas of development include: \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
            "techniques are being developed to provide insights into how AI models make decisions, \n",
            "enhancing trust and accountability. \n",
            "AI at the Edge \n",
            "AI at the edge involves processing data locally on devices, rather than relying on cloud-based \n",
            "servers. This approach reduc\n",
            "=====================================\n",
            "Context 3:\n",
            "odels make decisions, \n",
            "enhancing trust and accountability. \n",
            "AI at the Edge \n",
            "AI at the edge involves processing data locally on devices, rather than relying on cloud-based \n",
            "servers. This approach reduces latency, improves privacy, and enables AI applications in \n",
            "environments with limited connectivity. \n",
            "Quantum Computing and AI \n",
            "Quantum computing has the potential to significantly accelerate AI algorithms, enabling \n",
            "breakthroughs in areas such as drug discovery, materials science, and optimization. The \n",
            "intersection of quantum computing and AI is a promising area of research. \n",
            "Human-AI Collaboration \n",
            "The future of AI is likely to involve increased collaboration between humans and AI systems. This \n",
            "includes developing AI tools that augment human capabilities, support decision-making, and \n",
            "enhance productivity. \n",
            " \n",
            " \n",
            " \n",
            "AI for Social Good \n",
            "AI is increasingly being used to address social and environmental challenges, such as climate \n",
            "change, poverty, and healthcare disparities. AI for social \n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "# Load the validation dataset from a JSON file\n",
        "with open('./data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first question from the dataset to use as our query\n",
        "query = data[0]['question']\n",
        "\n",
        "# 检索最相关的块及其相邻块以提供上下文\n",
        "top_chunks = context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1)\n",
        "\n",
        "# Print the query for reference\n",
        "print(\"Query:\", query)\n",
        "# Print each retrieved chunk with a heading and separator\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-806-GllT7sW"
      },
      "source": [
        "## 使用检索到的上下文生成响应\n",
        "我们现在使用大语言模型生成响应。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mhGhKtA3T7sW"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt for the AI assistant\n",
        "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
        "\n",
        "def generate_response(system_prompt, user_message, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generates a response from the AI model based on the system prompt and user message.\n",
        "\n",
        "    Args:\n",
        "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
        "    user_message (str): The user's message or query.\n",
        "    model (str): The model to be used for generating the response. Default is \"doubao-lite-128k-240828\".\n",
        "\n",
        "    Returns:\n",
        "    dict: The response from the AI model.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Create the user prompt based on the top chunks\n",
        "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
        "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
        "\n",
        "# Generate AI response\n",
        "ai_response = generate_response(system_prompt, user_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucPP_xNTT7sW"
      },
      "source": [
        "## 评估 AI 响应\n",
        "我们将 AI 响应与预期答案进行比较，并分配一个分数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lO0oLb0hT7sW",
        "outputId": "2684f72f-6a30-4526-df21-fcea7f0ccc1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI assistant's response is very close to the true response, covering all the key points about Explainable AI (XAI) and its importance. Both responses emphasize transparency, understandability, trust, and accountability. The AI assistant's response is slightly more detailed, mentioning \"black boxes,\" which adds value but does not detract from the alignment with the true response.\n",
            "\n",
            "Score: 1\n"
          ]
        }
      ],
      "source": [
        "# Define the system prompt for the evaluation system\n",
        "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
        "\n",
        "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
        "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
        "\n",
        "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
        "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt, \"deepseek-v3-250324\")\n",
        "\n",
        "# Print the evaluation response\n",
        "print(evaluation_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmVvtiBAj24A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}