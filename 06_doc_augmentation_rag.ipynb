{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "Fkperd-y35_s"
      },
      "source": [
        "# 基于问题生成的文档增强 RAG\n",
        "\n",
        "本笔记本实现了一种通过问题生成进行文档增强的增强型 RAG 方法。通过为每个文本块生成相关问题，我们改进了检索过程，从而提高了语言模型的响应质量。\n",
        "\n",
        "在本实现中，我们遵循以下步骤：\n",
        "\n",
        "1. **数据摄取**：从 PDF 文件中提取文本。\n",
        "2. **分块**：将文本分割成可管理的块。\n",
        "3. **问题生成**：为每个块生成相关问题。\n",
        "4. **嵌入创建**：为块和生成的问题创建嵌入。\n",
        "5. **向量存储创建**：使用 NumPy 构建一个简单的向量存储。\n",
        "6. **语义搜索**：检索与用户查询相关的块和问题。\n",
        "7. **响应生成**：根据检索到的内容生成答案。\n",
        "8. **评估**：评估生成响应的质量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS7KYN4k35_t"
      },
      "source": [
        "## 环境设置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "XOTeK53SEfV_",
        "outputId": "30899f7d-856b-43f3-a032-a14d0f5fa681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NQdyDenR35_u"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_hHMeI35_v"
      },
      "source": [
        "## 从 PDF 文件中抽取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FDNJptx635_v"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgKuC9iJ35_w"
      },
      "source": [
        "## 分块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OnO9QfR035_x"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQyxcYae35_x"
      },
      "source": [
        "## OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "LRxfvgzqEvYM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rViyyX0u35_x"
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI client with the base URL and API key\n",
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ArHHkPR35_x"
      },
      "source": [
        "## 为文本块生成问题\n",
        "这是对简单 RAG 的关键增强。我们生成可以由每个文本块回答的问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OY64wKE635_y"
      },
      "outputs": [],
      "source": [
        "def generate_questions(text_chunk, num_questions=5, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    从给定的文本块中生成相关问题。\n",
        "\n",
        "    Args:\n",
        "    text_chunk (str): The text chunk to generate questions from.\n",
        "    num_questions (int): Number of questions to generate.\n",
        "    model (str): The model to use for question generation.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: List of generated questions.\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
        "\n",
        "    # Define the user prompt with the text chunk and the number of questions to generate\n",
        "    user_prompt = f\"\"\"\n",
        "    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
        "\n",
        "    {text_chunk}\n",
        "\n",
        "    Format your response as a numbered list of questions only, with no additional text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate questions using the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extract and clean questions from the response\n",
        "    questions_text = response.choices[0].message.content.strip()\n",
        "    questions = []\n",
        "\n",
        "    # Extract questions using regex pattern matching\n",
        "    for line in questions_text.split('\\n'):\n",
        "        # Remove numbering and clean up whitespace\n",
        "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
        "        if cleaned_line and cleaned_line.endswith('?'):\n",
        "            questions.append(cleaned_line)\n",
        "\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZj3L9Cu35_y"
      },
      "source": [
        "## 创建嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yFlQdKth35_y"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model=\"doubao-embedding-text-240715\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text using the specified OpenAI model.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "\n",
        "    Returns:\n",
        "    dict: The response from the OpenAI API containing the embeddings.\n",
        "    \"\"\"\n",
        "    # Create embeddings for the input text using the specified model\n",
        "    response = client.embeddings.create(\n",
        "        model=model,\n",
        "        input=text\n",
        "    )\n",
        "\n",
        "    return response  # Return the response containing the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-uhnY6d35_y"
      },
      "source": [
        "## 构建一个简单的向量存储\n",
        "我们将使用 NumPy 实现一个简单的向量存储。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "obJG1WnM35_y"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    使用 Numpy 实现的一个简单的向量存储\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))\n",
        "        self.texts.append(text)\n",
        "        self.metadata.append(metadata or {})\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],\n",
        "                \"metadata\": self.metadata[idx],\n",
        "                \"similarity\": score\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vWgMb--35_y"
      },
      "source": [
        "## 使用问题增强处理文档\n",
        "现在，我们将把所有内容整合在一起，处理文档，生成问题，并构建我们的增强型向量存储。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AWlDSFVc35_y"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):\n",
        "    \"\"\"\n",
        "    使用问题增强处理文档\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each text chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "    questions_per_chunk (int): Number of questions to generate per chunk.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[str], SimpleVectorStore]: Text chunks and vector store.\n",
        "    \"\"\"\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Chunking text...\")\n",
        "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(text_chunks)} text chunks\")\n",
        "\n",
        "    vector_store = SimpleVectorStore()\n",
        "\n",
        "    print(\"Processing chunks and generating questions...\")\n",
        "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Processing Chunks\")):\n",
        "        # Create embedding for the chunk itself\n",
        "        chunk_embedding_response = create_embeddings(chunk)\n",
        "        chunk_embedding = chunk_embedding_response.data[0].embedding\n",
        "\n",
        "        # Add the chunk to the vector store\n",
        "        vector_store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=chunk_embedding,\n",
        "            metadata={\"type\": \"chunk\", \"index\": i}\n",
        "        )\n",
        "\n",
        "        # Generate questions for this chunk\n",
        "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
        "\n",
        "        # Create embeddings for each question and add to vector store\n",
        "        for j, question in enumerate(questions):\n",
        "            question_embedding_response = create_embeddings(question)\n",
        "            question_embedding = question_embedding_response.data[0].embedding\n",
        "\n",
        "            # Add the question to the vector store\n",
        "            vector_store.add_item(\n",
        "                text=question,\n",
        "                embedding=question_embedding,\n",
        "                metadata={\"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk}\n",
        "            )\n",
        "\n",
        "    return text_chunks, vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jln0A6qI35_z"
      },
      "source": [
        "## 抽取、处理文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c51SXik835_z",
        "outputId": "1245cc7a-7e3e-4ecd-c28d-f05c00ea7007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Processing chunks and generating questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunks: 100%|██████████| 42/42 [02:15<00:00,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store contains 168 items\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the PDF file\n",
        "pdf_path = \"./data/AI_Information.pdf\"\n",
        "\n",
        "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
        "text_chunks, vector_store = process_document(\n",
        "    pdf_path,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    questions_per_chunk=3\n",
        ")\n",
        "\n",
        "print(f\"Vector store contains {len(vector_store.texts)} items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN0CI1x135_z"
      },
      "source": [
        "## 执行语义搜索\n",
        "我们实现了一个语义搜索函数，类似于简单 RAG 的实现，但适应了我们的增强型向量存储。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fsXyvUyD35_z"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, vector_store, k=5):\n",
        "    \"\"\"\n",
        "    Performs semantic search using the query and vector store.\n",
        "\n",
        "    Args:\n",
        "    query (str): The search query.\n",
        "    vector_store (SimpleVectorStore): The vector store to search in.\n",
        "    k (int): Number of results to return.\n",
        "\n",
        "    Returns:\n",
        "    List[Dict]: Top k most relevant items.\n",
        "    \"\"\"\n",
        "    # Create embedding for the query\n",
        "    query_embedding_response = create_embeddings(query)\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "\n",
        "    # Search the vector store\n",
        "    results = vector_store.similarity_search(query_embedding, k=k)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsRhU-2u35_z"
      },
      "source": [
        "## 在增强型向量存储上运行查询"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ba8jmbwa35_z",
        "outputId": "eb3b8f50-e847-48d7-f224-f4228b62f52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "\n",
            "Search Results:\n",
            "\n",
            "Relevant Document Chunks:\n",
            "\n",
            "Matched Questions:\n",
            "Question 1 (similarity: 0.9524):\n",
            "How does Explainable AI (XAI) aim to make AI systems more transparent?\n",
            "From chunk 10\n",
            "=====================================\n",
            "Question 2 (similarity: 0.9483):\n",
            "What are the focuses of research in Explainable AI (XAI)?\n",
            "From chunk 29\n",
            "=====================================\n",
            "Question 3 (similarity: 0.9441):\n",
            "What are the main aims of Explainable AI (XAI) techniques?\n",
            "From chunk 37\n",
            "=====================================\n",
            "Question 4 (similarity: 0.9376):\n",
            "What are the ethical implications of AI that need to be addressed?\n",
            "From chunk 26\n",
            "=====================================\n",
            "Question 5 (similarity: 0.9376):\n",
            "How does making AI systems understandable help build trust?\n",
            "From chunk 38\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('./data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Perform semantic search to find relevant content\n",
        "search_results = semantic_search(query, vector_store, k=5)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nSearch Results:\")\n",
        "\n",
        "# Organize results by type\n",
        "chunk_results = []\n",
        "question_results = []\n",
        "\n",
        "for result in search_results:\n",
        "    if result[\"metadata\"][\"type\"] == \"chunk\":\n",
        "        chunk_results.append(result)\n",
        "    else:\n",
        "        question_results.append(result)\n",
        "\n",
        "# Print chunk results first\n",
        "print(\"\\nRelevant Document Chunks:\")\n",
        "for i, result in enumerate(chunk_results):\n",
        "    print(f\"Context {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
        "    print(result[\"text\"][:300] + \"...\")\n",
        "    print(\"=====================================\")\n",
        "\n",
        "# Then print question matches\n",
        "print(\"\\nMatched Questions:\")\n",
        "for i, result in enumerate(question_results):\n",
        "    print(f\"Question {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
        "    print(result[\"text\"])\n",
        "    chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
        "    print(f\"From chunk {chunk_idx}\")\n",
        "    print(\"=====================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l7PDKaZ35_0"
      },
      "source": [
        "## 为响应生成上下文\n",
        "现在，我们通过结合相关块和问题的信息来准备上下文。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IKo6GbwD35_0"
      },
      "outputs": [],
      "source": [
        "def prepare_context(search_results):\n",
        "    \"\"\"\n",
        "    Prepares a unified context from search results for response generation.\n",
        "\n",
        "    Args:\n",
        "    search_results (List[Dict]): Results from semantic search.\n",
        "\n",
        "    Returns:\n",
        "    str: Combined context string.\n",
        "    \"\"\"\n",
        "    # Extract unique chunks referenced in the results\n",
        "    chunk_indices = set()\n",
        "    context_chunks = []\n",
        "\n",
        "    # First add direct chunk matches\n",
        "    for result in search_results:\n",
        "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
        "            chunk_indices.add(result[\"metadata\"][\"index\"])\n",
        "            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n",
        "\n",
        "    # Then add chunks referenced by questions\n",
        "    for result in search_results:\n",
        "        if result[\"metadata\"][\"type\"] == \"question\":\n",
        "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
        "            if chunk_idx not in chunk_indices:\n",
        "                chunk_indices.add(chunk_idx)\n",
        "                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n",
        "\n",
        "    # Combine all context chunks\n",
        "    full_context = \"\\n\\n\".join(context_chunks)\n",
        "    return full_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj1SK-YL35_0"
      },
      "source": [
        "## 基于检索到的块生成响应\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F9v4P2t335_0"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "    query (str): User's question.\n",
        "    context (str): Context information retrieved from the vector store.\n",
        "    model (str): Model to use for response generation.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated response.\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGMYAMMP35_0"
      },
      "source": [
        "## 生成并展示响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HOwpx8oK35_0",
        "outputId": "8837ac08-7142-41a6-86dc-f99ef8d637aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "\n",
            "Context: Chunk 10 (referenced by question 'How does Explainable AI (XAI) aim to make AI systems more transparent?'):\n",
            "control, accountability, and the \n",
            "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
            "AI development and deployment is crucial. \n",
            "Weaponization of AI \n",
            "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
            "concerns. International discussions and regulations are needed to address the risks associated \n",
            "with AI-powered weapons. \n",
            "Chapter 5: The Future of Artificial Intelligence \n",
            "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
            "across various domains. Key trends and areas of development include: \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
            "techniques are being developed to provide insights into how AI models make decisions, \n",
            "enhancing trust and accountability. \n",
            "AI at the Edge \n",
            "AI at the edge involves processing data locally on devices, rather than relying on cloud-based \n",
            "servers. This approach reduc\n",
            "\n",
            "Chunk 29 (referenced by question 'What are the focuses of research in Explainable AI (XAI)?'):\n",
            " incidents. \n",
            "Environmental Monitoring \n",
            "AI-powered environmental monitoring systems track air and water quality, detect pollution, and \n",
            "support environmental protection efforts. These systems provide real-time data, identify \n",
            "pollution sources, and inform environmental policies. \n",
            "Chapter 15: The Future of AI Research \n",
            "Advancements in Deep Learning \n",
            "Continued advancements in deep learning are expected to drive further breakthroughs in AI. \n",
            "Research is focused on developing more efficient and interpretable deep learning models, as well \n",
            "as exploring new architectures and training techniques. \n",
            "Explainable AI (XAI) \n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in \n",
            "XAI focuses on developing methods for explaining AI decisions, enhancing trust, and improving \n",
            "accountability. \n",
            "AI and Neuroscience \n",
            "The intersection of AI and neuroscience is a promising area of research. Understanding the \n",
            "human brain can inspire new AI algorithms and architectures, \n",
            "\n",
            "Chunk 37 (referenced by question 'What are the main aims of Explainable AI (XAI) techniques?'):\n",
            "systems. Explainable AI (XAI) \n",
            "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
            "fairness and accuracy. \n",
            "Privacy and Data Protection \n",
            "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
            "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
            "and complying with data protection regulations are crucial. \n",
            "Accountability and Responsibility \n",
            "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
            "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
            "developers, deployers, and users of AI systems. \n",
            "Chapter 20: Building Trust in AI \n",
            "Transparency and Explainability \n",
            "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
            "and providing insights into their decision-making processes helps users assess their reliability \n",
            "and fairness. \n",
            "Robustness and Reliability \n",
            "\n",
            "\n",
            "Chunk 26 (referenced by question 'What are the ethical implications of AI that need to be addressed?'):\n",
            "e recovery efforts. \n",
            "Chapter 13: The Social Impact of AI \n",
            "Addressing Societal Challenges \n",
            "AI has the potential to address significant societal challenges, such as climate change, poverty, \n",
            "and healthcare disparities. AI-powered solutions can improve resource management, enhance \n",
            "decision-making, and support sustainable development. \n",
            "AI for Social Good \n",
            "AI for social good initiatives leverage AI to tackle social and environmental problems. These \n",
            "projects focus on using AI to improve access to education, healthcare, and social services, \n",
            "promoting equity and well-being. \n",
            "Ethical Considerations \n",
            "Addressing the ethical implications of AI is crucial for ensuring its positive social impact. This \n",
            "includes promoting fairness, transparency, and accountability in AI systems, as well as protecting \n",
            "privacy and human rights. \n",
            "Public Perception and Trust \n",
            "Public perception and trust in AI are essential for its widespread adoption and positive social \n",
            "impact. Building trust requires transparency, \n",
            "\n",
            "Chunk 38 (referenced by question 'How does making AI systems understandable help build trust?'):\n",
            "to building trust in AI. Making AI systems understandable \n",
            "and providing insights into their decision-making processes helps users assess their reliability \n",
            "and fairness. \n",
            "Robustness and Reliability \n",
            "Ensuring that AI systems are robust and reliable is essential for building trust. This includes \n",
            "testing and validating AI models, monitoring their performance, and addressing potential \n",
            "vulnerabilities. \n",
            "User Control and Agency \n",
            "Empowering users with control over AI systems and providing them with agency in their \n",
            "interactions with AI enhances trust. This includes allowing users to customize AI settings, \n",
            "understand how their data is used, and opt out of AI-driven features. \n",
            "Ethical Design and Development \n",
            "Incorporating ethical considerations into the design and development of AI systems is crucial for \n",
            "building trust. This includes conducting ethical impact assessments, engaging stakeholders, and \n",
            "adhering to ethical guidelines and standards. \n",
            "Public Engagement and Education \n",
            "Engaging th\n",
            "\n",
            "Response:\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It is important as it enhances trust and accountability by providing insights into how AI models make decisions. Research in XAI focuses on developing methods for explaining AI decisions. Making AI systems understandable helps build trust by enabling users to assess their reliability and fairness. It also addresses ethical implications such as promoting fairness, transparency, and accountability in AI systems.\n"
          ]
        }
      ],
      "source": [
        "# Prepare context from search results\n",
        "context = prepare_context(search_results)\n",
        "\n",
        "# Generate response\n",
        "response_text = generate_response(query, context)\n",
        "\n",
        "print(\"\\nQuery:\", query)\n",
        "print(\"\\nContext:\", context)\n",
        "print(\"\\nResponse:\")\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB5lHetY35_0"
      },
      "source": [
        "## 评估 AI 响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bwB946n-35_0"
      },
      "outputs": [],
      "source": [
        "def evaluate_response(query, response, reference_answer, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Evaluates the AI response against a reference answer.\n",
        "\n",
        "    Args:\n",
        "    query (str): The user's question.\n",
        "    response (str): The AI-generated response.\n",
        "    reference_answer (str): The reference/ideal answer.\n",
        "    model (str): Model to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "    str: Evaluation feedback.\n",
        "    \"\"\"\n",
        "    # Define the system prompt for the evaluation system\n",
        "    evaluate_system_prompt = \"\"\"You are an intelligent evaluation system tasked with assessing AI responses.\n",
        "\n",
        "        Compare the AI assistant's response to the true/reference answer, and evaluate based on:\n",
        "        1. Factual correctness - Does the response contain accurate information?\n",
        "        2. Completeness - Does it cover all important aspects from the reference?\n",
        "        3. Relevance - Does it directly address the question?\n",
        "\n",
        "        Assign a score from 0 to 1:\n",
        "        - 1.0: Perfect match in content and meaning\n",
        "        - 0.8: Very good, with minor omissions/differences\n",
        "        - 0.6: Good, covers main points but misses some details\n",
        "        - 0.4: Partial answer with significant omissions\n",
        "        - 0.2: Minimal relevant information\n",
        "        - 0.0: Incorrect or irrelevant\n",
        "\n",
        "        Provide your score with justification.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the evaluation prompt\n",
        "    evaluation_prompt = f\"\"\"\n",
        "        User Query: {query}\n",
        "\n",
        "        AI Response:\n",
        "        {response}\n",
        "\n",
        "        Reference Answer:\n",
        "        {reference_answer}\n",
        "\n",
        "        Please evaluate the AI response against the reference answer.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate evaluation\n",
        "    eval_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": evaluate_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return eval_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1XnZSm135_0"
      },
      "source": [
        "## 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "buPAd_Oa35_1",
        "outputId": "e996cce3-40fc-42ca-cb48-73bee0926003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation:\n",
            "Score: 0.8\n",
            "Justification: The AI response is very good and covers the main points of the reference answer. It accurately defines Explainable AI as aiming to make systems more transparent and understandable and emphasizes its importance in building trust, accountability, and ensuring fairness. However, it might be slightly less than a perfect match as it specifically mentions \"research in XAI focuses on developing methods for explaining AI decisions\" which is not explicitly stated in the reference answer. Overall, it covers the main aspects with only minor omissions/differences.\n"
          ]
        }
      ],
      "source": [
        "# Get reference answer from validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# Evaluate the response\n",
        "evaluation = evaluate_response(query, response_text, reference_answer)\n",
        "\n",
        "print(\"\\nEvaluation:\")\n",
        "print(evaluation)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}