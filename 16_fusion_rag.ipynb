{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/16_fusion_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "1nDnACCIeQk3"
      },
      "source": [
        "# 融合检索：结合向量搜索和关键词搜索\n",
        "\n",
        "在这个笔记本中，我实现了一个融合检索系统，该系统结合了语义向量搜索和基于关键词的 BM25 检索的优势。这种方法通过捕捉概念相似性和精确关键词匹配，提高了检索质量。\n",
        "\n",
        "## 为什么融合检索很重要\n",
        "\n",
        "传统的 RAG 系统通常仅依赖向量搜索，但这存在局限性：\n",
        "\n",
        "- 向量搜索在语义相似性方面表现出色，但可能会遗漏精确关键词匹配\n",
        "- 关键词搜索适用于特定术语，但缺乏语义理解\n",
        "- 不同的查询更适合不同的检索方法\n",
        "\n",
        "融合检索通过以下方式结合了两者的优点：\n",
        "\n",
        "- 执行基于向量的检索和基于关键词的检索，系统能够捕捉到概念相似性和精确关键词匹配，能够有效处理更广泛的查询，弥补单一方法的不足。\n",
        "- 对每种方法的评分进行归一化\n",
        "- 使用加权公式将它们结合起来，alpha 参数可以根据具体用例或查询类型调整向量搜索和关键词搜索之间的平衡\n",
        "- 根据综合评分对文档进行排名"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVkTx1GeQk5"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/tivon-x/all-rag-techniques/blob/main/images/fusion_retrieval.svg?raw=1\" alt=\"Fusion Retrieval\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZqveTZwTfWXj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43--cUyDeQk5"
      },
      "source": [
        "## 环境设置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VBEA4GfseQk5"
      },
      "outputs": [],
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-tFEvydfeQk5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7nwh4PweQk6"
      },
      "source": [
        "## OpenAI Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1jtTDZ4qeQk6"
      },
      "outputs": [],
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WksdFtZ6eQk6"
      },
      "outputs": [],
      "source": [
        "text_model = \"doubao-1-5-lite-32k-250115\"\n",
        "image_model = \"doubao-1.5-vision-lite-250315\"\n",
        "embedding_model = \"doubao-embedding-large-text-240915\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DkTB9evJeQk6"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97P1JXwReQk6"
      },
      "source": [
        "## 文档处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JzQ-xN7OeQk6"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text content from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content\n",
        "    \"\"\"\n",
        "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
        "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
        "    text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document[page_num]  # Get the page object\n",
        "        text += page.get_text()  # Extract text from the page and append to the text string\n",
        "\n",
        "    return text  # Return the extracted text content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rESTSj6XeQk7"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        chunk_overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunks with text and metadata\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store chunks\n",
        "\n",
        "    # Iterate over the text with the specified chunk size and overlap\n",
        "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
        "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
        "        if chunk:  # Ensure we don't add empty chunks\n",
        "            chunk_data = {\n",
        "                \"text\": chunk,  # The chunk text\n",
        "                \"metadata\": {\n",
        "                    \"start_char\": i,  # Start character index of the chunk\n",
        "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
        "                }\n",
        "            }\n",
        "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
        "\n",
        "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
        "    return chunks  # Return the list of chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BTNDM1B2eQk7"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing extra whitespace and special characters.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
        "    text = text.replace('\\\\t', ' ')\n",
        "    text = text.replace('\\\\n', ' ')\n",
        "\n",
        "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RtKW9wWeQk7"
      },
      "source": [
        "## 向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LTzuTUGleQk7"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(texts, model=None):\n",
        "    \"\"\"\n",
        "    Create embeddings for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (str or List[str]): Input text(s)\n",
        "        model (str): Embedding model name\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: Embedding vectors\n",
        "    \"\"\"\n",
        "    model = model or embedding_model  # Use provided model or default to embedding_model\n",
        "\n",
        "    # Handle both string and list inputs\n",
        "    input_texts = texts if isinstance(texts, list) else [texts]\n",
        "\n",
        "    # Process in batches if needed (OpenAI API limits)\n",
        "    batch_size = 10\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Iterate over the input texts in batches\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
        "\n",
        "        # Create embeddings for the current batch\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=batch\n",
        "        )\n",
        "\n",
        "        # Extract embeddings from the response\n",
        "        batch_embeddings = [item.embedding for item in response.data]\n",
        "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
        "\n",
        "    # If input was a string, return just the first embedding\n",
        "    if isinstance(texts, str):\n",
        "        return all_embeddings[0]\n",
        "\n",
        "    # Otherwise return all embeddings\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kQ9c_YTyeQk7"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store text content\n",
        "        self.metadata = []  # List to store metadata\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text content\n",
        "            embedding (List[float]): The embedding vector\n",
        "            metadata (Dict, optional): Additional metadata\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Append the embedding vector\n",
        "        self.texts.append(text)  # Append the text content\n",
        "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
        "\n",
        "    def add_items(self, items, embeddings):\n",
        "        \"\"\"\n",
        "        Add multiple items to the vector store.\n",
        "\n",
        "        Args:\n",
        "            items (List[Dict]): List of text items\n",
        "            embeddings (List[List[float]]): List of embedding vectors\n",
        "        \"\"\"\n",
        "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
        "            self.add_item(\n",
        "                text=item[\"text\"],  # Extract text from item\n",
        "                embedding=embedding,  # Use corresponding embedding\n",
        "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
        "            )\n",
        "\n",
        "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding with similarity scores.\n",
        "\n",
        "        Args:\n",
        "            query_embedding (List[float]): Query embedding vector\n",
        "            k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            similarity = cosine_similarity([query_vector], [vector])[0][0]  # Compute cosine similarity\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results with scores\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Retrieve text by index\n",
        "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
        "                \"similarity\": float(score)  # Add similarity score\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_all_documents(self):\n",
        "        \"\"\"\n",
        "        Get all documents in the store.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: All documents\n",
        "        \"\"\"\n",
        "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCi9-WIeQk7"
      },
      "source": [
        "## 实现 BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iCw0O4GVeQk7"
      },
      "outputs": [],
      "source": [
        "def create_bm25_index(chunks):\n",
        "    \"\"\"\n",
        "    Create a BM25 index from the given chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[Dict]): List of text chunks\n",
        "\n",
        "    Returns:\n",
        "        BM25Okapi: A BM25 index\n",
        "    \"\"\"\n",
        "    # Extract text from each chunk\n",
        "    texts = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "    # Tokenize each document by splitting on whitespace\n",
        "    tokenized_docs = [text.split() for text in texts]\n",
        "\n",
        "    # Create the BM25 index using the tokenized documents\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    # Print the number of documents in the BM25 index\n",
        "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
        "\n",
        "    return bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gAbVy_AfeQk7"
      },
      "outputs": [],
      "source": [
        "def bm25_search(bm25, chunks, query, k=5):\n",
        "    \"\"\"\n",
        "    Search the BM25 index with a query.\n",
        "\n",
        "    Args:\n",
        "        bm25 (BM25Okapi): BM25 index\n",
        "        chunks (List[Dict]): List of text chunks\n",
        "        query (str): Query string\n",
        "        k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Top k results with scores\n",
        "    \"\"\"\n",
        "    # Tokenize the query by splitting it into individual words\n",
        "    query_tokens = query.split()\n",
        "\n",
        "    # Get BM25 scores for the query tokens against the indexed documents\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Initialize an empty list to store results with their scores\n",
        "    results = []\n",
        "\n",
        "    # Iterate over the scores and corresponding chunks\n",
        "    for i, score in enumerate(scores):\n",
        "        # Create a copy of the metadata to avoid modifying the original\n",
        "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
        "        # Add index to metadata\n",
        "        metadata[\"index\"] = i\n",
        "\n",
        "        results.append({\n",
        "            \"text\": chunks[i][\"text\"],\n",
        "            \"metadata\": metadata,  # Add metadata with index\n",
        "            \"bm25_score\": float(score)\n",
        "        })\n",
        "\n",
        "    # Sort the results by BM25 score in descending order\n",
        "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
        "\n",
        "    # Return the top k results\n",
        "    return results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QVp9WKVeQk8"
      },
      "source": [
        "## 混合检索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1HWpwrXGeQk8"
      },
      "outputs": [],
      "source": [
        "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    执行融合检索，结合向量搜索和BM25搜索。\n",
        "\n",
        "    Args:\n",
        "        query (str): Query string\n",
        "        chunks (List[Dict]): Original text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of results to return\n",
        "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Top k results based on combined scores\n",
        "    \"\"\"\n",
        "    print(f\"Performing fusion retrieval for query: {query}\")\n",
        "\n",
        "    # 避免除以零的情况\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Get vector search results\n",
        "    query_embedding = create_embeddings(query)  # Create embedding for the query\n",
        "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n",
        "\n",
        "    # Get BM25 search results\n",
        "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
        "\n",
        "    # Create dictionaries to map document index to score\n",
        "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
        "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
        "\n",
        "    # Ensure all documents have scores for both methods\n",
        "    all_docs = vector_store.get_all_documents()\n",
        "    combined_results = []\n",
        "\n",
        "    for i, doc in enumerate(all_docs):\n",
        "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
        "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
        "        combined_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"vector_score\": vector_score,\n",
        "            \"bm25_score\": bm25_score,\n",
        "            \"index\": i\n",
        "        })\n",
        "\n",
        "    # Extract scores as arrays\n",
        "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
        "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
        "\n",
        "    # 归一化\n",
        "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
        "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
        "\n",
        "    # 计算 combined scores\n",
        "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
        "\n",
        "    # Add combined scores to results\n",
        "    for i, score in enumerate(combined_scores):\n",
        "        combined_results[i][\"combined_score\"] = float(score)\n",
        "\n",
        "    # Sort by combined score (descending)\n",
        "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "\n",
        "    # Return top k results\n",
        "    top_results = combined_results[:k]\n",
        "\n",
        "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
        "    return top_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44zM4MsteQk8"
      },
      "source": [
        "## 文档处理流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9-_JVlb3eQk8"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for fusion retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        chunk_overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Clean the extracted text to remove extra whitespace and special characters\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Split the cleaned text into overlapping chunks\n",
        "    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
        "\n",
        "    # Extract the text content from each chunk for embedding creation\n",
        "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "\n",
        "    # Create embeddings for the chunk texts\n",
        "    embeddings = create_embeddings(chunk_texts)\n",
        "\n",
        "    # Initialize the vector store\n",
        "    vector_store = SimpleVectorStore()\n",
        "\n",
        "    # Add the chunks and their embeddings to the vector store\n",
        "    vector_store.add_items(chunks, embeddings)\n",
        "    print(f\"Added {len(chunks)} items to vector store\")\n",
        "\n",
        "    # Create a BM25 index from the chunks\n",
        "    bm25_index = create_bm25_index(chunks)\n",
        "\n",
        "    # Return the chunks, vector store, and BM25 index\n",
        "    return chunks, vector_store, bm25_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcDjwxtFeQk8"
      },
      "source": [
        "## 生成响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N7RVyLyweQk8"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context from retrieved documents\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
        "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n",
        "\n",
        "    # Format the user prompt with the context and query\n",
        "    user_prompt = f\"\"\"Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Please answer the question based on the provided context.\"\"\"\n",
        "\n",
        "    # Generate the response using the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=text_model,  # Specify the model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
        "            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n",
        "        ],\n",
        "        temperature=0.1  # Set the temperature for response generation\n",
        "    )\n",
        "\n",
        "    # Return the generated response\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs_BE1_ReQk8"
      },
      "source": [
        "## 检索函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tfq1NXmBeQk8"
      },
      "outputs": [],
      "source": [
        "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Answer a query using fusion RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results including retrieved documents and response\n",
        "    \"\"\"\n",
        "    # Retrieve documents using fusion retrieval method\n",
        "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgHyXdJ5eQk8"
      },
      "source": [
        "## 比较检索方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xT06hnbzeQk8"
      },
      "outputs": [],
      "source": [
        "def vector_only_rag(query, vector_store, k=5):\n",
        "    \"\"\"\n",
        "    Answer a query using only vector-based RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Retrieve documents using vector-based similarity search\n",
        "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-VcInJXgeQk9"
      },
      "outputs": [],
      "source": [
        "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
        "    \"\"\"\n",
        "    Answer a query using only BM25-based RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results\n",
        "    \"\"\"\n",
        "    # Retrieve documents using BM25 search\n",
        "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
        "\n",
        "    # Format the context from the retrieved documents by joining their text with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
        "\n",
        "    # Generate a response based on the query and the formatted context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the query, retrieved documents, and the generated response\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nZ1PxBveQk9"
      },
      "source": [
        "## 评估函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1O3r4dKKeQk9"
      },
      "outputs": [],
      "source": [
        "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Compare different retrieval methods for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        chunks (List[Dict]): Text chunks\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        bm25_index (BM25Okapi): BM25 index\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores in fusion retrieval\n",
        "        reference_answer (str, optional): Reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        Dict: Comparison results\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
        "\n",
        "    # Run vector-only RAG\n",
        "    print(\"\\nRunning vector-only RAG...\")\n",
        "    vector_result = vector_only_rag(query, vector_store, k)\n",
        "\n",
        "    # Run BM25-only RAG\n",
        "    print(\"\\nRunning BM25-only RAG...\")\n",
        "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
        "\n",
        "    # Run fusion RAG\n",
        "    print(\"\\nRunning fusion RAG...\")\n",
        "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
        "\n",
        "    # Compare responses from different retrieval methods\n",
        "    print(\"\\nComparing responses...\")\n",
        "    comparison = evaluate_responses(\n",
        "        query,\n",
        "        vector_result[\"response\"],\n",
        "        bm25_result[\"response\"],\n",
        "        fusion_result[\"response\"],\n",
        "        reference_answer\n",
        "    )\n",
        "\n",
        "    # Return the comparison results\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"vector_result\": vector_result,\n",
        "        \"bm25_result\": bm25_result,\n",
        "        \"fusion_result\": fusion_result,\n",
        "        \"comparison\": comparison\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lPQ8bgq9eQk9"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluate the responses from different retrieval methods.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_response (str): Response from vector-only RAG\n",
        "        bm25_response (str): Response from BM25-only RAG\n",
        "        fusion_response (str): Response from fusion RAG\n",
        "        reference_answer (str, optional): Reference answer\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation of responses\n",
        "    \"\"\"\n",
        "    # System prompt for the evaluator to guide the evaluation process\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
        "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
        "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
        "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
        "\n",
        "    Evaluate the responses based on:\n",
        "    - Relevance to the query\n",
        "    - Factual correctness\n",
        "    - Comprehensiveness\n",
        "    - Clarity and coherence\"\"\"\n",
        "\n",
        "    # User prompt containing the query and responses\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "    Vector-based response:\n",
        "    {vector_response}\n",
        "\n",
        "    BM25 keyword response:\n",
        "    {bm25_response}\n",
        "\n",
        "    Fusion response:\n",
        "    {fusion_response}\n",
        "    \"\"\"\n",
        "\n",
        "    # Add reference answer to the prompt if provided\n",
        "    if reference_answer:\n",
        "        user_prompt += f\"\"\"\n",
        "            Reference answer:\n",
        "            {reference_answer}\n",
        "        \"\"\"\n",
        "\n",
        "    # Add instructions for detailed comparison to the user prompt\n",
        "    user_prompt += \"\"\"\n",
        "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
        "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
        "    response = client.chat.completions.create(\n",
        "        model=text_model,  # Specify the model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
        "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
        "        ],\n",
        "        temperature=0  # Set the temperature for response generation\n",
        "    )\n",
        "\n",
        "    # Return the generated evaluation content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFPJpwaXeQk9"
      },
      "source": [
        "## 完整的评估流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "74CWmNdOeQk9"
      },
      "outputs": [],
      "source": [
        "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate fusion retrieval compared to other methods.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        test_queries (List[str]): List of test queries\n",
        "        reference_answers (List[str], optional): Reference answers\n",
        "        k (int): Number of documents to retrieve\n",
        "        alpha (float): Weight for vector scores in fusion retrieval\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
        "\n",
        "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
        "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
        "\n",
        "    # Initialize a list to store results for each query\n",
        "    results = []\n",
        "\n",
        "    # Iterate over each test query\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        # Get the reference answer if available\n",
        "        reference = None\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            reference = reference_answers[i]\n",
        "\n",
        "        # Compare retrieval methods for the current query\n",
        "        comparison = compare_retrieval_methods(\n",
        "            query,\n",
        "            chunks,\n",
        "            vector_store,\n",
        "            bm25_index,\n",
        "            k=k,\n",
        "            alpha=alpha,\n",
        "            reference_answer=reference\n",
        "        )\n",
        "\n",
        "        # Append the comparison results to the results list\n",
        "        results.append(comparison)\n",
        "\n",
        "        # Print the responses from different retrieval methods\n",
        "        print(\"\\n=== Vector-based Response ===\")\n",
        "        print(comparison[\"vector_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== BM25 Response ===\")\n",
        "        print(comparison[\"bm25_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== Fusion Response ===\")\n",
        "        print(comparison[\"fusion_result\"][\"response\"])\n",
        "\n",
        "        print(\"\\n=== Comparison ===\")\n",
        "        print(comparison[\"comparison\"])\n",
        "\n",
        "    # Generate an overall analysis of the fusion retrieval performance\n",
        "    overall_analysis = generate_overall_analysis(results)\n",
        "\n",
        "    # Return the results and overall analysis\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"overall_analysis\": overall_analysis\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5p4sF5eceQk9"
      },
      "outputs": [],
      "source": [
        "def generate_overall_analysis(results):\n",
        "    \"\"\"\n",
        "    Generate an overall analysis of fusion retrieval.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results from evaluating queries\n",
        "\n",
        "    Returns:\n",
        "        str: Overall analysis\n",
        "    \"\"\"\n",
        "    # System prompt to guide the evaluation process\n",
        "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
        "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
        "    1. Vector-based retrieval (semantic similarity)\n",
        "    2. BM25 keyword retrieval (keyword matching)\n",
        "    3. Fusion retrieval (combination of both)\n",
        "\n",
        "    Focus on:\n",
        "    1. Types of queries where each approach performs best\n",
        "    2. Overall strengths and weaknesses of each approach\n",
        "    3. How fusion retrieval balances the trade-offs\n",
        "    4. Recommendations for when to use each approach\"\"\"\n",
        "\n",
        "    # Create a summary of evaluations for each query\n",
        "    evaluations_summary = \"\"\n",
        "    for i, result in enumerate(results):\n",
        "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
        "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
        "\n",
        "    # User prompt containing the evaluations summary\n",
        "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries,\n",
        "    provide an overall analysis comparing these three approaches:\n",
        "\n",
        "    {evaluations_summary}\n",
        "\n",
        "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
        "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
        "\n",
        "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
        "    response = client.chat.completions.create(\n",
        "        model= text_model,  # Specify the model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated analysis content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgNaiIbKeQk9"
      },
      "source": [
        "## Evaluating Fusion Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KYChTXvagNF0",
        "outputId": "93b51307-f1d6-446f-8f1e-095fee165632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jnJ8KXLdeQk9",
        "outputId": "7780ef75-04c9-41d8-da07-a3b723b16521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EVALUATING FUSION RETRIEVAL ===\n",
            "\n",
            "Extracting text from ./drive/MyDrive/colab_data/AI_Information.pdf...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 items to vector store\n",
            "Created BM25 index with 42 documents\n",
            "\n",
            "\n",
            "=== Evaluating Query 1/1 ===\n",
            "Query: What are the main applications of transformer models in natural language processing?\n",
            "\n",
            "=== Comparing retrieval methods for query: What are the main applications of transformer models in natural language processing? ===\n",
            "\n",
            "\n",
            "Running vector-only RAG...\n",
            "\n",
            "Running BM25-only RAG...\n",
            "\n",
            "Running fusion RAG...\n",
            "Performing fusion retrieval for query: What are the main applications of transformer models in natural language processing?\n",
            "Retrieved 5 documents with fusion retrieval\n",
            "\n",
            "Comparing responses...\n",
            "\n",
            "=== Vector-based Response ===\n",
            "The provided context does not mention the main applications of transformer models in natural language processing. So, I cannot answer this question based on the given context. \n",
            "\n",
            "=== BM25 Response ===\n",
            "The provided context does not mention the main applications of transformer models in natural language processing. So, based on the given context, I am unable to answer this question fully. \n",
            "\n",
            "=== Fusion Response ===\n",
            "The provided context does not mention the main applications of transformer models in natural language processing. So, based on the given context, I cannot answer this question fully. \n",
            "\n",
            "=== Comparison ===\n",
            "**1. Relevance to the query**\n",
            "   - **Vector - based response**:\n",
            "     - **Strength**: It clearly states that it cannot answer based on the given context, which is relevant in a way as it acknowledges the lack of relevant information.\n",
            "     - **Weakness**: It completely fails to provide any relevant content about the applications of transformer models, which is the core of the query.\n",
            "   - **BM25 keyword response**:\n",
            "     - **Strength**: Similar to the vector - based response, it acknowledges the inability to answer due to lack of context, which is relevant in terms of setting the stage.\n",
            "     - **Weakness**: It does not offer any relevant information regarding the applications of transformer models.\n",
            "   - **Fusion response**:\n",
            "     - **Strength**: It also clearly indicates that it cannot answer based on the context, which is relevant to the situation.\n",
            "     - **Weakness**: It does not provide any relevant details about the applications of transformer models.\n",
            "\n",
            "   - In terms of relevance, all three responses are equally poor as they do not provide any relevant information about the main applications of transformer models.\n",
            "\n",
            "**2. Factual correctness**\n",
            "   - **Vector - based response**:\n",
            "     - **Strength**: Since it doesn't make any factual claims about the applications, there is no incorrect information.\n",
            "     - **Weakness**: It doesn't provide any correct information either.\n",
            "   - **BM25 keyword response**:\n",
            "     - **Strength**: It doesn't have any incorrect factual statements as it just says it can't answer.\n",
            "     - **Weakness**: It lacks correct factual content about the applications.\n",
            "   - **Fusion response**:\n",
            "     - **Strength**: It has no incorrect factual claims as it only states its inability to answer.\n",
            "     - **Weakness**: It doesn't offer any correct factual information about the applications.\n",
            "\n",
            "   - All three responses are factually neutral in the sense that they don't provide correct or incorrect facts about the applications of transformer models.\n",
            "\n",
            "**3. Comprehensiveness**\n",
            "   - **Vector - based response**:\n",
            "     - **Strength**: It is short and to the point in stating its inability to answer.\n",
            "     - **Weakness**: It is extremely uncomprehensive as it doesn't cover any aspect of the query.\n",
            "   - **BM25 keyword response**:\n",
            "     - **Strength**: It clearly communicates the lack of ability to answer.\n",
            "     - **Weakness**: It is highly uncomprehensive as it doesn't provide any details about the applications.\n",
            "   - **Fusion response**:\n",
            "     - **Strength**: It simply conveys the inability to answer.\n",
            "     - **Weakness**: It is completely uncomprehensive regarding the query's topic.\n",
            "\n",
            "   - None of the responses are comprehensive as they do not provide any information about the main applications of transformer models.\n",
            "\n",
            "**4. Clarity and coherence**\n",
            "   - **Vector - based response**:\n",
            "     - **Strength**: It is clear in stating that it can't answer based on the context.\n",
            "     - **Weakness**: It has no coherence with the query's topic as it doesn't provide any relevant content.\n",
            "   - **BM25 keyword response**:\n",
            "     - **Strength**: It is clear about its inability to answer.\n",
            "     - **Weakness**: It lacks coherence with the query as it doesn't offer any relevant details.\n",
            "   - **Fusion response**:\n",
            "     - **Strength**: It is clear in communicating its inability to answer.\n",
            "     - **Weakness**: It has no coherence with the query's subject matter.\n",
            "\n",
            "   - All three responses are clear in stating their inability to answer, but they lack coherence with the query as they do not provide any relevant information about the applications of transformer models.\n",
            "\n",
            "**Overall, no approach performed well for this query**\n",
            "   - The main reason is that all three responses rely on the lack of relevant context in the provided data. They do not offer any positive content about the main applications of transformer models. The vector - based, BM25 keyword, and fusion retrieval approaches all failed to address the core of the query, which is to list the main applications of transformer models in natural language processing. Their strengths in terms of simplicity (in clearly stating the inability to answer) are overshadowed by their complete lack of relevant information for this particular query.\n",
            "\n",
            "\n",
            "=== OVERALL ANALYSIS ===\n",
            "\n",
            "**1. Types of queries where each approach performs best**\n",
            "\n",
            "   - **Vector - based retrieval (semantic similarity)**:\n",
            "     - Performs well for queries that require understanding the semantic meaning of the text. For example, queries like \"Describe the emotional undertone of this passage\" or \"Find similar concepts to 'artificial intelligence' in this document set\". It can capture the semantic relationships between words and documents, making it suitable for more nuanced, meaning - based queries.\n",
            "   - **BM25 keyword retrieval (keyword matching)**:\n",
            "     - Is highly effective for straightforward keyword - based queries. Queries such as \"List all the products that contain 'widget' in their description\" or \"Find articles about 'climate change' in a news corpus\" are well - suited for BM25. It simply looks for the presence of specific keywords in the documents.\n",
            "   - **Fusion retrieval (combination of both)**:\n",
            "     - Works best when the query has both semantic and keyword - based aspects. For instance, a query like \"Find research papers on the latest advancements in 'autonomous vehicles' with a focus on safety features\" benefits from fusion. It can use the vector - based approach to capture the semantic relationship between \"autonomous vehicles\" and \"safety features\" while also leveraging BM25 to quickly identify relevant papers based on the keywords.\n",
            "\n",
            "**2. Overall strengths and weaknesses of each approach**\n",
            "\n",
            "   - **Vector - based retrieval (semantic similarity)**:\n",
            "     - **Strengths**:\n",
            "       - Can handle semantic variations well. For example, it can match \"car\" and \"automobile\" as they have similar semantic meanings.\n",
            "       - Useful for tasks where understanding the overall meaning of the query and documents is crucial, such as in information - seeking for complex research topics.\n",
            "     - **Weaknesses**:\n",
            "       - Computationally expensive, especially for large document collections.\n",
            "       - Performance can be highly dependent on the quality of the semantic embeddings used. If the embeddings are not well - trained, the results may be inaccurate.\n",
            "   - **BM25 keyword retrieval (keyword matching)**:\n",
            "     - **Strengths**:\n",
            "       - Simple and fast to compute. It can quickly return results for keyword - based queries without the need for complex semantic analysis.\n",
            "       - Effective for retrieving documents that exactly match or closely match the query keywords, which is useful in many practical applications like document search in a company's internal knowledge base.\n",
            "     - **Weaknesses**:\n",
            "       - Ignores semantic relationships between words. For example, it may not retrieve relevant documents if the query uses a synonym or related concept instead of the exact keyword.\n",
            "       - Can be less effective for more complex, context - dependent queries.\n",
            "   - **Fusion retrieval (combination of both)**:\n",
            "     - **Strengths**:\n",
            "       - Combines the strengths of both approaches. It can capture semantic relationships while also leveraging the speed and simplicity of keyword matching.\n",
            "       - More robust in handling a wide variety of query types compared to either individual approach alone.\n",
            "     - **Weaknesses**:\n",
            "       - More complex to implement compared to the individual methods.\n",
            "       - Requires careful tuning of the combination weights to achieve optimal performance.\n",
            "\n",
            "**3. How fusion retrieval balances the trade - offs**\n",
            "\n",
            "Fusion retrieval balances the trade - offs by using the vector - based approach to capture the semantic essence of the query and documents. This helps in dealing with semantic variations and understanding the overall context. At the same time, it incorporates the BM25 keyword - matching component. The keyword - matching part provides a quick and straightforward way to filter out documents that are likely to be relevant based on the presence of the query keywords. By combining these two, fusion retrieval can overcome the limitations of each method. For example, if a query has a main keyword but also requires understanding related semantic concepts, the vector - based part can find documents related to those concepts, and the BM25 part can ensure that the main keyword is present in the retrieved documents.\n",
            "\n",
            "**4. Recommendations for when to use each approach**\n",
            "\n",
            "   - **Vector - based retrieval**:\n",
            "     - Use when the query is about understanding semantic relationships, such as in semantic search, question - answering systems that require understanding the meaning behind the question, or when exploring a new and complex topic where semantic understanding is key.\n",
            "   - **BM25 keyword retrieval**:\n",
            "     - Ideal for simple, keyword - based searches where speed and exact keyword matches are important. This includes searching for specific terms in a database, looking for product names in an inventory, or retrieving news articles based on a particular event name.\n",
            "   - **Fusion retrieval**:\n",
            "     - Employ when the query has both semantic and keyword - based elements. This is common in many real - world search scenarios, such as searching for research papers, finding relevant products in an e - commerce site with specific features in mind, or looking for information in a heterogeneous document collection where a combination of semantic and keyword - based matching is needed for accurate results. \n"
          ]
        }
      ],
      "source": [
        "# Path to PDF document\n",
        "# Path to PDF document containing AI information for knowledge retrieval testing\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\"\n",
        "\n",
        "# Define a single AI-related test query\n",
        "test_queries = [\n",
        "    \"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n",
        "]\n",
        "\n",
        "# Optional reference answer\n",
        "reference_answers = [\n",
        "    \"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n",
        "]\n",
        "\n",
        "# Set parameters\n",
        "k = 5  # Number of documents to retrieve\n",
        "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_fusion_retrieval(\n",
        "    pdf_path=pdf_path,\n",
        "    test_queries=test_queries,\n",
        "    reference_answers=reference_answers,\n",
        "    k=k,\n",
        "    alpha=alpha\n",
        ")\n",
        "\n",
        "# Print overall analysis\n",
        "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
        "print(evaluation_results[\"overall_analysis\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}