{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsJNJCaBq9G9",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "# RAG 中的反馈循环\n",
        "\n",
        "在本笔记本中，我实现了一个带有反馈循环机制的 RAG 系统，该系统能够随着时间的推移不断改进。通过收集并整合用户反馈，我们的系统能够在每次交互中学习提供更相关、更高质量的回复。\n",
        "\n",
        "传统的 RAG 系统是静态的——它们仅基于嵌入相似度检索信息。通过引入反馈循环，我们创建了一个动态系统，能够实现以下功能：\n",
        "\n",
        "- 记住哪些有效（以及哪些无效）\n",
        "- 随时间调整文档相关性得分\n",
        "- 将成功的问答对纳入其知识库\n",
        "- 在每次用户交互中变得更智能\n",
        "\n",
        "该方法的优势\n",
        "1. 持续改进：系统从每次交互中学习，逐步提升性能。\n",
        "2. 个性化：通过整合用户反馈，系统可以随时间适应个人或群体偏好。\n",
        "3. 增加相关性：反馈循环有助于在未来检索中优先考虑更相关的文档。\n",
        "4. 质量控制：随着系统的演变，低质量或不相关的响应不太可能被重复。\n",
        "5. 适应性：系统可以随时间适应用户需求或文档内容的变化。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqmvtFhUJWj6"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/tivon-x/all-rag-techniques/blob/main/images/retrieval_with_feedback_loop.svg?raw=1\" alt=\"retrieval with feedback loop\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8pYPwCq9G-"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH_yZU8VsOMe",
        "outputId": "653318b8-e6aa-4f52-a8cd-517de3866746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4xx9T0Aq9G-"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXEkj61Kq9G_"
      },
      "source": [
        "## 提取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tSLz4f3q9G_"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yYz7P_Qq9G_"
      },
      "source": [
        "## 分块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-rIH75Oq9G_"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-O22eIq9G_"
      },
      "source": [
        "## OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtXwCF-wsjN7"
      },
      "outputs": [],
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iuymBKXtWVF"
      },
      "outputs": [],
      "source": [
        "model_name = \"doubao-lite-128k-240828\"\n",
        "embedding_model = \"doubao-embedding-text-240715\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54UMuDynq9G_"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un3j9lv-q9HA"
      },
      "source": [
        "## 向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3D5Ev3tq9HA"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "\n",
        "    This class provides an in-memory storage and retrieval system for\n",
        "    embedding vectors and their corresponding text chunks and metadata.\n",
        "    It supports basic similarity search functionality using cosine similarity.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store with empty lists for vectors, texts, and metadata.\n",
        "\n",
        "        The vector store maintains three parallel lists:\n",
        "        - vectors: NumPy arrays of embedding vectors\n",
        "        - texts: Original text chunks corresponding to each vector\n",
        "        - metadata: Optional metadata dictionaries for each item\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []    # List to store original text chunks\n",
        "        self.metadata = [] # List to store metadata for each text chunk\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "            text (str): The original text chunk to store.\n",
        "            embedding (List[float]): The embedding vector representing the text.\n",
        "            metadata (dict, optional): Additional metadata for the text chunk,\n",
        "                                      such as source, timestamp, or relevance scores.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert and store the embedding\n",
        "        self.texts.append(text)                   # Store the original text\n",
        "        self.metadata.append(metadata or {})      # Store metadata (empty dict if None)\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding using cosine similarity.\n",
        "\n",
        "        Args:\n",
        "            query_embedding (List[float]): Query embedding vector to compare against stored vectors.\n",
        "            k (int): Number of most similar results to return.\n",
        "            filter_func (callable, optional): Function to filter results based on metadata.\n",
        "                                             Takes metadata dict as input and returns boolean.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Top k most similar items, each containing:\n",
        "                - text: The original text\n",
        "                - metadata: Associated metadata\n",
        "                - similarity: Raw cosine similarity score\n",
        "                - relevance_score: Either metadata-based relevance or calculated similarity\n",
        "\n",
        "        Note: Returns empty list if no vectors are stored or none pass the filter.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if vector store is empty\n",
        "\n",
        "        # Convert query embedding to numpy array for vector operations\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate cosine similarity between query and each stored vector\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Skip items that don't pass the filter criteria\n",
        "            if filter_func and not filter_func(self.metadata[i]):\n",
        "                continue\n",
        "\n",
        "            # Calculate cosine similarity: dot product / (norm1 * norm2)\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Store index and similarity score\n",
        "\n",
        "        # Sort results by similarity score in descending order\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Construct result dictionaries for the top k matches\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],\n",
        "                \"metadata\": self.metadata[idx],\n",
        "                \"similarity\": score,\n",
        "                # Use pre-existing relevance score from metadata if available, otherwise use similarity\n",
        "                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyO3iz-pq9HA"
      },
      "source": [
        "## 创建嵌入向量\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lh7MKzfq9HA"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model = None, batch_size = 10):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text.\n",
        "\n",
        "    Args:\n",
        "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "    batch_size (int): batch size\n",
        "\n",
        "    Returns:\n",
        "    List[float] or List[List[float]]: The embedding vector(s).\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = embedding_model\n",
        "\n",
        "    # Convert single string to list for uniform processing\n",
        "    input_text = text if isinstance(text, list) else [text]\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(input_text), batch_size):\n",
        "      batch = input_text[i : i + batch_size]\n",
        "      # Create embeddings for the batch using the specified model\n",
        "      response = client.embeddings.create(\n",
        "          model=model,\n",
        "          input=batch\n",
        "      )\n",
        "      all_embeddings.extend(item.embedding for item in response.data)\n",
        "\n",
        "    return all_embeddings if isinstance(text, list) else all_embeddings[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WuPndDhq9HA"
      },
      "source": [
        "## 反馈系统功能\n",
        "\n",
        "现在我们将实现反馈系统的核心组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPRGcpvnq9HA"
      },
      "outputs": [],
      "source": [
        "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
        "    \"\"\"\n",
        "    将用户反馈格式化为字典。\n",
        "\n",
        "    Args:\n",
        "        query (str): 用户查询\n",
        "        response (str): 系统响应\n",
        "        relevance (int): 相关性得分 (1-5)\n",
        "        quality (int): 质量得分 (1-5)\n",
        "        comments (str): 可选的反馈评论\n",
        "\n",
        "    Returns:\n",
        "        Dict: 格式化反馈\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"relevance\": int(relevance),\n",
        "        \"quality\": int(quality),\n",
        "        \"comments\": comments,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDJH_7DKq9HA"
      },
      "outputs": [],
      "source": [
        "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    存储反馈到一个 JSON 文件中.\n",
        "\n",
        "    Args:\n",
        "        feedback (Dict): 反馈数据\n",
        "        feedback_file (str): 反馈文件路径，默认为 feedback_data.json\n",
        "    \"\"\"\n",
        "    with open(feedback_file, \"a\") as f:\n",
        "        json.dump(feedback, f)\n",
        "        f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpWsnt5Yq9HA"
      },
      "outputs": [],
      "source": [
        "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    从文件中加载反馈数据。\n",
        "\n",
        "    Args:\n",
        "        feedback_file (str): 反馈文件路径\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: 反馈数据列表\n",
        "    \"\"\"\n",
        "    feedback_data = []\n",
        "    try:\n",
        "        with open(feedback_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    feedback_data.append(json.loads(line.strip()))\n",
        "    except FileNotFoundError:\n",
        "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
        "\n",
        "    return feedback_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve2GwFaiq9HB"
      },
      "source": [
        "## 带有反馈意识的文档处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8vLrsGPq9HB"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    为带反馈循环的检索增强生成（RAG）处理文档。\n",
        "    该函数处理完整的文档处理流程：\n",
        "    1. 从 PDF 中提取文本\n",
        "    2. 带重叠的文本分块\n",
        "    3. 为分块创建嵌入\n",
        "    4. 将其与元数据一起存储在向量数据库中\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): 要处理的 PDF 文件路径\n",
        "    chunk_size (int): 每个文本块的字符数量\n",
        "    chunk_overlap (int): 连续文本块之间重叠的字符数量\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[str], SimpleVectorStore]: A tuple 包含:\n",
        "        - 文本块列表\n",
        "        - 填充了嵌入向量和元数据的向量存储库\n",
        "    \"\"\"\n",
        "    # 步骤1：从PDF文档中提取原始文本内容\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # 步骤2：将文本分割成易于管理且有重叠的块，以更好地保留上下文\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # 步骤3：为每个文本块生成向量嵌入\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # 步骤4：初始化向量数据库\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # 步骤5：将每个块及其嵌入添加到向量存储库中\n",
        "    # 包括用于基于反馈改进的元数据\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\n",
        "                \"index\": i,                # 在源文档的位置\n",
        "                \"source\": pdf_path,        # 源文件路径\n",
        "                \"relevance_score\": 1.0,    # 初始相关性得分（将根据反馈进行更新）\n",
        "                \"feedback_count\": 0        # 此块收到的反馈数量\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return chunks, store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP51EbBnq9HB"
      },
      "source": [
        "## 基于反馈的相关性调整"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWBYkdySq9HB"
      },
      "outputs": [],
      "source": [
        "def assess_feedback_relevance(query, doc_text, feedback):\n",
        "    \"\"\"\n",
        "    使用大语言模型评估过去的反馈条目是否与当前查询和文档相关。\n",
        "\n",
        "    此功能通过将当前查询、过去的查询+反馈以及文档内容发送给大语言模型进行相关性评估，\n",
        "    从而帮助确定哪些过去的反馈应该影响当前的检索。\n",
        "\n",
        "    Args:\n",
        "        query (str): 需要进行检索的当前用户查询\n",
        "        doc_text (str): 需要被评估的文本内容\n",
        "        feedback (Dict): 包含“query”和“response”键的先前反馈数据\n",
        "\n",
        "    Returns:\n",
        "        bool: 如果反馈被认为与当前查询/文档相关，则为True，否则为False\n",
        "    \"\"\"\n",
        "    # 定义 system prompt，指示大语言模型仅进行二元相关性判断\n",
        "    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n",
        "    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n",
        "\n",
        "    # user prompt，如果上下文窗口不够，截断 doc_text 和 feedback\n",
        "    user_prompt = f\"\"\"\n",
        "    Current query: {query}\n",
        "    Past query that received feedback: {feedback['query']}\n",
        "    Document content: {doc_text}\n",
        "    Past response that received feedback: {feedback['response']}\n",
        "\n",
        "    Is this past feedback relevant to the current query and document? (yes/no)\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM API with zero temperature for deterministic output\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0  # Use temperature=0 for consistent, deterministic responses\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip().lower()\n",
        "    return 'yes' in answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Fe24U_q9HB"
      },
      "outputs": [],
      "source": [
        "def adjust_relevance_scores(query, results, feedback_data):\n",
        "    \"\"\"\n",
        "    根据历史反馈调整文档相关性得分，以提高检索质量。\n",
        "\n",
        "    此函数分析过去的用户反馈，动态调整检索文档的相关性得分。\n",
        "    它识别与当前查询上下文相关的反馈，根据相关性评分计算得分调整因子，\n",
        "    并相应地重新排序结果。\n",
        "\n",
        "    Args:\n",
        "        query (str): 当前用户查询\n",
        "        results (List[Dict]): 检索到的文档及其原始相似性得分\n",
        "        feedback_data (List[Dict]): 包含用户评分的历史反馈\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: 调整后的相关性得分，经过排序\n",
        "    \"\"\"\n",
        "    # 如果没有反馈数据，则返回原始结果\n",
        "    if not feedback_data:\n",
        "        return results\n",
        "\n",
        "    print(\"Adjusting relevance scores based on feedback history...\")\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        document_text = result[\"text\"]\n",
        "        relevant_feedback = []\n",
        "\n",
        "        # 通过查询大语言模型来评估每个历史反馈项的相关性，\n",
        "        # 为这个特定的文档和查询组合找到相关的反馈\n",
        "        for feedback in feedback_data:\n",
        "            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n",
        "            if is_relevant:\n",
        "                relevant_feedback.append(feedback)\n",
        "\n",
        "        # 调整得分\n",
        "        if relevant_feedback:\n",
        "            # 计算所有反馈条目的平均相关性评分\n",
        "            # 该评分采用1-5级评分（1表示不相关，5表示高度相关）\n",
        "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
        "\n",
        "            # 将平均相关性转换为0.5-1.5范围内的得分调整因子\n",
        "            # - 低于3/5的评分将降低原始相似度（调整因子 < 1.0）\n",
        "            # - 高于3/5的评分将提高原始相似度（调整因子 > 1.0）\n",
        "            modifier = 0.5 + (avg_relevance / 5.0)\n",
        "\n",
        "            # 调整原始相似度得分\n",
        "            original_score = result[\"similarity\"]\n",
        "            adjusted_score = original_score * modifier\n",
        "\n",
        "            # 更新数据\n",
        "            result[\"original_similarity\"] = original_score  # 原始分数\n",
        "            result[\"similarity\"] = adjusted_score           # 新的分数\n",
        "            result[\"relevance_score\"] = adjusted_score      # 新的相关性得分\n",
        "            result[\"feedback_applied\"] = True               # 标记已经根据反馈进行调整\n",
        "            result[\"feedback_count\"] = len(relevant_feedback)  # 使用的反馈数量\n",
        "\n",
        "            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n",
        "\n",
        "    # 排序，从大到小\n",
        "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSTDf14Nq9HB"
      },
      "source": [
        "## 使用反馈调整索引"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJsfAPvgq9HB"
      },
      "outputs": [],
      "source": [
        "def fine_tune_index(current_store, chunks, feedback_data):\n",
        "    \"\"\"\n",
        "    通过高质量反馈增强向量存储库，以提高检索质量。\n",
        "\n",
        "    该函数通过以下方式实现持续学习过程：\n",
        "    1. 识别高质量反馈（评分很高的问答对）\n",
        "    2. 从成功的交互中创建新的检索项\n",
        "    3. 将这些项以增强的相关性权重添加到向量存储库中\n",
        "\n",
        "    Args:\n",
        "        current_store (SimpleVectorStore): 当前向量存储\n",
        "        chunks (List[str]): 原始的文档分块\n",
        "        feedback_data (List[Dict]): 包含相关性和质量评分的历史用户反馈\n",
        "\n",
        "    Returns:\n",
        "        SimpleVectorStore: 调整后的向量数据库\n",
        "    \"\"\"\n",
        "    print(\"Fine-tuning index with high-quality feedback...\")\n",
        "\n",
        "    # 筛选高质量的反馈（相关性和质量大于等于 4）\n",
        "    # 这能确保我们从最成功的交互中学习\n",
        "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
        "\n",
        "    if not good_feedback:\n",
        "        print(\"No high-quality feedback found for fine-tuning.\")\n",
        "        return current_store\n",
        "\n",
        "    # 新的向量数据库\n",
        "    new_store = SimpleVectorStore()\n",
        "\n",
        "    # 迁移数据\n",
        "    for i in range(len(current_store.texts)):\n",
        "        new_store.add_item(\n",
        "            text=current_store.texts[i],\n",
        "            embedding=current_store.vectors[i],\n",
        "            metadata=current_store.metadata[i].copy()  # 使用copy进行复制\n",
        "        )\n",
        "\n",
        "    # 使用好的反馈创建增强的内容\n",
        "    for feedback in good_feedback:\n",
        "        # 格式化一个新的文档，将问题及其高质量答案结合起来\n",
        "        # 这创建了可以直接解决用户查询的可检索内容\n",
        "        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n",
        "\n",
        "        # 生成合成文档的嵌入向量\n",
        "        embedding = create_embeddings(enhanced_text)\n",
        "\n",
        "        new_store.add_item(\n",
        "            text=enhanced_text,\n",
        "            embedding=embedding,\n",
        "            metadata={\n",
        "                \"type\": \"feedback_enhanced\",  # 标记来自于反馈\n",
        "                \"query\": feedback[\"query\"],   # 原始查询\n",
        "                \"relevance_score\": 1.2,       # 提高初始相关性，以优先考虑这些项\n",
        "                \"feedback_count\": 1,          # 跟踪反馈的整合情况\n",
        "                \"original_feedback\": feedback # 完整的反馈记录\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n",
        "\n",
        "    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n",
        "    return new_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eag7E2CXq9HB"
      },
      "source": [
        "## 基于反馈循环的 RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0fJxR-Nq9HB"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=None):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text from retrieved documents\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = mode_name\n",
        "\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the OpenAI API to generate a response based on the system and user prompts\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0  # Use temperature=0 for consistent, deterministic responses\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91TRk1LWq9HC"
      },
      "outputs": [],
      "source": [
        "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model=None):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline incorporating feedback loop.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store with document chunks\n",
        "        feedback_data (List[Dict]): History of feedback\n",
        "        k (int): Number of documents to retrieve\n",
        "        model (str): LLM model for response generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, retrieved documents, and response\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = model_name\n",
        "\n",
        "    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Step 1: Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Step 2: Perform initial retrieval based on query embedding\n",
        "    results = vector_store.similarity_search(query_embedding, k=k)\n",
        "\n",
        "    # Step 3: Adjust relevance scores of retrieved documents based on feedback\n",
        "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
        "\n",
        "    # Step 4: Extract texts from adjusted results for context building\n",
        "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
        "\n",
        "    # Step 5: Build context for response generation by concatenating retrieved texts\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "    # Step 6: Generate response using the context and query\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(query, context, model)\n",
        "\n",
        "    # Step 7: Compile the final result\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": adjusted_results,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Response ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DwvewfJq9HC"
      },
      "source": [
        "## 完整工作流：从初始设置到反馈收集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za4lLZ4Fq9HC"
      },
      "outputs": [],
      "source": [
        "def full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
        "    \"\"\"\n",
        "    执行一个完整的带反馈整合的检索增强生成（RAG）工作流，以实现持续改进。\n",
        "\n",
        "    该函数协调整个检索增强生成过程：\n",
        "    1. 加载历史反馈数据\n",
        "    2. 处理和分块文档\n",
        "    3. 根据之前的反馈可选地微调向量索引\n",
        "    4. 使用反馈调整的相关性得分执行检索和生成\n",
        "    5. 收集新的用户反馈以供未来的改进\n",
        "    6. 存储反馈以实现系统的持续学习\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): PDF 文档路径\n",
        "        query (str): 用户查询\n",
        "        feedback_data (List[Dict], optional): 预加载的反馈数据，如果为 None，从文件加载\n",
        "        feedback_file (str): 存储反馈历史的 JSON 文件路径\n",
        "        fine_tune (bool): 是否用成功的过去问答对增强索引\n",
        "\n",
        "    Returns:\n",
        "        Dict: 包含响应和检索元数据的结果\n",
        "    \"\"\"\n",
        "    # Step 1: Load historical feedback for relevance adjustment if not explicitly provided\n",
        "    if feedback_data is None:\n",
        "        feedback_data = load_feedback_data(feedback_file)\n",
        "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
        "\n",
        "    # Step 2: Process document through extraction, chunking and embedding pipeline\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Step 3: Fine-tune the vector index by incorporating high-quality past interactions\n",
        "    # This creates enhanced retrievable content from successful Q&A pairs\n",
        "    if fine_tune and feedback_data:\n",
        "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
        "\n",
        "    # Step 4: Execute core RAG with feedback-aware retrieval\n",
        "    # Note: This depends on the rag_with_feedback_loop function which should be defined elsewhere\n",
        "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
        "\n",
        "    # Step 5: Collect user feedback to improve future performance\n",
        "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
        "    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n",
        "    relevance = input()\n",
        "\n",
        "    print(\"Rate quality (1-5, with 5 being highest quality):\")\n",
        "    quality = input()\n",
        "\n",
        "    print(\"Any comments? (optional, press Enter to skip)\")\n",
        "    comments = input()\n",
        "\n",
        "    # Step 6: Format feedback into structured data\n",
        "    feedback = get_user_feedback(\n",
        "        query=query,\n",
        "        response=result[\"response\"],\n",
        "        relevance=int(relevance),\n",
        "        quality=int(quality),\n",
        "        comments=comments\n",
        "    )\n",
        "\n",
        "    # Step 7: Persist feedback to enable continuous system learning\n",
        "    store_feedback(feedback, feedback_file)\n",
        "    print(\"Feedback recorded. Thank you!\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FTvlL1lq9HC"
      },
      "source": [
        "## 评估反馈循环"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B-GuzVoq9HC"
      },
      "outputs": [],
      "source": [
        "def evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    通过比较反馈整合前后的性能，评估反馈循环对RAG质量的影响。\n",
        "\n",
        "    该函数运行一个对照实验，以衡量整合反馈对检索和生成的影响：\n",
        "    1. 第一轮：运行所有测试查询，不使用反馈\n",
        "    2. 根据参考答案（如果提供）生成合成反馈\n",
        "    3. 第二轮：使用反馈增强的检索运行相同的查询\n",
        "    4. 比较两轮之间的结果，以量化反馈的影响\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF document used as the knowledge base\n",
        "        test_queries (List[str]): List of test queries to evaluate system performance\n",
        "        reference_answers (List[str], optional): Reference/gold standard answers for evaluation\n",
        "                                                and synthetic feedback generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results containing:\n",
        "            - round1_results: Results without feedback\n",
        "            - round2_results: Results with feedback\n",
        "            - comparison: Quantitative comparison metrics between rounds\n",
        "    \"\"\"\n",
        "    print(\"=== Evaluating Feedback Loop Impact ===\")\n",
        "\n",
        "    # Create a temporary feedback file for this evaluation session only\n",
        "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
        "\n",
        "    # Initialize feedback collection (empty at the start)\n",
        "    feedback_data = []\n",
        "\n",
        "    # ----------------------- FIRST EVALUATION ROUND -----------------------\n",
        "    # Run all queries without any feedback influence to establish baseline performance\n",
        "    print(\"\\n=== ROUND 1: NO FEEDBACK ===\")\n",
        "    round1_results = []\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # Process document to create initial vector store\n",
        "        chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "        # Execute RAG without feedback influence (empty feedback list)\n",
        "        result = rag_with_feedback_loop(query, vector_store, [])\n",
        "        round1_results.append(result)\n",
        "\n",
        "        # Generate synthetic feedback if reference answers are available\n",
        "        # This simulates user feedback for training the system\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            # Calculate synthetic feedback scores based on similarity to reference answer\n",
        "            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n",
        "            # Convert similarity (0-1) to rating scale (1-5)\n",
        "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
        "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
        "\n",
        "            # Create structured feedback entry\n",
        "            feedback = get_user_feedback(\n",
        "                query=query,\n",
        "                response=result[\"response\"],\n",
        "                relevance=relevance,\n",
        "                quality=quality,\n",
        "                comments=f\"Synthetic feedback based on reference similarity: {similarity_to_ref:.2f}\"\n",
        "            )\n",
        "\n",
        "            # Add to in-memory collection and persist to temporary file\n",
        "            feedback_data.append(feedback)\n",
        "            store_feedback(feedback, temp_feedback_file)\n",
        "\n",
        "    # ----------------------- SECOND EVALUATION ROUND -----------------------\n",
        "    # Run the same queries with feedback incorporation to measure improvement\n",
        "    print(\"\\n=== ROUND 2: WITH FEEDBACK ===\")\n",
        "    round2_results = []\n",
        "\n",
        "    # Process document and enhance with feedback-derived content\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # Execute RAG with feedback influence\n",
        "        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
        "        round2_results.append(result)\n",
        "\n",
        "    # ----------------------- RESULTS ANALYSIS -----------------------\n",
        "    # Compare performance metrics between the two rounds\n",
        "    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n",
        "\n",
        "    # Clean up temporary evaluation artifacts\n",
        "    if os.path.exists(temp_feedback_file):\n",
        "        os.remove(temp_feedback_file)\n",
        "\n",
        "    return {\n",
        "        \"round1_results\": round1_results,\n",
        "        \"round2_results\": round2_results,\n",
        "        \"comparison\": comparison\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ1KuFwfq9HC"
      },
      "source": [
        "## 辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LsY_keJq9HC"
      },
      "outputs": [],
      "source": [
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"\n",
        "    Calculate semantic similarity between two texts using embeddings.\n",
        "\n",
        "    Args:\n",
        "        text1 (str): First text\n",
        "        text2 (str): Second text\n",
        "\n",
        "    Returns:\n",
        "        float: Similarity score between 0 and 1\n",
        "    \"\"\"\n",
        "    # Generate embeddings for both texts\n",
        "    embedding1 = create_embeddings(text1)\n",
        "    embedding2 = create_embeddings(text2)\n",
        "\n",
        "    # Convert embeddings to numpy arrays\n",
        "    vec1 = np.array(embedding1)\n",
        "    vec2 = np.array(embedding2)\n",
        "\n",
        "    # Calculate cosine similarity between the two vectors\n",
        "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro7HDKXxq9HC"
      },
      "outputs": [],
      "source": [
        "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Compare results from two rounds of RAG.\n",
        "\n",
        "    Args:\n",
        "        queries (List[str]): Test queries\n",
        "        round1_results (List[Dict]): Results from round 1\n",
        "        round2_results (List[Dict]): Results from round 2\n",
        "        reference_answers (List[str], optional): Reference answers\n",
        "\n",
        "    Returns:\n",
        "        str: Comparison analysis\n",
        "    \"\"\"\n",
        "    print(\"\\n=== COMPARING RESULTS ===\")\n",
        "\n",
        "    # System prompt to guide the AI's evaluation behavior\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from two versions:\n",
        "        1. Standard RAG: No feedback used\n",
        "        2. Feedback-enhanced RAG: Uses a feedback loop to improve retrieval\n",
        "\n",
        "        Analyze which version provides better responses in terms of:\n",
        "        - Relevance to the query\n",
        "        - Accuracy of information\n",
        "        - Completeness\n",
        "        - Clarity and conciseness\n",
        "    \"\"\"\n",
        "\n",
        "    comparisons = []\n",
        "\n",
        "    # Iterate over each query and its corresponding results from both rounds\n",
        "    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n",
        "        # Create a prompt for comparing the responses\n",
        "        comparison_prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "\n",
        "        Standard RAG Response:\n",
        "        {r1[\"response\"]}\n",
        "\n",
        "        Feedback-enhanced RAG Response:\n",
        "        {r2[\"response\"]}\n",
        "        \"\"\"\n",
        "\n",
        "        # Include reference answer if available\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            comparison_prompt += f\"\"\"\n",
        "            Reference Answer:\n",
        "            {reference_answers[i]}\n",
        "            \"\"\"\n",
        "\n",
        "        comparison_prompt += \"\"\"\n",
        "        Compare these responses and explain which one is better and why.\n",
        "        Focus specifically on how the feedback loop has (or hasn't) improved the response quality.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call the OpenAI API to generate a comparison analysis\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": comparison_prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        # Append the comparison analysis to the results\n",
        "        comparisons.append({\n",
        "            \"query\": query,\n",
        "            \"analysis\": response.choices[0].message.content\n",
        "        })\n",
        "\n",
        "        # Print a snippet of the analysis for each query\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "        print(f\"Analysis: {response.choices[0].message.content[:200]}...\")\n",
        "\n",
        "    return comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47J371Aq9HD"
      },
      "source": [
        "## 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0HiwwEd54rI",
        "outputId": "ddbeaa89-e896-44a4-b4bc-416157262e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai_PzHbvq9HD",
        "outputId": "130ce478-db0c-462c-b022-fcd0cbf8f96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluating Feedback Loop Impact ===\n",
            "\n",
            "=== ROUND 1: NO FEEDBACK ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "\n",
            "=== Processing query with feedback-enhanced RAG ===\n",
            "Query: What is a neural network and how does it function?\n",
            "Generating response...\n",
            "\n",
            "=== Response ===\n",
            "A neural network is a type of artificial intelligence model inspired by the structure and function of the human brain. It uses multiple layers (deep neural networks) to analyze data.\n",
            "\n",
            "Convolutional Neural Networks (CNNs) are a specific type of neural network particularly effective for processing images and videos. They use convolutional layers to automatically learn features from the input data and are widely used in object detection, facial recognition, and medical image analysis.\n",
            "\n",
            "Recurrent Neural Networks (RNNs) are designed to process sequential data such as text and time series. They have feedback connections that allow information to persist over time, making them suitable for tasks like language translation, speech recognition, and sentiment analysis.\n",
            "\n",
            "In general, neural networks work by processing input data through successive layers of interconnected nodes or neurons. Each layer performs a specific transformation or calculation on the input, and the output of one layer becomes the input for the next layer. Through training with large amounts of data and appropriate algorithms, the neural network learns patterns and relationships in the data to make predictions or perform other tasks. \n",
            "\n",
            "=== ROUND 2: WITH FEEDBACK ===\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Fine-tuning index with high-quality feedback...\n",
            "Added enhanced content from feedback: What is a neural network and how does it function?...\n",
            "Fine-tuned index now has 43 items (original: 42)\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "\n",
            "=== Processing query with feedback-enhanced RAG ===\n",
            "Query: What is a neural network and how does it function?\n",
            "Adjusting relevance scores based on feedback history...\n",
            "  Document 1: Adjusted score from 0.8267 to 1.0747 based on 1 feedback(s)\n",
            "  Document 2: Adjusted score from 0.7600 to 0.9880 based on 1 feedback(s)\n",
            "  Document 3: Adjusted score from 0.7351 to 0.9557 based on 1 feedback(s)\n",
            "  Document 4: Adjusted score from 0.7221 to 0.9387 based on 1 feedback(s)\n",
            "  Document 5: Adjusted score from 0.7143 to 0.9286 based on 1 feedback(s)\n",
            "Generating response...\n",
            "\n",
            "=== Response ===\n",
            "A neural network is a type of artificial intelligence model inspired by the structure and function of the human brain. It uses multiple layers (deep neural networks) to analyze data.\n",
            "\n",
            "Convolutional Neural Networks (CNNs) are a specific type of neural network particularly effective for processing images and videos. They use convolutional layers to automatically learn features from the input data and are widely used in object detection, facial recognition, and medical image analysis.\n",
            "\n",
            "Recurrent Neural Networks (RNNs) are designed to process sequential data such as text and time series. They have feedback connections that allow information to persist over time, making them suitable for tasks like language translation, speech recognition, and sentiment analysis.\n",
            "\n",
            "In general, neural networks work by processing input data through successive layers of interconnected nodes or neurons. Each layer performs a specific transformation or calculation on the input, and the output of one layer becomes the input for the next layer. Through training with large amounts of data and appropriate algorithms, the neural network learns patterns and relationships in the data to make predictions or perform other tasks. \n",
            "\n",
            "=== COMPARING RESULTS ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "Analysis: In terms of relevance to the query, both the standard RAG response and the feedback-enhanced RAG response are highly relevant. They provide a clear and comprehensive explanation of what a neural netwo...\n"
          ]
        }
      ],
      "source": [
        "# AI Document Path\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\"\n",
        "\n",
        "# Define test queries\n",
        "test_queries = [\n",
        "    \"What is a neural network and how does it function?\",\n",
        "\n",
        "    #################################################################################\n",
        "    ### Commented out queries to reduce the number of queries for testing purposes ###\n",
        "\n",
        "    # \"Describe the process and applications of reinforcement learning.\",\n",
        "    # \"What are the main applications of natural language processing in today's technology?\",\n",
        "    # \"Explain the impact of overfitting in machine learning models and how it can be mitigated.\"\n",
        "]\n",
        "\n",
        "# Define reference answers for evaluation\n",
        "reference_answers = [\n",
        "    \"A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. It consists of layers of nodes, with each node representing a neuron. Neural networks function by adjusting the weights of connections between nodes based on the error of the output compared to the expected result.\",\n",
        "\n",
        "    ############################################################################################\n",
        "    #### Commented out reference answers to reduce the number of queries for testing purposes ###\n",
        "\n",
        "#     \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. It involves exploration, exploitation, and learning from the consequences of actions. Applications include robotics, game playing, and autonomous vehicles.\",\n",
        "#     \"The main applications of natural language processing in today's technology include machine translation, sentiment analysis, chatbots, information retrieval, text summarization, and speech recognition. NLP enables machines to understand and generate human language, facilitating human-computer interaction.\",\n",
        "#     \"Overfitting in machine learning models occurs when a model learns the training data too well, capturing noise and outliers. This results in poor generalization to new data, as the model performs well on training data but poorly on unseen data. Mitigation techniques include cross-validation, regularization, pruning, and using more training data.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = evaluate_feedback_loop(\n",
        "    pdf_path=pdf_path,\n",
        "    test_queries=test_queries,\n",
        "    reference_answers=reference_answers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLXnCTcCq9HD"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# # Run a full RAG workflow\n",
        "########################################\n",
        "\n",
        "# # Run an interactive example\n",
        "# print(\"\\n\\n=== INTERACTIVE EXAMPLE ===\")\n",
        "# print(\"Enter your query about AI:\")\n",
        "# user_query = input()\n",
        "\n",
        "# # Load accumulated feedback\n",
        "# all_feedback = load_feedback_data()\n",
        "\n",
        "# # Run full workflow\n",
        "# result = full_rag_workflow(\n",
        "#     pdf_path=pdf_path,\n",
        "#     query=user_query,\n",
        "#     feedback_data=all_feedback,\n",
        "#     fine_tune=True\n",
        "# )\n",
        "\n",
        "########################################\n",
        "# # Run a full RAG workflow\n",
        "########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyYbaCkq9HD"
      },
      "source": [
        "## 可视化反馈的影响"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CIaE1E9q9HD",
        "outputId": "ed345528-ef01-4585-856a-0989e1793d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FEEDBACK IMPACT ANALYSIS ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "\n",
            "Analysis of feedback impact:\n",
            "In terms of relevance to the query, both the standard RAG response and the feedback-enhanced RAG response are highly relevant. They provide a clear and comprehensive explanation of what a neural network is and how different types (CNNs and RNNs) function.\n",
            "\n",
            "In terms of accuracy of information, both responses are accurate. They cover the main concepts and characteristics of neural networks, convolutional neural networks, and recurrent neural networks.\n",
            "\n",
            "In terms of completeness, both responses are quite complete. They provide detailed explanations of the different types of neural networks and their applications.\n",
            "\n",
            "However, in terms of clarity and conciseness, the reference answer might be slightly better. It provides a more concise and straightforward description of how neural networks function by emphasizing the adjustment of weights based on error. The standard RAG response and the feedback-enhanced RAG response are more detailed but might be a bit wordy.\n",
            "\n",
            "The feedback loop in the feedback-enhanced RAG system did not seem to have a significant impact on the improvement of the response quality in this case. Both versions provide similar and accurate information, and the reference answer offers a more concise formulation. It's possible that the specific nature of the query and the existing knowledge in the RAG systems did not lead to a notable improvement with the feedback loop. \n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Response length comparison (proxy for completeness):\n",
            "Round 1: 1212.0 chars\n"
          ]
        }
      ],
      "source": [
        "# Extract the comparison data which contains the analysis of feedback impact\n",
        "comparisons = evaluation_results['comparison']\n",
        "\n",
        "# Print out the analysis results to visualize feedback impact\n",
        "print(\"\\n=== FEEDBACK IMPACT ANALYSIS ===\\n\")\n",
        "for i, comparison in enumerate(comparisons):\n",
        "    print(f\"Query {i+1}: {comparison['query']}\")\n",
        "    print(f\"\\nAnalysis of feedback impact:\")\n",
        "    print(comparison['analysis'])\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Additionally, we can compare some metrics between rounds\n",
        "round_responses = [evaluation_results[f'round{round_num}_results'] for round_num in range(1, len(evaluation_results) - 1)]\n",
        "response_lengths = [[len(r[\"response\"]) for r in round] for round in round_responses]\n",
        "\n",
        "print(\"\\nResponse length comparison (proxy for completeness):\")\n",
        "avg_lengths = [sum(lengths) / len(lengths) for lengths in response_lengths]\n",
        "for round_num, avg_len in enumerate(avg_lengths, start=1):\n",
        "    print(f\"Round {round_num}: {avg_len:.1f} chars\")\n",
        "\n",
        "if len(avg_lengths) > 1:\n",
        "    changes = [(avg_lengths[i] - avg_lengths[i-1]) / avg_lengths[i-1] * 100 for i in range(1, len(avg_lengths))]\n",
        "    for round_num, change in enumerate(changes, start=2):\n",
        "        print(f\"Change from Round {round_num-1} to Round {round_num}: {change:.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}