{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/09_rse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "XYvhEzzZHMia"
      },
      "source": [
        "# 面向增强型 RAG 的相关片段提取（RSE）\n",
        "\n",
        "在本笔记本中，我们实现了一种相关片段提取（Relevant Segment Extraction, RSE）技术，以提升我们 RAG 系统中的上下文质量。我们不再简单地检索一组孤立的片段，而是识别并重构连续的文本段落，为我们的语言模型提供更好的上下文。\n",
        "\n",
        "## 核心概念\n",
        "\n",
        "相关的片段往往会在文档中聚集在一起。通过识别这些聚集区域并保留它们的连续性，我们为大语言模型提供了更连贯的上下文。\n",
        "\n",
        "## 概述\n",
        "\n",
        "相关段落提取（RSE）是一种从检索到的文本块中重建连续多块文本段的方法。该步骤发生在向量搜索（以及可选的重排序）之后，但在将检索到的上下文呈现给大语言模型（LLM）之前。这种方法确保相邻的文本块按照它们在原始文档中的顺序呈现给LLM。同时，它还会加入那些未被标记为相关但夹在高相关块之间的文本块，从而进一步改善提供给LLM的上下文。如本笔记本末尾所示的评估结果，这种方法能显著提升检索增强生成（RAG）的性能。\n",
        "\n",
        "## 动机\n",
        "\n",
        "在为RAG对文档进行分块时，选择合适的分块大小需要在多种权衡之间进行考量。大块文本能为LLM提供比小块更好的上下文，但也使得精确检索特定信息变得更加困难。有些查询（如简单的知识型问题）最适合用小块处理，而另一些查询（如高层次的问题）则需要非常大的块。有些查询可以通过文档中的一句话来回答，而另一些查询则需要整个章节才能妥善解答。大多数现实世界的RAG应用场景都会面临这些类型查询的组合。\n",
        "\n",
        "我们真正需要的是一个更动态的系统：当只需要短块时能检索短块，而在需要时也能检索非常大的块。那么，我们该如何实现这一点呢？\n",
        "\n",
        "我们的解决方案源于一个简单的洞察：**相关的文本块往往在其原始文档中聚集在一起**。\n",
        "\n",
        "## 关键组件\n",
        "\n",
        "#### 文本块键值存储\n",
        "RSE需要能够通过doc_id和chunk_index作为键快速从数据库中检索文本块内容。这是因为并非所有需要包含在给定段落中的文本块都会在初始搜索结果中被返回。因此，除了向量数据库外，可能还需要使用某种键值存储。\n",
        "\n",
        "## 方法细节\n",
        "\n",
        "#### 文档分块\n",
        "可以使用标准的文档分块方法。这里唯一的特殊要求是文档分块时不能有重叠。这样我们就可以通过连接文本块来重建文档的各个部分（即段落）。\n",
        "\n",
        "#### RSE优化\n",
        "在完成标准的文本块检索过程（理想情况下包括重排序步骤）后，就可以开始RSE过程。第一步是将相似度得分和相关性排名结合起来得到文本块值（chunk value）。这比单独使用相似度得分或单独使用排名提供了更稳健的起点。\n",
        "\n",
        "然后我们从每个文本块的值中减去一个常数阈值（假设为0.2），使得不相关的文本块具有负值（最低可达-0.2），而相关的文本块具有正值（最高可达0.8）。通过这种方式计算文本块值，我们可以将段落值定义为文本块值的总和。\n",
        "\n",
        "例如，假设文档中0-4号文本块的值如下：[-0.2, -0.2, 0.4, 0.8, -0.1]。那么仅包含2-3号文本块的段落值就是0.4+0.8=1.2。\n",
        "\n",
        "寻找最佳段落就变成了一个带约束的最大子数组和问题。我们使用带有少量启发式规则的暴力搜索来提高效率。这通常耗时约5-10毫秒。\n",
        "\n",
        "![RSE](https://github.com/NirDiamant/RAG_Techniques/blob/main/images/relevant-segment-extraction.svg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INXDCpz3HMic"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "nmGdHkIMIjgm",
        "outputId": "b83e26ab-689d-4358-ff4c-33aadd28b0a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zDT0_VENHMic"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l8qW5FVHMic"
      },
      "source": [
        "## 从 PDF 文件提取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "52AsqS91HMid"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sclLLHi2HMid"
      },
      "source": [
        "## 分块\n",
        "一旦我们提取了文本，我们会将其分成更小的、有重叠部分的块，以提高检索的准确性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8sFSHOOYHMid"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=800, overlap=0):\n",
        "    \"\"\"\n",
        "    Split text into non-overlapping chunks.\n",
        "    For RSE, we typically want non-overlapping chunks so we can reconstruct segments properly.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "        overlap (int): Overlap between chunks in characters\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Simple character-based chunking\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        if chunk:  # Ensure we don't add empty chunks\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B43kD62HMid"
      },
      "source": [
        "## OpenAI API Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "7PeQ7TBUIz-r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fJSvgdFnHMid"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1egvHV1HMid"
      },
      "source": [
        "## 构建向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uUqwdUlxHMid"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A lightweight vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, dimension=1536):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "\n",
        "        Args:\n",
        "            dimension (int): Dimension of embeddings\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.vectors = []\n",
        "        self.documents = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_documents(self, documents, vectors=None, metadata=None):\n",
        "        \"\"\"\n",
        "        Add documents to the vector store.\n",
        "\n",
        "        Args:\n",
        "            documents (List[str]): List of document chunks\n",
        "            vectors (List[List[float]], optional): List of embedding vectors\n",
        "            metadata (List[Dict], optional): List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        if vectors is None:\n",
        "            vectors = [None] * len(documents)\n",
        "\n",
        "        if metadata is None:\n",
        "            metadata = [{} for _ in range(len(documents))]\n",
        "\n",
        "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
        "            self.documents.append(doc)\n",
        "            self.vectors.append(vec)\n",
        "            self.metadata.append(meta)\n",
        "\n",
        "    def search(self, query_vector, top_k=5):\n",
        "        \"\"\"\n",
        "        Search for most similar documents.\n",
        "\n",
        "        Args:\n",
        "            query_vector (List[float]): Query embedding vector\n",
        "            top_k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of results with documents, scores, and metadata\n",
        "        \"\"\"\n",
        "        if not self.vectors or not self.documents:\n",
        "            return []\n",
        "\n",
        "        # Convert query vector to numpy array\n",
        "        query_array = np.array(query_vector)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            if vector is not None:\n",
        "                # Compute cosine similarity\n",
        "                similarity = np.dot(query_array, vector) / (\n",
        "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
        "                )\n",
        "                similarities.append((i, similarity))\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top-k results\n",
        "        results = []\n",
        "        for i, score in similarities[:top_k]:\n",
        "            results.append({\n",
        "                \"document\": self.documents[i],\n",
        "                \"score\": float(score),\n",
        "                \"metadata\": self.metadata[i]\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8NCP8lpHMie"
      },
      "source": [
        "## 创建嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w-iaYUZqHMie"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(texts, model=\"doubao-embedding-text-240715\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Generate embeddings for texts.\n",
        "\n",
        "    Args:\n",
        "        texts (List[str]): List of texts to embed\n",
        "        model (str): Embedding model to use\n",
        "        batch_size (int): size of a batch, for processing. Default is 100\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: List of embedding vectors\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return []  # Return an empty list if no texts are provided\n",
        "\n",
        "    all_embeddings = []  # Initialize a list to store all embeddings\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
        "\n",
        "        # Create embeddings for the current batch using the specified model\n",
        "        response = client.embeddings.create(\n",
        "            input=batch,\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Extract embeddings from the response\n",
        "        batch_embeddings = [item.embedding for item in response.data]\n",
        "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
        "\n",
        "    return all_embeddings  # Return the list of all embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8NuW0THMie"
      },
      "source": [
        "## 处理文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HOwTCw7UHMie"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=800):\n",
        "    \"\"\"\n",
        "    Process a document for use with RSE.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF document\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n",
        "    \"\"\"\n",
        "    print(\"Extracting text from document...\")\n",
        "    # Extract text from the PDF file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Chunking text into non-overlapping segments...\")\n",
        "    # 将提取的文本分割成不重叠的段落\n",
        "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "    print(\"Generating embeddings for chunks...\")\n",
        "    # Generate embeddings for the text chunks\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Create an instance of the SimpleVectorStore\n",
        "    vector_store = SimpleVectorStore()\n",
        "\n",
        "    # Add documents with metadata (including chunk index for later reconstruction)\n",
        "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
        "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
        "\n",
        "    # Track original document structure for segment reconstruction\n",
        "    doc_info = {\n",
        "        \"chunks\": chunks,\n",
        "        \"source\": pdf_path,\n",
        "    }\n",
        "\n",
        "    return chunks, vector_store, doc_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISY8-g9zHMie"
      },
      "source": [
        "## RSE 核心算法：计算文本块值并找到最佳段落\n",
        "现在我们已经具备了处理文档和为其片段生成嵌入向量的必要函数，可以实现 RSE 的核心算法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4tPOC4LMHMie"
      },
      "outputs": [],
      "source": [
        "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
        "    \"\"\"\n",
        "    结合相关性和排名计算文本块值.\n",
        "\n",
        "    Args:\n",
        "        query (str): Query text\n",
        "        chunks (List[str]): List of document chunks\n",
        "        vector_store (SimpleVectorStore): Vector store containing the chunks\n",
        "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of chunk values\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = create_embeddings([query])[0]\n",
        "\n",
        "    # Get all chunks with similarity scores\n",
        "    num_chunks = len(chunks)\n",
        "    results = vector_store.search(query_embedding, top_k=num_chunks)\n",
        "\n",
        "    # Create a mapping of chunk_index to relevance score\n",
        "    relevance_scores = {\n",
        "        result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results\n",
        "    }\n",
        "\n",
        "    # 对排名进行衰减，避免过大\n",
        "    decay_rate = 30\n",
        "\n",
        "    # Calculate chunk values (relevance score minus penalty)\n",
        "    chunk_values = []\n",
        "    for i in range(num_chunks):\n",
        "        # Get relevance score or default to 0 if not in results\n",
        "        score = relevance_scores.get(i, 0.0)\n",
        "        # 融合排名和相似度得分\n",
        "        fusion_value = np.exp(-i / decay_rate) * score\n",
        "        # 应用惩罚以将值转换为使不相关片段具有负值\n",
        "        value = fusion_value - irrelevant_chunk_penalty\n",
        "        chunk_values.append(value)\n",
        "\n",
        "    return chunk_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LZjYcDk0HMie"
      },
      "outputs": [],
      "source": [
        "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
        "    \"\"\"\n",
        "    使用最大子数组和算法的一个变体来找到最佳段落。\n",
        "\n",
        "    Args:\n",
        "        chunk_values (List[float]): Values for each chunk\n",
        "        max_segment_length (int): Maximum length of a single segment.\n",
        "          The lenght of a segment is the number of chunks in this segment.\n",
        "        total_max_length (int): Maximum total length across all segments\n",
        "        min_segment_value (float): Minimum value for a segment to be considered\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Tuple[int, int]], List[float]]: List of (start, end) indices for best segments and List of score of each segment\n",
        "    \"\"\"\n",
        "    print(\"Finding optimal continuous text segments...\")\n",
        "\n",
        "    # a segment: (start, end)\n",
        "    best_segments = []\n",
        "    segment_scores = []\n",
        "    total_included_chunks = 0\n",
        "\n",
        "    # Keep finding segments until we hit our limits\n",
        "    while total_included_chunks < total_max_length:\n",
        "        best_score = min_segment_value  # Minimum threshold for a segment\n",
        "        best_segment = None\n",
        "\n",
        "        # Try each possible starting position\n",
        "        for start in range(len(chunk_values)):\n",
        "            # Skip if this start position is already in a selected segment\n",
        "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
        "                continue\n",
        "\n",
        "            # Try each possible segment length\n",
        "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
        "                end = start + length\n",
        "\n",
        "                # Skip if end position is already in a selected segment\n",
        "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
        "                    continue\n",
        "\n",
        "                # Calculate segment value as sum of chunk values\n",
        "                segment_value = sum(chunk_values[start:end])\n",
        "\n",
        "                # Update best segment if this one is better\n",
        "                if segment_value > best_score:\n",
        "                    best_score = segment_value\n",
        "                    best_segment = (start, end)\n",
        "\n",
        "        # If we found a good segment, add it\n",
        "        if best_segment:\n",
        "            best_segments.append(best_segment)\n",
        "            segment_scores.append(best_score)\n",
        "            total_included_chunks += best_segment[1] - best_segment[0] # length of this segment\n",
        "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
        "        else:\n",
        "            # No more good segments to find\n",
        "            break\n",
        "\n",
        "    # Sort segments by their starting position for readability\n",
        "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
        "\n",
        "    return best_segments, segment_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTGgG0PgHMif"
      },
      "source": [
        "## 重建段落并将其应用于RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vAKasCqKHMif"
      },
      "outputs": [],
      "source": [
        "def reconstruct_segments(chunks, best_segments):\n",
        "    \"\"\"\n",
        "    Reconstruct text segments based on chunk indices.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of all document chunks\n",
        "        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of reconstructed text segments\n",
        "    \"\"\"\n",
        "    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n",
        "\n",
        "    for start, end in best_segments:\n",
        "        # Join the chunks in this segment to form the complete segment text\n",
        "        segment_text = \" \".join(chunks[start:end])\n",
        "        # Append the segment text and its range to the reconstructed_segments list\n",
        "        reconstructed_segments.append({\n",
        "            \"text\": segment_text,\n",
        "            \"segment_range\": (start, end),\n",
        "        })\n",
        "\n",
        "    return reconstructed_segments  # Return the list of reconstructed text segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mUo9tRX0HMif"
      },
      "outputs": [],
      "source": [
        "def format_segments_for_context(segments):\n",
        "    \"\"\"\n",
        "    Format segments into a context string for the LLM.\n",
        "\n",
        "    Args:\n",
        "        segments (List[Dict]): List of segment dictionaries\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted context text\n",
        "    \"\"\"\n",
        "    context = []  # Initialize an empty list to store the formatted context\n",
        "\n",
        "    for i, segment in enumerate(segments):\n",
        "        # Create a header for each segment with its index and chunk range\n",
        "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
        "        context.append(segment_header)  # Add the segment header to the context list\n",
        "        context.append(segment['text'])  # Add the segment text to the context list\n",
        "        context.append(\"-\" * 80)  # Add a separator line for readability\n",
        "\n",
        "    # Join all elements in the context list with double newlines and return the result\n",
        "    return \"\\n\\n\".join(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfS5ptd2HMif"
      },
      "source": [
        "## 生成响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VaqHBcqUHMif"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"doubao-lite-128k-240828\"):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text from relevant segments\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    print(\"Generating response using relevant segments as context...\")\n",
        "\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "    The context consists of document segments that have been retrieved as relevant to the user's query.\n",
        "    Use the information from these segments to provide a comprehensive and accurate answer.\n",
        "    If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a helpful answer based on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate the response using the specified model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930aPbp6HMif"
      },
      "source": [
        "## 完成 RSE pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7WoFVFlqHMif"
      },
      "outputs": [],
      "source": [
        "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with Relevant Segment Extraction.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        chunk_size (int): Size of chunks\n",
        "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
        "\n",
        "    Returns:\n",
        "        Dict: Result with query, segments, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
        "\n",
        "    # Calculate relevance scores and chunk values based on the query\n",
        "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
        "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
        "\n",
        "    # Find the best segments of text based on chunk values\n",
        "    best_segments, scores = find_best_segments(\n",
        "        chunk_values,\n",
        "        max_segment_length=20,\n",
        "        total_max_length=30,\n",
        "        min_segment_value=0.2\n",
        "    )\n",
        "\n",
        "    # Reconstruct text segments from the best chunks\n",
        "    print(\"\\nReconstructing text segments from chunks...\")\n",
        "    segments = reconstruct_segments(chunks, best_segments)\n",
        "\n",
        "    # Format the segments into a context string for the language model\n",
        "    context = format_segments_for_context(segments)\n",
        "\n",
        "    # Generate a response from the language model using the context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Compile the result into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"segments\": segments,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== FINAL RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89PGCQAjHMif"
      },
      "source": [
        "## 与标准检索比较\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Jp_dFxjHHMif"
      },
      "outputs": [],
      "source": [
        "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
        "    \"\"\"\n",
        "    Standard RAG with top-k retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        k (int): Number of chunks to retrieve\n",
        "        chunk_size (int): Size of chunks\n",
        "\n",
        "    Returns:\n",
        "        Dict: Result with query, chunks, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
        "\n",
        "    # Create an embedding for the query\n",
        "    print(\"Creating query embedding and retrieving chunks...\")\n",
        "    query_embedding = create_embeddings([query])[0]\n",
        "\n",
        "    # Retrieve the top-k most relevant chunks based on the query embedding\n",
        "    results = vector_store.search(query_embedding, top_k=k)\n",
        "    retrieved_chunks = [result[\"document\"] for result in results]\n",
        "\n",
        "    # Format the retrieved chunks into a context string\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"CHUNK {i+1}:\\n{chunk}\"\n",
        "        for i, chunk in enumerate(retrieved_chunks)\n",
        "    ])\n",
        "\n",
        "    # Generate a response from the language model using the context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Compile the result into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"chunks\": retrieved_chunks,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== FINAL RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx-VFMIsHMif"
      },
      "source": [
        "## 评估 RES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "G9KDRuGhHMig"
      },
      "outputs": [],
      "source": [
        "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Compare RSE with standard top-k retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        reference_answer (str, optional): Reference answer for evaluation\n",
        "    \"\"\"\n",
        "    print(\"\\n========= EVALUATION =========\\n\")\n",
        "\n",
        "    # Run the RAG with Relevant Segment Extraction (RSE) method\n",
        "    rse_result = rag_with_rse(pdf_path, query)\n",
        "\n",
        "    # Run the standard top-k retrieval method\n",
        "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
        "\n",
        "    # If a reference answer is provided, evaluate the responses\n",
        "    if reference_answer:\n",
        "        print(\"\\n=== COMPARING RESULTS ===\")\n",
        "\n",
        "        # Create an evaluation prompt to compare the responses against the reference answer\n",
        "        evaluation_prompt = f\"\"\"\n",
        "            Query: {query}\n",
        "\n",
        "            Reference Answer:\n",
        "            {reference_answer}\n",
        "\n",
        "            Response from Standard Retrieval:\n",
        "            {standard_result[\"response\"]}\n",
        "\n",
        "            Response from Relevant Segment Extraction:\n",
        "            {rse_result[\"response\"]}\n",
        "\n",
        "            Compare these two responses against the reference answer. Which one is:\n",
        "            1. More accurate and comprehensive\n",
        "            2. Better at addressing the user's query\n",
        "            3. Less likely to include irrelevant information\n",
        "\n",
        "            Explain your reasoning for each point.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Evaluating responses against reference answer...\")\n",
        "\n",
        "        # Generate the evaluation using the specified model\n",
        "        evaluation = client.chat.completions.create(\n",
        "            model=\"doubao-lite-128k-240828\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
        "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Print the evaluation results\n",
        "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "        print(evaluation.choices[0].message.content)\n",
        "\n",
        "    # Return the results of both methods\n",
        "    return {\n",
        "        \"rse_result\": rse_result,\n",
        "        \"standard_result\": standard_result\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "05kP4qlTaQKf",
        "outputId": "83d45730-44f6-4ca3-9ade-3e0f8b3b3ea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MPZa-FUwHMig",
        "outputId": "04e2cf78-e454-4ec8-b8c2-b1a704a8f3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= EVALUATION =========\n",
            "\n",
            "\n",
            "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Extracting text from document...\n",
            "Chunking text into non-overlapping segments...\n",
            "Created 42 chunks\n",
            "Generating embeddings for chunks...\n",
            "\n",
            "Calculating relevance scores and chunk values...\n",
            "Finding optimal continuous text segments...\n",
            "Found segment (0, 20) with score 7.0652\n",
            "Found segment (20, 40) with score 1.7484\n",
            "\n",
            "Reconstructing text segments from chunks...\n",
            "Generating response using relevant segments as context...\n",
            "\n",
            "=== FINAL RESPONSE ===\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It focuses on developing methods for explaining AI decisions, enhancing trust, and improving accountability. It is considered important because many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. By making AI systems explainable, users can assess their fairness and accuracy, which helps build trust and ensures that AI is used in a responsible and ethical manner. XAI techniques provide insights into how AI models make decisions, which is crucial for addressing concerns about bias, fairness, and transparency in AI systems. \n",
            "\n",
            "=== STARTING STANDARD TOP-K RETRIEVAL ===\n",
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Extracting text from document...\n",
            "Chunking text into non-overlapping segments...\n",
            "Created 42 chunks\n",
            "Generating embeddings for chunks...\n",
            "Creating query embedding and retrieving chunks...\n",
            "Generating response using relevant segments as context...\n",
            "\n",
            "=== FINAL RESPONSE ===\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It aims to provide insights into how AI makes decisions, enabling users to assess their fairness and accuracy. This is important because many AI systems, especially deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. Enhancing transparency and explainability is crucial for building trust and accountability. It helps users understand and trust the AI systems, and also allows for better assessment of their reliability and fairness. XAI techniques are being developed to achieve these goals and are essential for the future of AI. \n",
            "\n",
            "=== COMPARING RESULTS ===\n",
            "Evaluating responses against reference answer...\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "1. Both responses are quite accurate and comprehensive as they both cover the main points mentioned in the reference answer. They explain that Explainable AI aims to make AI systems transparent and understandable, provides insights into decision-making, and is important for building trust, accountability, and ensuring fairness. They also mention the \"black box\" nature of many AI systems and the need for XAI techniques. There isn't a significant difference in accuracy and comprehensiveness between the two.\n",
            "2. Both responses address the user's query well as they clearly explain what Explainable AI is and why it is important. They provide relevant information and explanations that directly answer the query. There isn't a notable difference in how well they address the query.\n",
            "3. In terms of including irrelevant information, both responses are focused on the main topic of Explainable AI and its importance. There is no obvious irrelevant information in either response. However, if we had to choose, perhaps the Relevant Segment Extraction response might be slightly more focused as it specifically mentions \"developing methods for explaining AI decisions\" which is a bit more specific than the general mention in the Standard Retrieval response.\n",
            "\n",
            "Overall, both responses are of similar quality and fulfill the requirements of answering the user's query. If we had to pick one, the Relevant Segment Extraction response might have a slight edge in terms of being a bit more specific. \n"
          ]
        }
      ],
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('./drive/MyDrive/colab_data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\"\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_methods(pdf_path, query, reference_answer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}