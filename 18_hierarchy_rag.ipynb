{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 用于RAG的分级索引\n",
    "\n",
    "实现一种用于RAG系统的分级索引方法(Hierarchical Indices)。这种技术通过使用两级搜索方法来提高检索效果：首先通过摘要识别相关的文档部分，然后从这些部分中检索具体细节。\n",
    "\n",
    "传统的RAG方法将所有文本块一视同仁，这可能导致：\n",
    "\n",
    "- 当文本块过小时，上下文信息丢失\n",
    "- 当文档集合较大时，检索结果无关\n",
    "- 在整个语料库中搜索效率低下\n",
    "\n",
    "分级检索解决了这些问题，具体方式如下：\n",
    "\n",
    "- 为较大的文档部分创建简洁的摘要\n",
    "- 首先搜索这些摘要以确定相关部分\n",
    "- 然后仅从这些部分中检索详细信息\n",
    "- 在保留具体细节的同时保持上下文信息\n",
    "\n",
    "实现步骤：\n",
    "\n",
    "- 从 PDF 中提取页面\n",
    "- 为每一页创建摘要，将摘要文本和元数据添加到摘要列表中\n",
    "- 为每一页创建详细块，将页面的文本切分为块\n",
    "- 为以上两个创建嵌入，并行其存入向量存储中\n",
    "- 使用查询分层检索相关块：先检索相关的摘要，收集来自相关摘要的页面，然后过滤掉不是相关页面的块，从这些相关页面检索详细块\n",
    "- 根据检索到的块生成回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"images/hierarchical_indices.svg\" alt=\"hierarchical_indices\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"images/hierarchical_indices_example.svg\" alt=\"hierarchical_indices\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab环境\n",
    "from google.colab import userdata\n",
    "# 使用火山引擎\n",
    "api_key = userdata.get(\"ARK_API_KEY\")\n",
    "base_url = userdata.get(\"ARK_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = \"doubao-1-5-lite-32k-250115\"\n",
    "image_model = \"doubao-1.5-vision-lite-250315\"\n",
    "embedding_model = \"doubao-embedding-large-text-240915\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    从PDF文件中提取文本内容，并按页分离。\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF文件的路径\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 包含文本内容和元数据的页面列表\n",
    "    \"\"\"\n",
    "    print(f\"正在提取文本 {pdf_path}...\")  # 打印正在处理的PDF路径\n",
    "    pdf = fitz.open(pdf_path)  # 使用PyMuPDF打开PDF文件\n",
    "    pages = []  # 初始化一个空列表，用于存储包含文本内容的页面\n",
    "\n",
    "    # 遍历PDF中的每一页\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # 获取当前页\n",
    "        text = page.get_text()  # 从当前页提取文本\n",
    "\n",
    "        # 跳过文本非常少的页面（少于50个字符）\n",
    "        if len(text.strip()) > 50:\n",
    "            # 将页面文本和元数据添加到列表中\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # 源文件路径\n",
    "                    \"page\": page_num + 1  # 页面编号（从1开始）\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"已提取 {len(pages)} 页的内容\")  # 打印已提取的页面数量\n",
    "    return pages  # 返回包含文本内容和元数据的页面列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    将文本分割为重叠的块，同时保留元数据。\n",
    "\n",
    "    Args:\n",
    "        text (str): 要分割的输入文本\n",
    "        metadata (Dict): 要保留的元数据\n",
    "        chunk_size (int): 每个块的大小（以字符为单位）\n",
    "        overlap (int): 块之间的重叠大小（以字符为单位）\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 包含元数据的文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化一个空列表，用于存储块\n",
    "\n",
    "    # 按指定的块大小和重叠量遍历文本\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # 提取文本块\n",
    "\n",
    "        # 跳过非常小的块（少于50个字符）\n",
    "        if chunk_text and len(chunk_text.strip()) > 50:\n",
    "            # 创建元数据的副本，并添加块特定的信息\n",
    "            chunk_metadata = metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_index\": len(chunks),  # 块的索引\n",
    "                \"start_char\": i,  # 块的起始字符索引\n",
    "                \"end_char\": i + len(chunk_text),  # 块的结束字符索引\n",
    "                \"is_summary\": False  # 标志，表示这不是摘要\n",
    "            })\n",
    "\n",
    "            # 将带有元数据的块添加到列表中\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": chunk_metadata\n",
    "            })\n",
    "\n",
    "    return chunks  # 返回带有元数据的块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    使用NumPy实现的简单向量存储。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化向量存储。\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 用于存储嵌入向量的列表\n",
    "        self.texts = []  # 用于存储原始文本的列表\n",
    "        self.metadata = []  # 用于存储每个文本元数据的列表\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        向向量存储中添加一个项目。\n",
    "\n",
    "        Args:\n",
    "            text (str): 原始文本。\n",
    "            embedding (List[float]): 嵌入向量。\n",
    "            metadata (dict, optional): 额外的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 将嵌入转换为numpy数组并添加到向量列表中\n",
    "        self.texts.append(text)  # 将原始文本添加到文本列表中\n",
    "        self.metadata.append(metadata or {})  # 添加元数据到元数据列表中，如果没有提供则使用空字典\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入最相似的项目。\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 查询嵌入向量。\n",
    "            k (int): 返回的结果数量。\n",
    "            filter_func (callable, optional): 可选的过滤函数，用于筛选元数据。\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 包含文本和元数据的前k个最相似项。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 如果没有存储向量，则返回空列表\n",
    "\n",
    "        # 将查询嵌入转换为numpy数组\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        # 使用余弦相似度计算相似度\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 如果存在过滤函数且该元数据不符合条件，则跳过该项\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "            # 计算查询向量与存储向量之间的余弦相似度\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 添加索引和相似度分数\n",
    "\n",
    "        # 按相似度排序（降序）\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 返回前k个结果\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 添加对应的文本\n",
    "                \"metadata\": self.metadata[idx],  # 添加对应的元数据\n",
    "                \"similarity\": score  # 添加相似度分数\n",
    "            })\n",
    "\n",
    "        return results  # 返回前k个最相似项的列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=None):\n",
    "    \"\"\"\n",
    "    为给定文本创建嵌入向量。\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): 输入文本列表\n",
    "        model (str): 嵌入模型名称\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: 嵌入向量列表\n",
    "    \"\"\"\n",
    "\n",
    "    model = model or embedding_model  # 如果未指定模型，则使用默认的嵌入模型\n",
    "\n",
    "    # 处理空输入的情况\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # 分批次处理（OpenAI API 的限制）\n",
    "    batch_size = 10\n",
    "    all_embeddings = []\n",
    "\n",
    "    # 遍历输入文本，按批次生成嵌入\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 获取当前批次的文本\n",
    "\n",
    "        # 调用 OpenAI 接口生成嵌入\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "\n",
    "        # 提取当前批次的嵌入向量\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 将当前批次的嵌入向量加入总列表\n",
    "\n",
    "    return all_embeddings  # 返回所有嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_summary(page_text):\n",
    "    \"\"\"\n",
    "    生成页面的简洁摘要。\n",
    "\n",
    "    Args:\n",
    "        page_text (str): 页面的文本内容\n",
    "\n",
    "    Returns:\n",
    "        str: 生成的摘要\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the summarization model\n",
    "    system_prompt = \"\"\"You are an expert summarization system.\n",
    "    Create a detailed summary of the provided text. \n",
    "    Focus on capturing the main topics, key information, and important facts.\n",
    "    Your summary should be comprehensive enough to understand what the page contains\n",
    "    but more concise than the original.\"\"\"\n",
    "\n",
    "    # 如果输入文本超过最大令牌限制，则截断\n",
    "    max_tokens = 6000\n",
    "    truncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n",
    "\n",
    "    # 向OpenAI API发出请求以生成摘要\n",
    "    response = client.chat.completions.create(\n",
    "        model=text_model,  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize this text:\\n\\n{truncated_text}\"}  # User message with the text to summarize\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated summary content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分级文档处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    将文档处理为分层索引。\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件的路径\n",
    "        chunk_size (int): 每个详细块的大小\n",
    "        chunk_overlap (int): 块之间的重叠量\n",
    "\n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: 摘要和详细向量存储\n",
    "    \"\"\"\n",
    "    # Extract pages from PDF\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create summaries for each page\n",
    "    print(\"Generating page summaries...\")\n",
    "    summaries = []\n",
    "    for i, page in enumerate(pages):\n",
    "        print(f\"Summarizing page {i+1}/{len(pages)}...\")\n",
    "        summary_text = generate_page_summary(page[\"text\"])\n",
    "        \n",
    "        # Create summary metadata\n",
    "        summary_metadata = page[\"metadata\"].copy()\n",
    "        summary_metadata.update({\"is_summary\": True})\n",
    "        \n",
    "        # Append the summary text and metadata to the summaries list\n",
    "        summaries.append({\n",
    "            \"text\": summary_text,\n",
    "            \"metadata\": summary_metadata\n",
    "        })\n",
    "    \n",
    "    # Create detailed chunks for each page\n",
    "    detailed_chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the detailed_chunks list with the chunks from the current page\n",
    "        detailed_chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(detailed_chunks)} detailed chunks\")\n",
    "    \n",
    "    # Create embeddings for summaries\n",
    "    print(\"Creating embeddings for summaries...\")\n",
    "    summary_texts = [summary[\"text\"] for summary in summaries]\n",
    "    summary_embeddings = create_embeddings(summary_texts)\n",
    "    \n",
    "    # Create embeddings for detailed chunks\n",
    "    print(\"Creating embeddings for detailed chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Create vector stores\n",
    "    summary_store = SimpleVectorStore()\n",
    "    detailed_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add summaries to summary store\n",
    "    for i, summary in enumerate(summaries):\n",
    "        summary_store.add_item(\n",
    "            text=summary[\"text\"],\n",
    "            embedding=summary_embeddings[i],\n",
    "            metadata=summary[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Add chunks to detailed store\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        detailed_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\n",
    "    return summary_store, detailed_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分级检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    使用分层索引检索信息。\n",
    "\n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        summary_store (SimpleVectorStore): 文档摘要存储\n",
    "        detailed_store (SimpleVectorStore): 详细块存储\n",
    "        k_summaries (int): 要检索的摘要数量\n",
    "        k_chunks (int): 每个摘要要检索的块数量\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 检索到的带有相关性分数的块\n",
    "    \"\"\"\n",
    "    print(f\"Performing hierarchical retrieval for query: {query}\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # First, retrieve relevant summaries\n",
    "    summary_results = summary_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_summaries\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(summary_results)} relevant summaries\")\n",
    "    \n",
    "    # Collect pages from relevant summaries\n",
    "    relevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n",
    "    \n",
    "    # Create a filter function to only keep chunks from relevant pages\n",
    "    def page_filter(metadata):\n",
    "        return metadata[\"page\"] in relevant_pages\n",
    "    \n",
    "    # Then, retrieve detailed chunks from only those relevant pages\n",
    "    detailed_results = detailed_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_chunks * len(relevant_pages),\n",
    "        filter_func=page_filter\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")\n",
    "    \n",
    "    # For each result, add which summary/page it came from\n",
    "    for result in detailed_results:\n",
    "        page = result[\"metadata\"][\"page\"]\n",
    "        matching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\n",
    "        if matching_summaries:\n",
    "            result[\"summary\"] = matching_summaries[0][\"text\"]\n",
    "    \n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用上下文生成回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        retrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Extract text from chunks and prepare context parts\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        page_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\n",
    "        context_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n",
    "    \n",
    "    # Combine all context parts into a single context string\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\n",
    "Use the information from the context to answer the user's question accurately.\n",
    "If the context doesn't contain relevant information, acknowledge that.\n",
    "Include page numbers when referencing specific information.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用分级检索实现完整的RAG流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n",
    "                    k_summaries=3, k_chunks=5, regenerate=False):\n",
    "    \"\"\"\n",
    "    Complete hierarchical RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        regenerate (bool): Whether to regenerate vector stores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Create store filenames for caching\n",
    "    summary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\n",
    "    detailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n",
    "    \n",
    "    # Process document and create stores if needed\n",
    "    if regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\n",
    "        print(\"Processing document and creating vector stores...\")\n",
    "        # Process the document to create hierarchical indices and vector stores\n",
    "        summary_store, detailed_store = process_document_hierarchically(\n",
    "            pdf_path, chunk_size, chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Save the summary store to a file for future use\n",
    "        with open(summary_store_file, 'wb') as f:\n",
    "            pickle.dump(summary_store, f)\n",
    "        \n",
    "        # Save the detailed store to a file for future use\n",
    "        with open(detailed_store_file, 'wb') as f:\n",
    "            pickle.dump(detailed_store, f)\n",
    "    else:\n",
    "        # Load existing summary store from file\n",
    "        print(\"Loading existing vector stores...\")\n",
    "        with open(summary_store_file, 'rb') as f:\n",
    "            summary_store = pickle.load(f)\n",
    "        \n",
    "        # Load existing detailed store from file\n",
    "        with open(detailed_store_file, 'rb') as f:\n",
    "            detailed_store = pickle.load(f)\n",
    "    \n",
    "    # Retrieve relevant chunks hierarchically using the query\n",
    "    retrieved_chunks = retrieve_hierarchically(\n",
    "        query, summary_store, detailed_store, k_summaries, k_chunks\n",
    "    )\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"summary_count\": len(summary_store.texts),\n",
    "        \"detailed_count\": len(detailed_store.texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准 RAG（非分级，用于对比）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n",
    "    \"\"\"\n",
    "    Standard RAG pipeline without hierarchical retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k (int): Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Extract pages from the PDF document\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create chunks directly from all pages\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the chunks list with the chunks from the current page\n",
    "        chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks for standard RAG\")\n",
    "    \n",
    "    # Create a vector store to hold the chunks\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Create embeddings for the chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Add chunks to the vector store\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve the most relevant chunks based on the query embedding\n",
    "    retrieved_chunks = store.similarity_search(query_embedding, k=k)\n",
    "    print(f\"Retrieved {len(retrieved_chunks)} chunks with standard RAG\")\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return the results including the query, response, and retrieved chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(query, pdf_path, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare hierarchical and standard RAG approaches.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        reference_answer (str, optional): Reference answer for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing RAG approaches for query: {query} ===\")\n",
    "    \n",
    "    # Run hierarchical RAG\n",
    "    print(\"\\nRunning hierarchical RAG...\")\n",
    "    hierarchical_result = hierarchical_rag(query, pdf_path)\n",
    "    hier_response = hierarchical_result[\"response\"]\n",
    "    \n",
    "    # Run standard RAG\n",
    "    print(\"\\nRunning standard RAG...\")\n",
    "    standard_result = standard_rag(query, pdf_path)\n",
    "    std_response = standard_result[\"response\"]\n",
    "    \n",
    "    # Compare results from hierarchical and standard RAG\n",
    "    comparison = compare_responses(query, hier_response, std_response, reference_answer)\n",
    "    \n",
    "    # Return a dictionary with the comparison results\n",
    "    return {\n",
    "        \"query\": query,  # The original query\n",
    "        \"hierarchical_response\": hier_response,  # Response from hierarchical RAG\n",
    "        \"standard_response\": std_response,  # Response from standard RAG\n",
    "        \"reference_answer\": reference_answer,  # Reference answer for evaluation\n",
    "        \"comparison\": comparison,  # Comparison analysis\n",
    "        \"hierarchical_chunks_count\": len(hierarchical_result[\"retrieved_chunks\"]),  # Number of chunks retrieved by hierarchical RAG\n",
    "        \"standard_chunks_count\": len(standard_result[\"retrieved_chunks\"])  # Number of chunks retrieved by standard RAG\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, hierarchical_response, standard_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare responses from hierarchical and standard RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        hierarchical_response (str): Response from hierarchical RAG\n",
    "        standard_response (str): Response from standard RAG\n",
    "        reference (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the responses\n",
    "    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \n",
    "Compare the two responses to the same query, one generated using hierarchical retrieval\n",
    "and the other using standard retrieval.\n",
    "\n",
    "Evaluate them based on:\n",
    "1. Accuracy: Which response provides more factually correct information?\n",
    "2. Comprehensiveness: Which response better covers all aspects of the query?\n",
    "3. Coherence: Which response has better logical flow and organization?\n",
    "4. Page References: Does either response make better use of page references?\n",
    "\n",
    "Be specific in your analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "\n",
    "    # Create the user prompt with the query and both responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Response from Hierarchical RAG:\n",
    "{hierarchical_response}\n",
    "\n",
    "Response from Standard RAG:\n",
    "{standard_response}\"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the user prompt\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\"\"\"\n",
    "\n",
    "    # Add the final instruction to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "\n",
    "Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the comparison analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated comparison analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation with multiple test queries.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for queries\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    \n",
    "    # Iterate over each query in the test queries\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Query: {query}\")  # Print the current query\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]  # Retrieve the reference answer for the current query\n",
    "        \n",
    "        # Compare hierarchical and standard RAG approaches\n",
    "        result = compare_approaches(query, pdf_path, reference)\n",
    "        results.append(result)  # Append the result to the results list\n",
    "    \n",
    "    # Generate overall analysis of the evaluation results\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,  # Return the individual results\n",
    "        \"overall_analysis\": overall_analysis  # Return the overall analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from individual query evaluations\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the results\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
    "Based on multiple test queries, provide an overall analysis comparing hierarchical RAG \n",
    "with standard RAG.\n",
    "\n",
    "Focus on:\n",
    "1. When hierarchical retrieval performs better and why\n",
    "2. When standard retrieval performs better and why\n",
    "3. The overall strengths and weaknesses of each approach\n",
    "4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of the evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Hierarchical chunks: {result['hierarchical_chunks_count']}, Standard chunks: {result['standard_chunks_count']}\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # Define the user prompt with the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations comparing hierarchical vs standard RAG across {len(results)} queries, \n",
    "provide an overall analysis of these two approaches:\n",
    "\n",
    "{evaluations_summary}\n",
    "\n",
    "Please provide a comprehensive analysis of the relative strengths and weaknesses of hierarchical RAG \n",
    "compared to standard RAG, with specific focus on retrieval quality and response generation.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the overall analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the evaluations summary\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated overall analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分级RAG与标准RAG方法的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and creating vector stores...\n",
      "Extracting text from data/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Generating page summaries...\n",
      "Summarizing page 1/15...\n",
      "Summarizing page 2/15...\n",
      "Summarizing page 3/15...\n",
      "Summarizing page 4/15...\n",
      "Summarizing page 5/15...\n",
      "Summarizing page 6/15...\n",
      "Summarizing page 7/15...\n",
      "Summarizing page 8/15...\n",
      "Summarizing page 9/15...\n",
      "Summarizing page 10/15...\n",
      "Summarizing page 11/15...\n",
      "Summarizing page 12/15...\n",
      "Summarizing page 13/15...\n",
      "Summarizing page 14/15...\n",
      "Summarizing page 15/15...\n",
      "Created 47 detailed chunks\n",
      "Creating embeddings for summaries...\n",
      "Creating embeddings for detailed chunks...\n",
      "Created vector stores with 15 summaries and 47 chunks\n",
      "Performing hierarchical retrieval for query: What are the key applications of transformer models in natural language processing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_9608\\2918097221.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Add the similarity score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant summaries\n",
      "Retrieved 10 detailed chunks from relevant pages\n",
      "\n",
      "=== Response ===\n",
      "I couldn't find any information about transformer models in the provided context. The context appears to focus on various applications of Artificial Intelligence (AI) and Machine Learning (ML), including computer vision, deep learning, reinforcement learning, and more. However, transformer models are not mentioned.\n",
      "\n",
      "If you're looking for information on transformer models, I'd be happy to try and help you find it. Alternatively, if you have any other questions based on the provided context, I'd be happy to try and assist you.\n",
      "Query: How do transformers handle sequential data compared to RNNs?\n",
      "\n",
      "=== Comparing RAG approaches for query: How do transformers handle sequential data compared to RNNs? ===\n",
      "\n",
      "Running hierarchical RAG...\n",
      "Loading existing vector stores...\n",
      "Performing hierarchical retrieval for query: How do transformers handle sequential data compared to RNNs?\n",
      "Retrieved 3 relevant summaries\n",
      "Retrieved 10 detailed chunks from relevant pages\n",
      "\n",
      "Running standard RAG...\n",
      "Extracting text from data/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Created 47 chunks for standard RAG\n",
      "Creating embeddings for chunks...\n",
      "Retrieved 15 chunks with standard RAG\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "Based on the provided evaluation, I will provide a comprehensive analysis of the relative strengths and weaknesses of hierarchical RAG compared to standard RAG.\n",
      "\n",
      "**Overview of Hierarchical RAG and Standard RAG**\n",
      "\n",
      "Hierarchical RAG (Retrieval Algorithm for Generating) is an extension of the standard RAG approach, which involves dividing the input text into smaller chunks or sub-sequences to facilitate more efficient and effective retrieval. The hierarchical approach further divides these chunks into smaller sub-chunks, allowing for more granular and detailed retrieval.\n",
      "\n",
      "Standard RAG, on the other hand, uses a single chunk size to retrieve relevant information from the input text.\n",
      "\n",
      "**Strengths of Hierarchical RAG**\n",
      "\n",
      "1. **Improved Retrieval Quality**: Hierarchical RAG's ability to divide the input text into smaller sub-chunks allows for more precise retrieval, as it can capture subtle nuances and relationships between words and phrases that may be missed by standard RAG.\n",
      "2. **Enhanced Response Generation**: By considering multiple levels of granularity, hierarchical RAG can generate more accurate and informative responses, as it can take into account the context and relationships between different parts of the input text.\n",
      "3. **Better Handling of Complex Input Text**: Hierarchical RAG is particularly well-suited for handling complex input text, such as long documents or texts with multiple layers of abstraction.\n",
      "\n",
      "**Weaknesses of Hierarchical RAG**\n",
      "\n",
      "1. **Increased Computational Complexity**: The hierarchical approach requires more computational resources and processing power, as it needs to handle multiple levels of granularity.\n",
      "2. **Higher Risk of Overfitting**: The increased number of parameters and complexity of the hierarchical model can lead to overfitting, particularly if the training data is limited or biased.\n",
      "\n",
      "**Strengths of Standard RAG**\n",
      "\n",
      "1. **Simpler and Faster**: Standard RAG is a simpler and faster approach, as it only requires a single chunk size and less computational resources.\n",
      "2. **Less Risk of Overfitting**: The standard model has fewer parameters and is less prone to overfitting, making it a more robust and reliable choice.\n",
      "\n",
      "**Weaknesses of Standard RAG**\n",
      "\n",
      "1. **Limited Retrieval Quality**: Standard RAG's single chunk size can lead to limited retrieval quality, as it may not capture the full range of nuances and relationships between words and phrases.\n",
      "2. **Less Effective for Complex Input Text**: Standard RAG is less effective for handling complex input text, as it may struggle to capture the context and relationships between different parts of the text.\n",
      "\n",
      "**When to Use Each Approach**\n",
      "\n",
      "1. **Use Hierarchical RAG**:\n",
      "\t* When dealing with complex input text, such as long documents or texts with multiple layers of abstraction.\n",
      "\t* When high retrieval quality and response generation are critical, such as in applications requiring accurate and informative responses.\n",
      "\t* When computational resources are not a concern, and the benefits of hierarchical retrieval outweigh the costs.\n",
      "2. **Use Standard RAG**:\n",
      "\t* When dealing with simple input text, such as short documents or texts with a clear and concise structure.\n",
      "\t* When computational resources are limited, and speed is a priority.\n",
      "\t* When the goal is to quickly retrieve relevant information, rather than generating accurate and informative responses.\n",
      "\n",
      "In conclusion, hierarchical RAG offers improved retrieval quality and response generation, but at the cost of increased computational complexity and risk of overfitting. Standard RAG, on the other hand, is simpler and faster, but may have limited retrieval quality and be less effective for complex input text. The choice of approach depends on the specific requirements and constraints of the application.\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Example query about AI for testing the hierarchical RAG approach\n",
    "query = \"What are the key applications of transformer models in natural language processing?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# Test query for formal evaluation (using only one query as requested)\n",
    "test_queries = [\n",
    "    \"How do transformers handle sequential data compared to RNNs?\"\n",
    "]\n",
    "\n",
    "# Reference answer for the test query to enable comparison\n",
    "reference_answers = [\n",
    "    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n",
    "]\n",
    "\n",
    "# Run the evaluation comparing hierarchical and standard RAG approaches\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis of the comparison\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
