{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tivon-x/all-rag-techniques/blob/main/12_adaptive_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "WblfNklALxFS"
      },
      "source": [
        "# 自适应检索以增强 RAG 系统\n",
        "\n",
        "在本笔记本中，我实现了一个自适应检索系统，该系统可以根据查询的类型动态选择最合适的检索策略。这种方法显著增强了我们的 RAG 系统在各种问题范围内提供准确且相关响应的能力。\n",
        "\n",
        "不同类型的问题需要不同的检索策略。我们的系统：\n",
        "\n",
        "1. 对查询类型进行分类（事实性、分析性、意见性或上下文性）\n",
        "2. 选择合适的检索策略\n",
        "3. 执行专门的检索技术\n",
        "4. 生成定制化的响应\n",
        "\n",
        "该系统首先将用户的查询分为以下四个类别：\n",
        "- 事实性：寻求具体、可验证信息的查询。\n",
        "- 分析性：需要全面分析或解释的查询。\n",
        "- 意见性：关于主观问题或寻求不同观点的查询。\n",
        "- 上下文性：依赖于用户特定上下文的查询。\n",
        "\n",
        "\n",
        "每种查询类型触发特定的检索策略：\n",
        "\n",
        "1. 事实性策略\n",
        "- 使用大语言模型对原始查询进行增强，以提高精确度。\n",
        "- 根据增强后的查询检索文档。\n",
        "- 使用大语言模型按相关性对文档进行排序。\n",
        "\n",
        "2. 分析性策略\n",
        "- 使用大语言模型生成多个子查询，以涵盖主查询的不同方面。\n",
        "- 为每个子查询检索文档。\n",
        "- 使用大语言模型确保最终文档选择的多样性。\n",
        "\n",
        "3. 意见性策略\n",
        "- 使用大语言模型识别主题的不同观点。\n",
        "- 检索代表每种观点的文档。\n",
        "- 使用大语言模型从检索到的文档中选择一系列不同的观点。\n",
        "\n",
        "4. 上下文性策略\n",
        "- 使用大语言模型将用户特定的上下文纳入查询。\n",
        "- 根据上下文化查询执行检索。\n",
        "- 在考虑相关性和用户上下文的情况下对文档进行排序。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngAoZlALxFT"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/tivon-x/all-rag-techniques/blob/main/images/adaptive_retrieval.svg?raw=1\" alt=\"adaptive retrieval\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QusANiM2LxFU"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fitz库需要从pymudf那里安装\n",
        "%pip install --quiet --force-reinstall pymupdf"
      ],
      "metadata": {
        "id": "xDpWFBDMMefN",
        "outputId": "6b76882e-8608-4beb-96d4-8c9343417fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qMdzdOG3LxFU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbsfFl06LxFU"
      },
      "source": [
        "## 提取文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YjH-cjxcLxFV"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page_num in range(mypdf.page_count):\n",
        "        page = mypdf[page_num]  # Get the page\n",
        "        text = page.get_text(\"text\")  # Extract text from the page\n",
        "        all_text += text  # Append the extracted text to the all_text string\n",
        "\n",
        "    return all_text  # Return the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V0Ip8r_LxFV"
      },
      "source": [
        "## 分块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BCqnLW2SLxFV"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDxUU2p8LxFV"
      },
      "source": [
        "## OpenAI API Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab环境\n",
        "from google.colab import userdata\n",
        "# 使用火山引擎\n",
        "api_key = userdata.get(\"ARK_API_KEY\")\n",
        "base_url = userdata.get(\"ARK_BASE_URL\")"
      ],
      "metadata": {
        "id": "YnX4oM-qMu9F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"doubao-lite-128k-240828\"\n",
        "embedding_model = \"doubao-embedding-text-240715\""
      ],
      "metadata": {
        "id": "N1d4PFPVMwat"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "goTH2cQSLxFV"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=base_url,\n",
        "    api_key=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soln3sdILxFW"
      },
      "source": [
        "## 向量数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "66ZF8xMCLxFW"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "        filter_func (callable, optional): Function to filter results.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = np.array(query_embedding)\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Apply filter if provided\n",
        "            if filter_func and not filter_func(self.metadata[i]):\n",
        "                continue\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the text\n",
        "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfB7wsnqLxFW"
      },
      "source": [
        "## 向量嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MWzlJFkFLxFW"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text, model = None, batch_size = 10):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text.\n",
        "\n",
        "    Args:\n",
        "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
        "    model (str): The model to be used for creating embeddings.\n",
        "    batch_size (int): batch size\n",
        "\n",
        "    Returns:\n",
        "    List[float] or List[List[float]]: The embedding vector(s).\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = embedding_model\n",
        "\n",
        "    # Convert single string to list for uniform processing\n",
        "    input_text = text if isinstance(text, list) else [text]\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(input_text), batch_size):\n",
        "      batch = input_text[i : i + batch_size]\n",
        "      # Create embeddings for the batch using the specified model\n",
        "      response = client.embeddings.create(\n",
        "          model=model,\n",
        "          input=batch\n",
        "      )\n",
        "      all_embeddings.extend(item.embedding for item in response.data)\n",
        "\n",
        "    return all_embeddings if isinstance(text, list) else all_embeddings[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1C3oAmiLxFW"
      },
      "source": [
        "## 文档处理流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "scYdqvIqLxFW"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for use with adaptive retrieval.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the extracted text\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Create embeddings for the text chunks\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Initialize the vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add each chunk and its embedding to the vector store with metadata\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "\n",
        "    # Return the chunks and the vector store\n",
        "    return chunks, store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkTJhqCLxFX"
      },
      "source": [
        "## 查询分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9uzAQ9PdLxFX"
      },
      "outputs": [],
      "source": [
        "def classify_query(query, model=None):\n",
        "    \"\"\"\n",
        "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Query category\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = model_name\n",
        "\n",
        "    # Define the system prompt to guide the AI's classification\n",
        "    system_prompt = \"\"\"You are an expert at classifying questions.\n",
        "        Classify the given query into exactly one of these categories:\n",
        "        - Factual: Queries seeking specific, verifiable information.\n",
        "        - Analytical: Queries requiring comprehensive analysis or explanation.\n",
        "        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
        "        - Contextual: Queries that depend on user-specific context.\n",
        "\n",
        "        Return ONLY the category name, without any explanation or additional text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the query to be classified\n",
        "    user_prompt = f\"Classify this query: {query}\"\n",
        "\n",
        "    # Generate the classification response from the AI model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and strip the category from the response\n",
        "    category = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Define the list of valid categories\n",
        "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
        "\n",
        "    # Ensure the returned category is valid\n",
        "    for valid in valid_categories:\n",
        "        if valid in category:\n",
        "            return valid\n",
        "\n",
        "    # Default to \"Factual\" if classification fails\n",
        "    return \"Factual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6njpQ0OMLxFX"
      },
      "source": [
        "## 实现专门的检索策略\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 事实性策略 - 关注精确性"
      ],
      "metadata": {
        "id": "3vwRP6h8TZAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "srxhq0FILxFX"
      },
      "outputs": [],
      "source": [
        "def factual_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    事实性策略 - 关注精确性\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # 使用 LLM 增强查询\n",
        "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
        "        Your task is to reformulate the given factual query to make it more precise and\n",
        "        specific for information retrieval. Focus on key entities and their relationships.\n",
        "\n",
        "        Provide ONLY the enhanced query without any explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Enhance this factual query: {query}\"\n",
        "\n",
        "    # Generate the enhanced query using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and print the enhanced query\n",
        "    enhanced_query = response.choices[0].message.content.strip()\n",
        "    print(f\"Enhanced query: {enhanced_query}\")\n",
        "\n",
        "    # Create embeddings for the enhanced query\n",
        "    query_embedding = create_embeddings(enhanced_query)\n",
        "\n",
        "    # Perform initial similarity search to retrieve documents\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
        "\n",
        "    # Initialize a list to store ranked results\n",
        "    ranked_results = []\n",
        "\n",
        "    # 使用大语言模型按相关性对文档进行评分和排序\n",
        "    for doc in initial_results:\n",
        "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
        "        ranked_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"similarity\": doc[\"similarity\"],\n",
        "            \"relevance_score\": relevance_score\n",
        "        })\n",
        "\n",
        "    # 根据相关性得分排序结果，降序\n",
        "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top k results\n",
        "    return ranked_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinQHDPHLxFX"
      },
      "source": [
        "### 2. 分析性策略 - 全面覆盖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j55CcQK2LxFX"
      },
      "outputs": [],
      "source": [
        "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    ### 2. 分析性策略 - 全面覆盖各方面查询角度\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # 子查询构建\n",
        "    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n",
        "    Generate sub-questions that explore different aspects of the main analytical query.\n",
        "    These sub-questions should cover the breadth of the topic and help retrieve\n",
        "    comprehensive information.\n",
        "\n",
        "    Return a list of exactly 3 sub-questions, one per line.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the main query\n",
        "    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
        "\n",
        "    # Generate the sub-questions using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Extract and clean the sub-questions\n",
        "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
        "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
        "    print(f\"Generated sub-queries: {sub_queries}\")\n",
        "\n",
        "    # Retrieve documents for each sub-query\n",
        "    all_results = []\n",
        "    for sub_query in sub_queries:\n",
        "        # Create embeddings for the sub-query\n",
        "        sub_query_embedding = create_embeddings(sub_query)\n",
        "        # Perform similarity search for the sub-query\n",
        "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Ensure diversity by selecting from different sub-query results\n",
        "    # Remove duplicates (same text content)\n",
        "    unique_texts = set()\n",
        "    diverse_results = []\n",
        "\n",
        "    for result in all_results:\n",
        "        if result[\"text\"] not in unique_texts:\n",
        "            unique_texts.add(result[\"text\"])\n",
        "            diverse_results.append(result)\n",
        "\n",
        "    # If we need more results to reach k, add more from initial results\n",
        "    if len(diverse_results) < k:\n",
        "        # Direct retrieval for the main query\n",
        "        main_query_embedding = create_embeddings(query)\n",
        "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
        "\n",
        "        for result in main_results:\n",
        "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
        "                unique_texts.add(result[\"text\"])\n",
        "                diverse_results.append(result)\n",
        "\n",
        "    # Return the top k diverse results\n",
        "    return diverse_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2KrkLCLxFY"
      },
      "source": [
        "### 3. 意见性策略 - 多样化视角"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gIUWBgdPLxFY"
      },
      "outputs": [],
      "source": [
        "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
        "    \"\"\"\n",
        "    意见性策略 - 多样化视角\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # 使用 LLM 识别不同的视角\n",
        "    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n",
        "        For the given query about opinions or viewpoints, identify different perspectives\n",
        "        that people might have on this topic.\n",
        "\n",
        "        Return a list of exactly 3 different viewpoint angles, one per line.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the user prompt with the main query\n",
        "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
        "\n",
        "    # Generate the different perspectives using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Extract and clean the viewpoints\n",
        "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
        "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
        "    print(f\"Identified viewpoints: {viewpoints}\")\n",
        "\n",
        "    # Retrieve documents representing each viewpoint\n",
        "    all_results = []\n",
        "    for viewpoint in viewpoints:\n",
        "        # Combine the main query with the viewpoint\n",
        "        combined_query = f\"{query} {viewpoint}\"\n",
        "        # Create embeddings for the combined query\n",
        "        viewpoint_embedding = create_embeddings(combined_query)\n",
        "        # Perform similarity search for the combined query\n",
        "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
        "\n",
        "        # Mark results with the viewpoint they represent\n",
        "        for result in results:\n",
        "            result[\"viewpoint\"] = viewpoint\n",
        "\n",
        "        # Add the results to the list of all results\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # 确保每个角度尽可能都有一篇文档\n",
        "    selected_results = []\n",
        "    for viewpoint in viewpoints:\n",
        "        # Filter documents by viewpoint\n",
        "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
        "        if viewpoint_docs:\n",
        "            selected_results.append(viewpoint_docs[0])\n",
        "\n",
        "    # Fill remaining slots with highest similarity docs\n",
        "    remaining_slots = k - len(selected_results)\n",
        "    if remaining_slots > 0:\n",
        "        # Sort remaining docs by similarity\n",
        "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
        "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "        selected_results.extend(remaining_docs[:remaining_slots])\n",
        "\n",
        "    # Return the top k results\n",
        "    return selected_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tE3EIbaLxFY"
      },
      "source": [
        "### 4. 上下文性策略 - 用户上下文整合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mL5IpPSXLxFY"
      },
      "outputs": [],
      "source": [
        "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    上下文性策略 - 用户上下文整合\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to return\n",
        "        user_context (str): Additional user context\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
        "\n",
        "    # If no user context provided, try to infer it from the query\n",
        "    if not user_context:\n",
        "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
        "For the given query, infer what contextual information might be relevant or implied\n",
        "but not explicitly stated. Focus on what background would help answering this query.\n",
        "\n",
        "Return a brief description of the implied context.\"\"\"\n",
        "\n",
        "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
        "\n",
        "        # Generate the inferred context using the LLM\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        # Extract and print the inferred context\n",
        "        user_context = response.choices[0].message.content.strip()\n",
        "        print(f\"Inferred context: {user_context}\")\n",
        "\n",
        "    # 重新构建查询以纳入上下文\n",
        "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
        "    Given a query and some contextual information, create a more specific query that\n",
        "    incorporates the context to get more relevant information.\n",
        "\n",
        "    Return ONLY the reformulated query without explanation.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "    Context: {user_context}\n",
        "\n",
        "    Reformulate the query to incorporate this context:\"\"\"\n",
        "\n",
        "    # Generate the contextualized query using the LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract and print the contextualized query\n",
        "    contextualized_query = response.choices[0].message.content.strip()\n",
        "    print(f\"Contextualized query: {contextualized_query}\")\n",
        "\n",
        "    # Retrieve documents based on the contextualized query\n",
        "    query_embedding = create_embeddings(contextualized_query)\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
        "\n",
        "    # Rank documents considering both relevance and user context\n",
        "    ranked_results = []\n",
        "\n",
        "    for doc in initial_results:\n",
        "        # 考虑上下文对文档相关性进行评分\n",
        "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
        "        ranked_results.append({\n",
        "            \"text\": doc[\"text\"],\n",
        "            \"metadata\": doc[\"metadata\"],\n",
        "            \"similarity\": doc[\"similarity\"],\n",
        "            \"context_relevance\": context_relevance\n",
        "        })\n",
        "\n",
        "    # Sort by context relevance and return top k results\n",
        "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
        "    return ranked_results[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIXH5DwNLxFY"
      },
      "source": [
        "## 文档评分的辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fMnrhvOELxFY"
      },
      "outputs": [],
      "source": [
        "def score_document_relevance(query, document, model=None):\n",
        "    \"\"\"\n",
        "    使用大语言模型对文档与查询的相关性进行评分。\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        document (str): Document text\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        float: Relevance score from 0-10\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "      model = model_name\n",
        "\n",
        "    # System prompt to instruct the model on how to rate relevance\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
        "        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
        "        0 = Completely irrelevant\n",
        "        10 = Perfectly addresses the query\n",
        "\n",
        "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    # you can truncate document if the context length is small\n",
        "    doc_preview = document\n",
        "\n",
        "    # User prompt containing the query and document preview\n",
        "    user_prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "\n",
        "        Document: {doc_preview}\n",
        "\n",
        "        Relevance score (0-10):\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the score from the model's response\n",
        "    score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract numeric score using regex\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
        "    if match:\n",
        "        score = float(match.group(1))\n",
        "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
        "    else:\n",
        "        # Default score if extraction fails\n",
        "        return 5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6EsFckFcLxFY"
      },
      "outputs": [],
      "source": [
        "def score_document_context_relevance(query, context, document, model=None):\n",
        "    \"\"\"\n",
        "    考虑查询和上下文对文档的相关性进行评分。\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): User context\n",
        "        document (str): Document text\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        float: Relevance score from 0-10\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the model on how to rate relevance considering context\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
        "        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
        "        when considering the provided context, where:\n",
        "        0 = Completely irrelevant\n",
        "        10 = Perfectly addresses the query in the given context\n",
        "\n",
        "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    model = model_name if not model else model\n",
        "\n",
        "    # you can truncate document if it's too long\n",
        "    doc_preview = document\n",
        "\n",
        "    # User prompt containing the query, context, and document preview\n",
        "    user_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "    Context: {context}\n",
        "\n",
        "    Document: {doc_preview}\n",
        "\n",
        "    Relevance score considering context (0-10):\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the score from the model's response\n",
        "    score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract numeric score using regex\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
        "    if match:\n",
        "        score = float(match.group(1))\n",
        "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
        "    else:\n",
        "        # Default score if extraction fails\n",
        "        return 5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwxunrh4LxFZ"
      },
      "source": [
        "## 自适应检索器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mv9v_bCFLxFZ"
      },
      "outputs": [],
      "source": [
        "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    通过选择并执行合适的策略来执行自适应检索。\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        k (int): Number of documents to retrieve\n",
        "        user_context (str): Optional user context for contextual queries\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Retrieved documents\n",
        "    \"\"\"\n",
        "    # Classify the query to determine its type\n",
        "    query_type = classify_query(query)\n",
        "    print(f\"Query classified as: {query_type}\")\n",
        "\n",
        "    # Select and execute the appropriate retrieval strategy based on the query type\n",
        "    if query_type == \"Factual\":\n",
        "        # Use the factual retrieval strategy for precise information\n",
        "        results = factual_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Analytical\":\n",
        "        # Use the analytical retrieval strategy for comprehensive coverage\n",
        "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Opinion\":\n",
        "        # Use the opinion retrieval strategy for diverse perspectives\n",
        "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
        "    elif query_type == \"Contextual\":\n",
        "        # Use the contextual retrieval strategy, incorporating user context\n",
        "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
        "    else:\n",
        "        # Default to factual retrieval strategy if classification fails\n",
        "        results = factual_retrieval_strategy(query, vector_store, k)\n",
        "\n",
        "    return results  # Return the retrieved documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-DiAN5aLxFZ"
      },
      "source": [
        "## 生成响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ENiiblkMLxFZ"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, results, query_type, model=None):\n",
        "    \"\"\"\n",
        "    Generate a response based on query, retrieved documents, and query type.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Retrieved documents\n",
        "        query_type (str): Type of query\n",
        "        model (str): LLM model\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    model = model if model else model_name\n",
        "\n",
        "    # Prepare context from retrieved documents by joining their texts with separators\n",
        "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
        "\n",
        "    # Create custom system prompt based on query type\n",
        "    if query_type == \"Factual\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
        "    Answer the question based on the provided context. Focus on accuracy and precision.\n",
        "    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    elif query_type == \"Analytical\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
        "    Based on the provided context, offer a comprehensive analysis of the topic.\n",
        "    Cover different aspects and perspectives in your explanation.\n",
        "    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
        "\n",
        "    elif query_type == \"Opinion\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
        "    Based on the provided context, present different perspectives on the topic.\n",
        "    Ensure fair representation of diverse opinions without showing bias.\n",
        "    Acknowledge where the context presents limited viewpoints.\"\"\"\n",
        "\n",
        "    elif query_type == \"Contextual\":\n",
        "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
        "    Answer the question considering both the query and its context.\n",
        "    Make connections between the query context and the information in the provided documents.\n",
        "    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    else:\n",
        "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
        "\n",
        "    # Create user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Please provide a helpful response based on the context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response using the OpenAI client\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-81Z6sonLxFZ"
      },
      "source": [
        "## RAG 流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i17lEi96LxFZ"
      },
      "outputs": [],
      "source": [
        "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with adaptive retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        k (int): Number of documents to retrieve\n",
        "        user_context (str): Optional user context\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, retrieved documents, query type, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Classify the query to determine its type\n",
        "    query_type = classify_query(query)\n",
        "    print(f\"Query classified as: {query_type}\")\n",
        "\n",
        "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
        "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
        "\n",
        "    # Generate a response based on the query, retrieved documents, and query type\n",
        "    response = generate_response(query, retrieved_docs, query_type)\n",
        "\n",
        "    # Compile the results into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"query_type\": query_type,\n",
        "        \"retrieved_documents\": retrieved_docs,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXutGilLxFZ"
      },
      "source": [
        "## 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RJbLSrRzLxFZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Compare adaptive retrieval with standard retrieval on a set of test queries.\n",
        "\n",
        "    This function processes a document, runs both standard and adaptive retrieval methods\n",
        "    on each test query, and compares their performance. If reference answers are provided,\n",
        "    it also evaluates the quality of responses against these references.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document to be processed as the knowledge source\n",
        "        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n",
        "        reference_answers (List[str], optional): Reference answers for evaluation metrics\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results containing individual query results and overall comparison\n",
        "    \"\"\"\n",
        "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
        "\n",
        "    # Process document to extract text, create chunks and build the vector store\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Initialize collection for storing comparison results\n",
        "    results = []\n",
        "\n",
        "    # Process each test query with both retrieval methods\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # --- Standard retrieval approach ---\n",
        "        print(\"\\n--- Standard Retrieval ---\")\n",
        "        # Create embedding for the query\n",
        "        query_embedding = create_embeddings(query)\n",
        "        # Retrieve documents using simple vector similarity\n",
        "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
        "        # Generate response using a generic approach\n",
        "        standard_response = generate_response(query, standard_docs, \"General\")\n",
        "\n",
        "        # --- Adaptive retrieval approach ---\n",
        "        print(\"\\n--- Adaptive Retrieval ---\")\n",
        "        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
        "        query_type = classify_query(query)\n",
        "        # Retrieve documents using the strategy appropriate for this query type\n",
        "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
        "        # Generate a response tailored to the query type\n",
        "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
        "\n",
        "        # Store complete results for this query\n",
        "        result = {\n",
        "            \"query\": query,\n",
        "            \"query_type\": query_type,\n",
        "            \"standard_retrieval\": {\n",
        "                \"documents\": standard_docs,\n",
        "                \"response\": standard_response\n",
        "            },\n",
        "            \"adaptive_retrieval\": {\n",
        "                \"documents\": adaptive_docs,\n",
        "                \"response\": adaptive_response\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add reference answer if available for this query\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            result[\"reference_answer\"] = reference_answers[i]\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Display preview of both responses for quick comparison\n",
        "        print(\"\\n--- Responses ---\")\n",
        "        print(f\"Standard: {standard_response[:200]}...\")\n",
        "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
        "\n",
        "    # Calculate comparative metrics if reference answers are available\n",
        "    if reference_answers:\n",
        "        comparison = compare_responses(results)\n",
        "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "        print(comparison)\n",
        "\n",
        "    # Return the complete evaluation results\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iRx8rac3LxFZ"
      },
      "outputs": [],
      "source": [
        "def compare_responses(results):\n",
        "    \"\"\"\n",
        "    Compare standard and adaptive responses against reference answers.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results containing both types of responses\n",
        "\n",
        "    Returns:\n",
        "        str: Comparison analysis\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI in comparing responses\n",
        "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
        "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
        "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
        "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
        "\n",
        "    # Initialize the comparison text with a header\n",
        "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
        "\n",
        "    # Iterate through each result to compare responses\n",
        "    for i, result in enumerate(results):\n",
        "        # Skip if there is no reference answer for the query\n",
        "        if \"reference_answer\" not in result:\n",
        "            continue\n",
        "\n",
        "        # Add query details to the comparison text\n",
        "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
        "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
        "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
        "\n",
        "        # Add standard retrieval response to the comparison text\n",
        "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
        "\n",
        "        # Add adaptive retrieval response to the comparison text\n",
        "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
        "\n",
        "        # Create the user prompt for the AI to compare the responses\n",
        "        user_prompt = f\"\"\"\n",
        "        Reference Answer: {result['reference_answer']}\n",
        "\n",
        "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
        "\n",
        "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
        "\n",
        "        Provide a detailed comparison of the two responses.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate the comparison analysis using the OpenAI client\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        # Add the AI's comparison analysis to the comparison text\n",
        "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
        "\n",
        "    return comparison_text  # Return the complete comparison analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf5zRPILLxFa"
      },
      "source": [
        "## 评估自适应检索系统（定制化查询）\n",
        "\n",
        "使用自适应 RAG 评估系统的最后一步是调用 evaluate_adaptive_vs_standard() 函数，并提供您的 PDF 文档和测试查询："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "johO-mU5Syz9",
        "outputId": "a98ea80f-eac7-4fcb-9816-91d2ff782aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1fWDUjxiLxFd"
      },
      "outputs": [],
      "source": [
        "# Path to your knowledge source document\n",
        "# This PDF file contains the information that the RAG system will use\n",
        "pdf_path = \"./drive/MyDrive/colab_data/AI_Information.pdf\"\n",
        "\n",
        "# Define test queries covering different query types to demonstrate\n",
        "# how adaptive retrieval handles various query intentions\n",
        "test_queries = [\n",
        "    \"What is Explainable AI (XAI)?\",                                              # Factual query - seeking definition/specific information\n",
        "    # \"How do AI ethics and governance frameworks address potential societal impacts?\",  # Analytical query - requiring comprehensive analysis\n",
        "    # \"Is AI development moving too fast for proper regulation?\",                   # Opinion query - seeking diverse perspectives\n",
        "    # \"How might explainable AI help in healthcare decisions?\",                     # Contextual query - benefits from context-awareness\n",
        "]\n",
        "\n",
        "# Reference answers for more thorough evaluation\n",
        "# These can be used to objectively assess response quality against a known standard\n",
        "reference_answers = [\n",
        "    \"Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\",\n",
        "    # \"AI ethics and governance frameworks address potential societal impacts by establishing guidelines and principles to ensure AI systems are developed and used responsibly. These frameworks focus on fairness, accountability, transparency, and the protection of human rights to mitigate risks and promote beneficial output.5.\",\n",
        "    # \"Opinions on whether AI development is moving too fast for proper regulation vary. Some argue that rapid advancements outpace regulatory efforts, leading to potential risks and ethical concerns. Others believe that innovation should continue at its current pace, with regulations evolving alongside to address emerging challenges.\",\n",
        "    # \"Explainable AI can significantly aid healthcare decisions by providing transparent and understandable insights into AI-driven recommendations. This transparency helps healthcare professionals trust AI systems, make informed decisions, and improve patient output by understanding the rationale behind AI suggestions.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DtXR8uEdLxFd",
        "outputId": "40ed506e-1d4f-43eb-a92d-a4d7a1f90874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "\n",
            "\n",
            "Query 1: What is Explainable AI (XAI)?\n",
            "\n",
            "--- Standard Retrieval ---\n",
            "\n",
            "--- Adaptive Retrieval ---\n",
            "Query classified as: Factual\n",
            "Executing Factual retrieval strategy for: 'What is Explainable AI (XAI)?'\n",
            "Enhanced query: What is the specific definition and characteristics of Explainable AI (XAI)? What are the key techniques and methods used in Explainable AI (XAI)? How does Explainable AI (XAI) differ from other types of AI in terms of explainability?\n",
            "\n",
            "--- Responses ---\n",
            "Standard: Explainable AI (XAI) aims to make AI systems more transparent and understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountab...\n",
            "Adaptive: Explainable AI (XAI) aims to make AI decisions more understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountability. XAI is ...\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "# Evaluation of Standard vs. Adaptive Retrieval\n",
            "\n",
            "## Query 1: What is Explainable AI (XAI)?\n",
            "*Query Type: Factual*\n",
            "\n",
            "**Reference Answer:**\n",
            "Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\n",
            "\n",
            "**Standard Retrieval Response:**\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountability. XAI is being developed to address the need for users to assess the reliability and fairness of AI systems. It focuses on making AI systems more interpretable and providing explanations for their actions, which is crucial for building trust in AI across various domains such as environmental monitoring, healthcare, and finance. \n",
            "\n",
            "**Adaptive Retrieval Response:**\n",
            "Explainable AI (XAI) aims to make AI decisions more understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountability. XAI is focused on making AI systems more transparent and interpretable, aiming to address concerns about the black-box nature of some AI models. It helps users assess the fairness and accuracy of AI systems and enables them to have a better understanding of how and why the systems make certain decisions. \n",
            "\n",
            "**Comparison Analysis:**\n",
            "**Strengths of the Standard Retrieval Response**:\n",
            "- It provides a more comprehensive description by elaborating on the need for XAI to address various domains such as environmental monitoring, healthcare, and finance. This shows a broader understanding of the applications and significance of XAI.\n",
            "- It emphasizes the development of techniques to provide insights and enhance trust and accountability, which is a key aspect of XAI. It gives a sense of the practical efforts involved in making AI systems more transparent.\n",
            "\n",
            "**Weaknesses of the Standard Retrieval Response**:\n",
            "- While it is comprehensive, it may lack a bit of focus and clarity in some areas. For example, the description could be more concise and to the point without sacrificing the essential information.\n",
            "- It might not stand out as much in terms of presenting a unique perspective or highlighting specific aspects that make XAI particularly important or innovative.\n",
            "\n",
            "**Strengths of the Adaptive Retrieval Response**:\n",
            "- It is more focused on the core goal of making AI decisions more understandable, which directly addresses one of the main objectives of XAI. It cuts to the chase and emphasizes the key aspect that users are often concerned about - the interpretability of AI decisions.\n",
            "- It highlights the concern about the black-box nature of some AI models and how XAI aims to address this. This shows an awareness of a common issue and the role that XAI plays in resolving it.\n",
            "\n",
            "**Weaknesses of the Adaptive Retrieval Response**:\n",
            "- It may be seen as less comprehensive compared to the standard response as it does not mention specific domains or the broader context of XAI as extensively. It focuses more on the immediate goal of making decisions understandable rather than the overall picture.\n",
            "- It might give the impression of being a bit one-sided or lacking in the detailed explanations that the standard response provides.\n",
            "\n",
            "In conclusion, both responses have their strengths and weaknesses. The standard response offers a more detailed and broad view of XAI, while the adaptive response is more focused and direct on the key aspect of making AI decisions understandable. Depending on the context and the audience, either response could be useful. If the goal is to provide a comprehensive overview, the standard response is better. If the focus is on quickly communicating the main purpose of XAI, the adaptive response is more appropriate. \n",
            "\n",
            "\n",
            "# Evaluation of Standard vs. Adaptive Retrieval\n",
            "\n",
            "## Query 1: What is Explainable AI (XAI)?\n",
            "*Query Type: Factual*\n",
            "\n",
            "**Reference Answer:**\n",
            "Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\n",
            "\n",
            "**Standard Retrieval Response:**\n",
            "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountability. XAI is being developed to address the need for users to assess the reliability and fairness of AI systems. It focuses on making AI systems more interpretable and providing explanations for their actions, which is crucial for building trust in AI across various domains such as environmental monitoring, healthcare, and finance. \n",
            "\n",
            "**Adaptive Retrieval Response:**\n",
            "Explainable AI (XAI) aims to make AI decisions more understandable. It involves developing techniques to provide insights into how AI models make decisions, enhancing trust and accountability. XAI is focused on making AI systems more transparent and interpretable, aiming to address concerns about the black-box nature of some AI models. It helps users assess the fairness and accuracy of AI systems and enables them to have a better understanding of how and why the systems make certain decisions. \n",
            "\n",
            "**Comparison Analysis:**\n",
            "**Strengths of the Standard Retrieval Response**:\n",
            "- It provides a more comprehensive description by elaborating on the need for XAI to address various domains such as environmental monitoring, healthcare, and finance. This shows a broader understanding of the applications and significance of XAI.\n",
            "- It emphasizes the development of techniques to provide insights and enhance trust and accountability, which is a key aspect of XAI. It gives a sense of the practical efforts involved in making AI systems more transparent.\n",
            "\n",
            "**Weaknesses of the Standard Retrieval Response**:\n",
            "- While it is comprehensive, it may lack a bit of focus and clarity in some areas. For example, the description could be more concise and to the point without sacrificing the essential information.\n",
            "- It might not stand out as much in terms of presenting a unique perspective or highlighting specific aspects that make XAI particularly important or innovative.\n",
            "\n",
            "**Strengths of the Adaptive Retrieval Response**:\n",
            "- It is more focused on the core goal of making AI decisions more understandable, which directly addresses one of the main objectives of XAI. It cuts to the chase and emphasizes the key aspect that users are often concerned about - the interpretability of AI decisions.\n",
            "- It highlights the concern about the black-box nature of some AI models and how XAI aims to address this. This shows an awareness of a common issue and the role that XAI plays in resolving it.\n",
            "\n",
            "**Weaknesses of the Adaptive Retrieval Response**:\n",
            "- It may be seen as less comprehensive compared to the standard response as it does not mention specific domains or the broader context of XAI as extensively. It focuses more on the immediate goal of making decisions understandable rather than the overall picture.\n",
            "- It might give the impression of being a bit one-sided or lacking in the detailed explanations that the standard response provides.\n",
            "\n",
            "In conclusion, both responses have their strengths and weaknesses. The standard response offers a more detailed and broad view of XAI, while the adaptive response is more focused and direct on the key aspect of making AI decisions understandable. Depending on the context and the audience, either response could be useful. If the goal is to provide a comprehensive overview, the standard response is better. If the focus is on quickly communicating the main purpose of XAI, the adaptive response is more appropriate. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation comparing adaptive vs standard retrieval\n",
        "# This will process each query using both methods and compare the results\n",
        "evaluation_results = evaluate_adaptive_vs_standard(\n",
        "    pdf_path=pdf_path,                  # Source document for knowledge extraction\n",
        "    test_queries=test_queries,          # List of test queries to evaluate\n",
        "    reference_answers=reference_answers  # Optional ground truth for comparison\n",
        ")\n",
        "\n",
        "# The results will show a detailed comparison between standard retrieval and\n",
        "# adaptive retrieval performance across different query types, highlighting\n",
        "# where adaptive strategies provide improved outcomes\n",
        "print(evaluation_results[\"comparison\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-new-specific-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}